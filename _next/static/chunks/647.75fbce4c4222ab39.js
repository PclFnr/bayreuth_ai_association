(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[647],{5789:function(){},4647:function(e,t,i){"use strict";var n;function a(e){return void 0===e||e}function o(e){let t=Array(e);for(let i=0;i<e;i++)t[i]=s();return t}function s(){return Object.create(null)}function r(e,t){return t.length-e.length}function l(e){return"string"==typeof e}function h(e){return"object"==typeof e}function c(e){return"function"==typeof e}function d(e,t){var i=u;if(e&&(t&&(e=p(e,t)),this.H&&(e=p(e,this.H)),this.J&&1<e.length&&(e=p(e,this.J)),i||""===i)){if(t=e.split(i),this.filter){e=this.filter,i=t.length;let n=[];for(let a=0,o=0;a<i;a++){let i=t[a];i&&!e[i]&&(n[o++]=i)}e=n}else e=t}return e}i.r(t),i.d(t,{search:function(){return $}});let u=/[\p{Z}\p{S}\p{P}\p{C}]+/u,f=/[\u0300-\u036f]/g;function m(e,t){let i=Object.keys(e),n=i.length,a=[],o="",s=0;for(let r=0,l,h;r<n;r++)(h=e[l=i[r]])?(a[s++]=g(t?"(?!\\b)"+l+"(\\b|_)":l),a[s++]=h):o+=(o?"|":"")+l;return o&&(a[s++]=g(t?"(?!\\b)("+o+")(\\b|_)":"("+o+")"),a[s]=""),a}function p(e,t){for(let i=0,n=t.length;i<n&&(e=e.replace(t[i],t[i+1]));i+=2);return e}function g(e){return RegExp(e,"g")}function w(e){let t="",i="";for(let n=0,a=e.length,o;n<a;n++)(o=e[n])!==i&&(t+=i=o);return t}function b(e){return d.call(this,(""+e).toLowerCase(),!1)}let y={},v={};function x(e){k(e,"add"),k(e,"append"),k(e,"search"),k(e,"update"),k(e,"remove")}function k(e,t){e[t+"Async"]=function(){let e;let i=this,n=arguments;var a=n[n.length-1];return c(a)&&(e=a,delete n[n.length-1]),a=new Promise(function(e){setTimeout(function(){i.async=!0;let a=i[t].apply(i,n);i.async=!1,e(a)})}),e?(a.then(e),this):a}}function A(e,t,i,n){let a=e.length,o=[],r,l,h=0;n&&(n=[]);for(let c=a-1;0<=c;c--){let d=e[c],u=d.length,f=s(),m=!r;for(let e=0;e<u;e++){let s=d[e],u=s.length;if(u)for(let e=0,d,p;e<u;e++)if(p=s[e],r){if(r[p]){if(!c){if(i)i--;else if(o[h++]=p,h===t)return o}(c||n)&&(f[p]=1),m=!0}if(n&&(d=(l[p]||0)+1,l[p]=d,d<a)){let e=n[d-2]||(n[d-2]=[]);e[e.length]=p}}else f[p]=1}if(n)r||(l=f);else if(!m)return[];r=f}if(n)for(let e=n.length-1,a,s;0<=e;e--){s=(a=n[e]).length;for(let e=0,n;e<s;e++)if(!r[n=a[e]]){if(i)i--;else if(o[h++]=n,h===t)return o;r[n]=1}}return o}function T(e){this.l=!0!==e&&e,this.cache=s(),this.h=[]}function I(e,t,i){h(e)&&(e=e.query);let n=this.cache.get(e);return n||(n=this.search(e,t,i),this.cache.set(e,n)),n}T.prototype.set=function(e,t){if(!this.cache[e]){var i=this.h.length;for(i===this.l?delete this.cache[this.h[i-1]]:i++,--i;0<i;i--)this.h[i]=this.h[i-1];this.h[0]=e}this.cache[e]=t},T.prototype.get=function(e){let t=this.cache[e];if(this.l&&t&&(e=this.h.indexOf(e))){let t=this.h[e-1];this.h[e-1]=this.h[e],this.h[e]=t}return t};let P={memory:{charset:"latin:extra",D:3,B:4,m:!1},performance:{D:3,B:3,s:!1,context:{depth:2,D:1}},match:{charset:"latin:extra",G:"reverse"},score:{charset:"latin:advanced",D:20,B:3,context:{depth:3,D:9}},default:{}};function L(e,t,i,n,a,o,s,r){setTimeout(function(){let l=e(i?i+"."+n:n,JSON.stringify(s));l&&l.then?l.then(function(){t.export(e,t,i,a,o+1,r)}):t.export(e,t,i,a,o+1,r)})}function O(e,t){if(!(this instanceof O))return new O(e);if(e){l(e)?e=P[e]:(i=e.preset)&&(e=Object.assign({},i[i],e)),i=e.charset;var i,n=e.lang;l(i)&&(-1===i.indexOf(":")&&(i+=":default"),i=v[i]),l(n)&&(n=y[n])}else e={};let r,h,c=e.context||{};if(this.encode=e.encode||i&&i.encode||b,this.register=t||s(),this.D=r=e.resolution||9,this.G=t=i&&i.G||e.tokenize||"strict",this.depth="strict"===t&&c.depth,this.l=a(c.bidirectional),this.s=h=a(e.optimize),this.m=a(e.fastupdate),this.B=e.minlength||1,this.C=e.boost,this.map=h?o(r):s(),this.A=r=c.resolution||1,this.h=h?o(r):s(),this.F=i&&i.F||e.rtl,this.H=(t=e.matcher||n&&n.H)&&m(t,!1),this.J=(t=e.stemmer||n&&n.J)&&m(t,!0),i=t=e.filter||n&&n.filter){i=t,n=s();for(let e=0,t=i.length;e<t;e++)n[i[e]]=1;i=n}this.filter=i,this.cache=(t=e.cache)&&new T(t)}function G(e,t,i,n,a){return i&&1<e?t+(n||0)<=e?i+(a||0):(e-1)/(t+(n||0))*(i+(a||0))+1|0:0}function S(e,t,i,n,a,o,r){let l=r?e.h:e.map;(!t[i]||r&&!t[i][r])&&(e.s&&(l=l[n]),r?((t=t[i]||(t[i]=s()))[r]=1,l=l[r]||(l[r]=s())):t[i]=1,l=l[i]||(l[i]=[]),e.s||(l=l[n]||(l[n]=[])),o&&l.includes(a)||(l[l.length]=a,e.m&&((e=e.register[a]||(e.register[a]=[]))[e.length]=l)))}function C(e,t,i,n,a,o,s,r){let l=[],h=r?e.h:e.map;if(e.s||(h=z(h,s,r,e.l)),h){let i=0,c=Math.min(h.length,r?e.A:e.D);for(let t=0,d=0,u,f;t<c&&(!(u=h[t])||(e.s&&(u=z(u,s,r,e.l)),a&&u&&o&&((f=u.length)<=a?(a-=f,u=null):(u=u.slice(a),a=0)),!u||(l[i++]=u,!o||!((d+=u.length)>=n))));t++);if(i)return o?F(l,n,0):void(t[t.length]=l)}return!i&&l}function F(e,t,i){return e=1===e.length?e[0]:[].concat.apply([],e),i||e.length>t?e.slice(i,i+t):e}function z(e,t,i,n){return e=i?(e=e[(n=n&&t>i)?t:i])&&e[n?i:t]:e[t]}function q(e,t,i,n,a){let o=0;if(e.constructor===Array){if(a)-1!==(t=e.indexOf(t))?1<e.length&&(e.splice(t,1),o++):o++;else{a=Math.min(e.length,i);for(let s=0,r;s<a;s++)(r=e[s])&&(o=q(r,t,i,n,a),n||o||delete e[s])}}else for(let s in e)(o=q(e[s],t,i,n,a))||delete e[s];return o}function R(e){e=e.data;var t=self._index;let i=e.args;var n=e.task;"init"===n?(n=e.options||{},e=e.factory,t=n.encode,n.cache=!1,t&&0===t.indexOf("function")&&(n.encode=Function("return "+t)()),e?(Function("return "+e)()(self),self._index=new self.FlexSearch.Index(n),delete self.FlexSearch):self._index=new O(n)):(e=e.id,t=t[n].apply(t,i),postMessage("search"===n?{id:e,msg:t}:{id:e}))}(n=O.prototype).append=function(e,t){return this.add(e,t,!0)},n.add=function(e,t,i,n){if(t&&(e||0===e)){if(!n&&!i&&this.register[e])return this.update(e,t);if(n=(t=this.encode(t)).length){let c=s(),d=s(),u=this.depth,f=this.D;for(let m=0;m<n;m++){let p=t[this.F?n-1-m:m];var a=p.length;if(p&&a>=this.B&&(u||!d[p])){var o=G(f,n,m),r="";switch(this.G){case"full":if(2<a){for(o=0;o<a;o++)for(var l=a;l>o;l--)if(l-o>=this.B){var h=G(f,n,m,a,o);S(this,d,r=p.substring(o,l),h,e,i)}break}case"reverse":if(1<a){for(l=a-1;0<l;l--)(r=p[l]+r).length>=this.B&&S(this,d,r,G(f,n,m,a,l),e,i);r=""}case"forward":if(1<a){for(l=0;l<a;l++)(r+=p[l]).length>=this.B&&S(this,d,r,o,e,i);break}default:if(this.C&&(o=Math.min(o/this.C(t,p,m)|0,f-1)),S(this,d,p,o,e,i),u&&1<n&&m<n-1){for(a=s(),r=this.A,o=p,l=Math.min(u+1,n-m),a[o]=1,h=1;h<l;h++)if((p=t[this.F?n-1-m-h:m+h])&&p.length>=this.B&&!a[p]){a[p]=1;let t=this.l&&p>o;S(this,c,t?o:p,G(r+(n/2>r?0:1),n,m,l-1,h-1),e,i,t?p:o)}}}}}this.m||(this.register[e]=1)}}return this},n.search=function(e,t,i){let n,a,o;i||(!t&&h(e)?e=(i=e).query:h(t)&&(i=t));let l=[],c,d,u=0;if(i){e=i.query||e,t=i.limit,u=i.offset||0;var f=i.context;d=i.suggest}if(e&&1<(c=(e=this.encode(""+e)).length)){i=s();var m=[];for(let t=0,n=0,a;t<c;t++)if((a=e[t])&&a.length>=this.B&&!i[a]){if(!this.s&&!d&&!this.map[a])return l;m[n++]=a,i[a]=1}c=(e=m).length}if(!c)return l;for(t||(t=100),f=this.depth&&1<c&&!1!==f,i=0,f?(n=e[0],i=1):1<c&&e.sort(r);i<c;i++){if(o=e[i],f?(a=C(this,l,d,t,u,2===c,o,n),d&&!1===a&&l.length||(n=o)):a=C(this,l,d,t,u,1===c,o),a)return a;if(d&&i===c-1){if(!(m=l.length)){if(f){f=0,i=-1;continue}return l}if(1===m)return F(l[0],t,u)}}return A(l,t,u,d)},n.contain=function(e){return!!this.register[e]},n.update=function(e,t){return this.remove(e).add(e,t)},n.remove=function(e,t){let i=this.register[e];if(i){if(this.m)for(let t=0,n;t<i.length;t++)(n=i[t]).splice(n.indexOf(e),1);else q(this.map,e,this.D,this.s),this.depth&&q(this.h,e,this.A,this.s);if(t||delete this.register[e],this.cache){t=this.cache;for(let i=0,n;i<t.h.length;i++)n=t.h[i],t.cache[n].includes(e)&&(t.h.splice(i--,1),delete t.cache[n])}}return this},n.searchCache=I,n.export=function(e,t,i,n,a,o){let r,l,h=!0;switch(void 0===o&&(h=new Promise(e=>{o=e})),a||(a=0)){case 0:if(r="reg",this.m)for(let e in l=s(),this.register)l[e]=1;else l=this.register;break;case 1:r="cfg",l={doc:0,opt:this.s?1:0};break;case 2:r="map",l=this.map;break;case 3:r="ctx",l=this.h;break;default:void 0===i&&o&&o();return}return L(e,t||this,i,r,n,a,l,o),h},n.import=function(e,t){if(t)switch(l(t)&&(t=JSON.parse(t)),e){case"cfg":this.s=!!t.opt;break;case"reg":this.m=!1,this.register=t;break;case"map":this.map=t;break;case"ctx":this.h=t}},x(O.prototype);let D=0;function M(e){var t;if(!(this instanceof M))return new M(e);e?c(t=e.encode)&&(e.encode=t.toString()):e={},(t=(self||window)._factory)&&(t=t.toString());let n="undefined"==typeof window&&self.exports,a=this;this.o=function(e,t,n){let a;try{a=t?new(i(5789)).Worker("//node/node.js"):e?new Worker(URL.createObjectURL(new Blob(["onmessage="+R.toString()],{type:"text/javascript"}))):new Worker(l(n)?n:"worker/worker.js",{type:"module"})}catch(e){}return a}(t,n,e.worker),this.h=s(),this.o&&(n?this.o.on("message",function(e){a.h[e.id](e.msg),delete a.h[e.id]}):this.o.onmessage=function(e){e=e.data,a.h[e.id](e.msg),delete a.h[e.id]},this.o.postMessage({task:"init",factory:t,options:e}))}function j(e){M.prototype[e]=M.prototype[e+"Async"]=function(){let t;let i=this,n=[].slice.call(arguments);var a=n[n.length-1];return c(a)&&(t=a,n.splice(n.length-1,1)),a=new Promise(function(t){setTimeout(function(){i.h[++D]=t,i.o.postMessage({task:e,id:D,args:n})})}),t?(a.then(t),this):a}}function B(e){if(!(this instanceof B))return new B(e);var t,i=e.document||e.doc||e;this.K=[],this.h=[],this.A=[],this.register=s(),this.key=(t=i.key||i.id)&&H(t,this.A)||"id",this.m=a(e.fastupdate),this.C=(t=i.store)&&!0!==t&&[],this.store=t&&s(),this.I=(t=i.tag)&&H(t,this.A),this.l=t&&s(),this.cache=(t=e.cache)&&new T(t),e.cache=!1,this.o=e.worker,this.async=!1,t=s();let n=i.index||i.field||i;l(n)&&(n=[n]);for(let i=0,a,o;i<n.length;i++)l(a=n[i])||(o=a,a=a.field),o=h(o)?Object.assign({},e,o):e,this.o&&(t[a]=new M(o),t[a].o||(this.o=!1)),this.o||(t[a]=new O(o,this.register)),this.K[i]=H(a,this.A),this.h[i]=a;if(this.C)for(l(e=i.store)&&(e=[e]),i=0;i<e.length;i++)this.C[i]=H(e[i],this.A);this.index=t}function H(e,t){let i=e.split(":"),n=0;for(let a=0;a<i.length;a++)0<=(e=i[a]).indexOf("[]")&&(e=e.substring(0,e.length-2))&&(t[n]=!0),e&&(i[n++]=e);return n<i.length&&(i.length=n),1<n?i:i[0]}function E(e,t){if(l(t))e=e[t];else for(let i=0;e&&i<t.length;i++)e=e[t[i]];return e}function W(e,t,i,n){let a=this.l[e],o=a&&a.length-i;if(o&&0<o)return(o>t||i)&&(a=a.slice(i,i+t)),n&&(a=U.call(this,a)),{tag:e,result:a}}function U(e){let t=Array(e.length);for(let i=0,n;i<e.length;i++)n=e[i],t[i]={id:n,doc:this.store[n]};return t}j("add"),j("append"),j("search"),j("update"),j("remove"),(n=B.prototype).add=function(e,t,i){if(h(e)&&(e=E(t=e,this.key)),t&&(e||0===e)){if(!i&&this.register[e])return this.update(e,t);for(let n=0,a,o;n<this.h.length;n++)o=this.h[n],l(a=this.K[n])&&(a=[a]),function e(t,i,n,a,o,s,r,l){if(t=t[r]){if(a===i.length-1){if(t.constructor===Array){if(n[a]){for(i=0;i<t.length;i++)o.add(s,t[i],!0,!0);return}t=t.join(" ")}o.add(s,t,l,!0)}else if(t.constructor===Array)for(r=0;r<t.length;r++)e(t,i,n,a,o,s,r,l);else r=i[++a],e(t,i,n,a,o,s,r,l)}}(t,a,this.A,0,this.index[o],e,a[0],i);if(this.I){let n=E(t,this.I),a=s();l(n)&&(n=[n]);for(let t=0,o,s;t<n.length;t++)if(!a[o=n[t]]&&(a[o]=1,s=this.l[o]||(this.l[o]=[]),!i||!s.includes(e))&&(s[s.length]=e,this.m)){let t=this.register[e]||(this.register[e]=[]);t[t.length]=s}}if(this.store&&(!i||!this.store[e])){let i;if(this.C){i=s();for(let e=0,n;e<this.C.length;e++)l(n=this.C[e])?i[n]=t[n]:function e(t,i,n,a,o){if(t=t[o],a===n.length-1)i[o]=t;else if(t){if(t.constructor===Array)for(i=i[o]=Array(t.length),o=0;o<t.length;o++)e(t,i,n,a,o);else i=i[o]||(i[o]=s()),o=n[++a],e(t,i,n,a,o)}}(t,i,n,0,n[0])}this.store[e]=i||t}}return this},n.append=function(e,t){return this.add(e,t,!0)},n.update=function(e,t){return this.remove(e).add(e,t)},n.remove=function(e){if(h(e)&&(e=E(e,this.key)),this.register[e]){for(var t=0;t<this.h.length&&(this.index[this.h[t]].remove(e,!this.o),!this.m);t++);if(this.I&&!this.m)for(let i in this.l){let n=(t=this.l[i]).indexOf(e);-1!==n&&(1<t.length?t.splice(n,1):delete this.l[i])}this.store&&delete this.store[e],delete this.register[e]}return this},n.search=function(e,t,i,n){i||(!t&&h(e)?(i=e,e=""):h(t)&&(i=t,t=0));let a=[],o=[],r,c,d,u,f,m,p=0;if(i){if(i.constructor===Array)d=i,i=null;else{if(e=i.query||e,d=(r=i.pluck)||i.index||i.field,u=i.tag,c=this.store&&i.enrich,f="and"===i.bool,t=i.limit||t||100,m=i.offset||0,u&&(l(u)&&(u=[u]),!e)){for(let e=0,i;e<u.length;e++)(i=W.call(this,u[e],t,m,c))&&(a[a.length]=i,p++);return p?a:[]}l(d)&&(d=[d])}}d||(d=this.h),f=f&&(1<d.length||u&&1<u.length);let g=!n&&(this.o||this.async)&&[];for(let r=0,h,w,b;r<d.length;r++){let y;if(l(w=d[r])||(w=(y=w).field,e=y.query||e,t=y.limit||t,c=y.enrich||c),g)g[r]=this.index[w].searchAsync(e,t,y||i);else{if(b=(h=n?n[r]:this.index[w].search(e,t,y||i))&&h.length,u&&b){let e=[],i=0;f&&(e[0]=[h]);for(let t=0,n,a;t<u.length;t++)n=u[t],(b=(a=this.l[n])&&a.length)&&(i++,e[e.length]=f?[a]:a);i&&(b=(h=f?A(e,t||100,m||0):function(e,t){let i=s(),n=s(),a=[];for(let t=0;t<e.length;t++)i[e[t]]=1;for(let e=0,o;e<t.length;e++){o=t[e];for(let e=0,t;e<o.length;e++)i[t=o[e]]&&!n[t]&&(n[t]=1,a[a.length]=t)}return a}(h,e)).length)}if(b)o[p]=w,a[p++]=h;else if(f)return[]}}if(g){let n=this;return new Promise(function(a){Promise.all(g).then(function(o){a(n.search(e,t,i,o))})})}if(!p)return[];if(r&&(!c||!this.store))return a[0];for(let e=0,t;e<o.length;e++){if((t=a[e]).length&&c&&(t=U.call(this,t)),r)return t;a[e]={field:o[e],result:t}}return a},n.contain=function(e){return!!this.register[e]},n.get=function(e){return this.store[e]},n.set=function(e,t){return this.store[e]=t,this},n.searchCache=I,n.export=function(e,t,i,n,a,o){let s;if(void 0===o&&(s=new Promise(e=>{o=e})),a||(a=0),n||(n=0),n<this.h.length){let i=this.h[n],s=this.index[i];t=this,setTimeout(function(){s.export(e,t,a?i:"",n,a++,o)||(n++,a=1,t.export(e,t,i,n,a,o))})}else{let t,s;switch(a){case 1:t="tag",s=this.l,i=null;break;case 2:t="store",s=this.store,i=null;break;default:o();return}L(e,this,i,t,n,a,s,o)}return s},n.import=function(e,t){if(t)switch(l(t)&&(t=JSON.parse(t)),e){case"tag":this.l=t;break;case"reg":this.m=!1,this.register=t;for(let e=0,i;e<this.h.length;e++)(i=this.index[this.h[e]]).register=t,i.m=!1;break;case"store":this.store=t;break;default:let i=(e=e.split("."))[0];e=e[1],i&&e&&this.index[i].import(e,t)}},x(B.prototype);let N=[g("[\xe0\xe1\xe2\xe3\xe4\xe5]"),"a",g("[\xe8\xe9\xea\xeb]"),"e",g("[\xec\xed\xee\xef]"),"i",g("[\xf2\xf3\xf4\xf5\xf6ő]"),"o",g("[\xf9\xfa\xfb\xfcű]"),"u",g("[\xfdŷ\xff]"),"y",g("\xf1"),"n",g("[\xe7c]"),"k",g("\xdf"),"s",g(" & ")," and "];function V(e){var t=e=""+e;return t.normalize&&(t=t.normalize("NFD").replace(f,"")),d.call(this,t.toLowerCase(),!e.normalize&&N)}let _=/[^a-z0-9]+/,J={b:"p",v:"f",w:"f",z:"s",x:"s",ß:"s",d:"t",n:"m",c:"k",g:"k",j:"k",q:"k",i:"e",y:"e",u:"o"};function K(e){e=V.call(this,e).join(" ");let t=[];if(e){let i=e.split(_),n=i.length;for(let a=0,o,s=0;a<n;a++)if((e=i[a])&&(!this.filter||!this.filter[e])){let i=J[o=e[0]]||o,n=i;for(let t=1;t<e.length;t++){let a=J[o=e[t]]||o;a&&a!==n&&(i+=a,n=a)}t[s++]=i}}return t}let Z=[g("ae"),"a",g("oe"),"o",g("sh"),"s",g("th"),"t",g("ph"),"f",g("pf"),"f",g("(?![aeo])h(?![aeo])"),"",g("(?!^[aeo])h(?!^[aeo])"),""];function Y(e,t){return e&&(2<(e=K.call(this,e).join(" ")).length&&(e=p(e,Z)),t||(1<e.length&&(e=w(e)),e&&(e=e.split(" ")))),e||[]}let Q=g("(?!\\b)[aeo]");v["latin:default"]={encode:b,F:!1,G:""},v["latin:simple"]={encode:V,F:!1,G:""},v["latin:balance"]={encode:K,F:!1,G:"strict"},v["latin:advanced"]={encode:Y,F:!1,G:""},v["latin:extra"]={encode:function(e){return e&&(1<(e=Y.call(this,e,!0)).length&&(e=e.replace(Q,"")),1<e.length&&(e=w(e)),e&&(e=e.split(" "))),e||[]},F:!1,G:""};let X=new({Index:O,Document:B,Worker:M,registerCharset:function(e,t){v[e]=t},registerLanguage:function(e,t){y[e]=t}}).Document({tokenize:"full",document:{id:"url",index:"content",store:["title","pageTitle"]},context:{resolution:9,depth:2,bidirectional:!0}});for(let{url:e,sections:t}of[{url:"/",sections:[["Home",null,["The Bayreuth AI Association is an association of students enthusiastic about artificial intelligence. We want to promote the exchange on the topic of artificial intelligence with a focus on the practical implementation of algorithms.Our mission is to create a space for the open exchange of ideas and experiences in the hands-on use of AI algorithms in and around Bayreuth.","Everyone is welcome to join us – from 1st semester to professor.","Our meetings are organized by topic and start with a short information about the topic before we share our experiences about it and learn from each other. In small projects we want to implement our ideas together.","We meet every second Tuesday from 18:00 – 20:00 in room S65, building RW I. For the exact dates, check out the Important dates section.","For those attending, feel free to join our open WhatsApp group were we send updates and share relvant AI news and info: Link to join","Please feel free to contact us if you are interested!","This website provides an overview of past topics, corresponding codes / repositories and is meant as a dynamically growing source of knowledge."]]]},{url:"/docs/about-our-meetings",sections:[["About our meetings",null,["The meetings are generally held every two weeks on Tuesday evenings, at 18:00 (exact starting time is 18:15) in room S 110. The exact dates are in our Important dates page.","The first 15-20 minutes of every meeting will be spent discussing recent news happening on the fields of AI, machine learning and computer science. Then the schedule will depend on the type of meeting.","Our meetings have two flavours: AI-nstruction (technical, workshop or presentation-based approach) and AI-ntellectuals (non-technical, a more discussion-based approach). The aim of the AI-nstruction meetings is to share experience-based knowledge and resources to other students who are interested in applying AI and machine learning in their own research or personal projects. On the other hand, the goal of the AI-ntellectuals meetings is to promote interdisciplinary discussion and exchange ideas.","AI-nstruction meetings are focused on a topic, which could be an AI tool, a machine learning algorithm/paradigm or a specific resource of value for people interested in working with AI and machine learning. These meetings are usually led by a knowledge ambassador, a person with applied research experience on the topic. In this meetings, participants should have some knowledge of computer science and programming (mainly in machine learning with Python) to be able to follow and make the most of the meeting.","AI-ntellectuals meetings are semi-structured open discussions for anyone interested in AI topics. Depending on the number of participants and their interests, we might split into groups around different topics in AI, such as Ethics, Biases, Explainability, Social Impact, etc. Participants from any field and degree are very welcome, the more diverse the better!","In the future, new flavours of meetings and events will be included as well. For more information on our next meetings, please see our events calendar."]]]},{url:"/docs/dynamic-few-shot-text-classification",sections:[["Dynamic few-shot text classification",null,[]],["Overview","overview",["Dynamic Few-Shot Classification is an extension of Few-Shot Text Classification that is more suitable for larger datasets. Instead of using a fixed set of examples for each class, it constructs a dynamic subset for each sample on the fly. This allows to efficiently utilize the limited contex window of the model and save the number of consumed tokens.","Dynamic Few-Shot Classification can be motivated from a variety of ways, from prior studies show-casing how kNN-Pretraining improves question answering capabilities ([1]), or literature that shows, how dynamically adjusting the prompt template can improve performance ([2]). In this sense it would also make sense, that dynamically adjusting the examples may be better for performance as well. However, hand-crafting each prompt is an exhaustive task ([3]) and not in the spirit of automated classification.","Let's consider a toy example, where the goal is to determine whether the review is about a book or a movie. The training dataset consists of 6 samples, 3 for each class:","Now let's say we want to classify the following review:","Since the query is about a sci-fi book, we would like to only examples 1 and 4 to be used for classification, since they are the most relevant. If we use the dynamic few-shot classifier with 1 example per class, and investigate which examples were selected, we can see that the model successfully identified examples 1 and 4 as the most relevant ones:","This is achieved by adding a KNN search algorithm as an additional preprocessor. If we assume that the most relevant examples are the closest ones in space, then the problem reduces to a nearest neighbors search and can be tackled in three steps:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","DynamicFewShotOllamaClassifier"]]]},{url:"/docs/few-shot-text-classification",sections:[["Few-shot text classification",null,[]],["Overview","overview",["Few-shot text classification is a task of classifying a text into one of the pre-defined classes based on a few examples of each class. For example, given a few examples of the class positive, negative, and neutral, the model should be able to classify a new text into one of these classes. This concept is sometimes called in-context-learning ([1]). This promises great results but may be very unstable ([2]).","Like Sciit-LLM, Scikit-Ollama does not select a subset of the training data, and instead use the entire training set to construct the examples. Therefore, if your training set is large, you might want to consider splitting it into training and validation sets, while keeping the training set small (we recommend not to exceed 10 examples per class).","Also keep in mind that the order of the examples may have an influence on model performance!","Example using llama3:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","FewShotOllamaClassifier","MultiLabelFewShotOllamaClassifier"]]]},{url:"/docs/how-to-contribute",sections:[["How to contribute \uD83E\uDD17",null,["Hi and thanks for your interest in ! There are many ways to contribute, regardless of your background or experience level. The following will guide you through contributing.","If you haven't already please check out scikit-llm which this project is based off! Contributions to their work will likely also find their way here!"]],["How Can I Contribute?","how-can-i-contribute",["There are several ways you can contribute to this project:","Before contributing, we recommend that you open an issue to discuss your planned changes. This allows us to align our goals, provide guidance, and potentially find other contributors interested in collaborating on the same feature or bug fix.","When contributing to this project, you must agree that you have authored 100% of the content, that you have the necessary rights to the content and that the content you contribute may be provided under the project license."]]]},{url:"/docs/introduction-backend-families",sections:[["Backend families",null,["On a high level, Scikit-LLM estimators are divided based on the language model backend family they use. The backend family is defined by the API format and does not necessarily correspond to the language model architecture. For example, all backends that follow the OpenAI API format are groupped into gpt family regardless the actual language model architecture or provider. Eeach backend family has its own set of estimators which are located in the  sub-module.","For example, the Zero-Shot Classifier is available as  for the gpt family, and as  for the vertex family. The separation between the backend families is necessary to allow for a reasonable level of flexibility if/when model providers introduce model-specific features that are not supported by other providers and hence cannot be easily abstracted away. At the same time, the number of model families is kept to a minimum to simplify the usage and maintenance of the library. Since the OpenAI API is by far the most popular and widely used, backends that follow that format are preferred over the others.","Whenever the backend family supports multiple backends, the default one is used unless the  parameter specifies a particular backend namespace. For example, the default backend for the gpt family is the OpenAI backend. However, you can use the Azure backend by setting . However, please note that not every estimator supports every backend."]],["GPT Family","gpt-family",["The GPT family includes all backends that follow the OpenAI API format.","OpenAI (default)","The OpenAI backend is the default backend for the GPT family. It is used whenever the  parameter does not specify a particular backend namespace.","To use the OpenAI backend, you need to set your OpenAI API key and organization ID as follows:","Azure","OpenAI models can be alternatively used as a part of the Azure OpenAI service. To use the Azure backend, you need to provide your Azure API key and endpoint as follows:","When using the Azure backend, the model should be specified as . For example, if you created a gpt-3.5 deployment under the name my-model, you should use .","GPT4ALL","GPT4ALL is an open-source library that provides a unified API for multiple small-scale language models, that can be run locally on a consumer-grade hardware, even without a GPU. To use the GPT4ALL backend, you need to install the corresponding extension as follows:","Then, you can use the GPT4ALL by specifying the model as , which will be downloaded automatically. For the full list of available models, please refer to the GPT4ALL official documentation.","The models available through the GPT4ALL out of the box have very limited capabilities and are not recommended for most of the use cases. In addition, not all models are permitted for commercial use. Please check the license of the model you are using before deploying it in production.","Custom URL","Custom URL backend allows to use any GPT estimator with any OpenAI-compatible provider (either running locally or in the cloud).","In order to use the backend, it is necessary to set a global custom url:","When using  and  backends within the same script, it is necessary to reset the custom url configuration using ."]],["Vertex Family","vertex-family",["The Vertex family currently includes a single (default) backend, which is the Google Vertex AI.","In order to use the Vertex backend, you need to configure your Google Cloud credentials as follows:","Log in to Google Cloud Console and create a Google Cloud project. After the project is created, select this project from a list of projects next to the Google Cloud logo (upper left corner).","Search for Vertex AI in the search bar and select it from the list of services.","Install a Google Cloud CLI on the local machine by following the steps from the official documentation, and set the application default credentials by running the following command:","Configure Scikit-LLM with your project ID:","Additionally, for tuning LLMs in Vertex, it is required to have to have 64 cores of the TPU v3 pod training resource. By default this quota is set to 0 cores and has to be increased as follows (ignore this if you are not planning to use the tunable estimators):"]]]},{url:"/docs/ollama-setup",sections:[["Ollama setup",null,[]],["","",["To get started with Ollama, follow the instructions on their download page. Or if you're on Linux:","Configure server options as you see fit and then launch the client:","Then download a model using the ollama cli:","You can optionally test if everything works as you expect by using a chat session:"]]]},{url:"/docs/quick-start",sections:[["Quick start",null,[]],["","",["To get started with Dingo, you can install the framework using pip:","Now we can create a simple pipeline that summarizes the text provided as input and translates it into French. In this particular example, we will use GPT-3.5 model from OpenAI, but Dingo supports other LLM providers as well.","Firstly, make sure to set the  environment variable to your OpenAI API key:","Next, create a new Python script and import the necessary modules:","Then, define the pipeline by creating and chaining the building blocks together:","Finally, run the pipeline with the input text:","To deploy the pipeline as a web service, you can use the following code:","This will start a web server on port 8000, exposing the pipeline as a REST API. You can now send requests using any HTTP client, or even using the official OpenAI python client library:","In this example, we have created a simple pipeline which is not designed for multi-turn conversations. To make it compatible with OpenAI Chat structure, Dingo defines special message roles like  and  which are used to pass the input arguments to the pipeline. Section Core goes into more details on differences between context and chat inputs and how to handle them in Dingo."]]]},{url:"/docs/text-summarization",sections:[["Text summarization",null,[]],["Overview","overview",["LLMs excel at performing summarization tasks. Scikit-LLM provides a summarizer that can be used both as stand-alone estimator, or as a preprocessor (in this case we can make an analogy with a dimensionality reduction preprocessor).","Example:","Please be aware that the  hyperparameter sets a soft limit, which is not strictly enforced outside of the prompt. Therefore, in some cases, the actual number of words might be slightly higher.","Additionally, it is possible to generate a summary, emphasizing a specific concept, by providing an optional parameter :"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","GPTSummarizer"]]]},{url:"/docs/text-translation",sections:[["Text translation",null,[]],["Overview","overview",["LLMs have proven their proficiency in translation tasks. The actual performance will heavily depend on the efficacy of the chosen model. Nonetheless we implemented the same capabilites as  and offer a locally running translation interface.","Example:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","OllamaTranslator"]]]},{url:"/docs/text-vectorization",sections:[["Text vectorization",null,[]],["Overview","overview",["LLMs can be used solely for data preprocessing by embedding a chunk of text of arbitrary length to a fixed-dimensional vector, that can be further used with virtually any model (e.g. classification, regression, clustering, etc.).","With Scikit-Ollama you can choose from a large variety of embedding models. The quality of which you can check on leaderboards such as Huggingface's MTEB. In the following example we will work with the default  embedding model. Simply download it using the usual Ollama CLI command:","Example 1: Embedding the text","Example 2: Combining the vectorizer with the XGBoost classifier in a scikit-learn pipeline"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","OllamaVectorizer"]]]},{url:"/docs/tunable-text-classification",sections:[["Tunable text classification",null,[]],["Overview","overview",["Tunable estimators allow to fine-tune the underlying LLM for a classification task. Usually, tuning is performed directly in the cloud (e.g. OpenAI, Vertex), therefore it is not required to have a GPU on your local machine. However, be aware that tuning can be costly and time-consuming. We recommend to first try the in-context learning estimators, and only if they do not provide satisfactory results, to try the tunable estimators.","Example using GPT-3.5-Turbo-0613:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","GPTClassifier","MultiLabelGPTClassifier","VertexClassifier"]]]},{url:"/docs/tunable-text-to-text",sections:[["Tunable text-to-text",null,[]],["Overview","overview",["Tunable text-to-text estimators are estimators that can be tuned to perform a variety of tasks, including but not limited to text summarization, question answering, and text translation. These estimators use the provided data as-is, without any additional preprocessing, or constructing prompts. While this approach allows for more flexibility, it is the user's responsibility to ensure that the data is formatted correctly."]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","TunableGPTText2Text","TunableVertexText2Text"]]]},{url:"/docs/use-cases-local-chatbot",sections:[["Building a local chatbot",null,[]],["Overview","overview",["In the previous tutorial, we have built a RAG chatbot using a closed-source LLM and embedding model from OpenAI. Since some users prefer running LLMs locally, this tutorial will demonstrate how to build a RAG chatbot using a fully local, open-source solution by changing just two Dingo components."]],["Chatbot Architecture and Technical Stack","chatbot-architecture-and-technical-stack",["","The application will consist of the following components:","Streamlit application: provides a frontend interface for users to interact with a chatbot.","FastAPI: facilitates communication between the frontend and backend.","CapybaraHermes-2.5-Mistral-7B-GGUF: LLM that generates responses upon receiving user queries.","Embedding model from SentenceTransformers: computes text embeddings.","QDrant: vector database that stores embedded chunks of text.","There are two main differences to an architecture used in the previous tutorial:","For running the model locally, Dingo can use  that is a Python binding for  library which allows to run models converted to GGUF, a binary file format for storing models for inference with .","You can find many GGUF models on Hugging Face Hub. We have chosen  model prvided by TheBloke for this tutorial.","In order to download the model, you must go to , where you will find many different files to choose from. They correspond to different quantization types of the model. Quantization involves reducing the memory needed to store model weights by decreasing their precision (for example, from 32-bit floating points to 4-bit integers). Higher precision usually leads to a higher accuracy but also requires more computational resources, which can make the model slower and more costly to operate. Decreasing the precision allows loading large models that typically would not fit into memory, and accelerating the inference. Usually, a 4-bit quantization is considered to be an optimal balance between performance, and size/speed for LLMs.","SentenceTransformers is a Python toolkit that is built on top of Hugging Face's transformers library. It facilitates using transformer models, like BERT, RoBERTa, and others, for generating sentence embeddings. These embeddings can be used for tasks such as clustering, semantic search, and classification of texts. You can check the provided pre-trained models tuned for specific tasks either on the page of SentenceTransformers here, or on the Hugging Face Hub. The models on Hugging Face Hub have a widget that allows running inferences and playing with the model directly in the browser."]],["Implementation","implementation",["As the first step, we need to initialize an embedding model, a chat model and a vector store that will be populated with embedded chunks in the next step.","The subsequent steps involve populating the vector store, creating a RAG pipeline, and building a chatbot UI. These steps are exactly the same as in the previous tutorial.","By asking a question about the Phi-3 family of models, we can verify that our local model accurately retrieves the relevant information:",""]],["Conclusion","conclusion",["In this tutorial we have built a simple local chatbot that utilizes RAG technique and successfully retrieves information from a vector store to generate up-to-date responses. It can be seen that Dingo provides developers with flexibility, as the components of a LLM pipeline can be easily exchanged. For example, we were able to switch from a proprietary solution to a fully open-source solution running locally by simply changing two components of the pipeline."]]]},{url:"/docs/use-cases-rag-agent",sections:[["Building a RAG agent",null,[]],["Overview","overview",["In previous tutorials, we built a pipeline that embeds the chunks of text similar to user's query to a system message, which allows the chatbot to access the external knowledge base. However, in practice, this approach may be too naive, as it:","All of these limitations can be addressed by building a more sophisticated pipeline logic, that might have a routing and query-rewriting mechanisms. However, a viable alternative is to use an  which can inherently perform all of these tasks.","The fundamental concept of agents involves using a language model to determine a sequence of actions (including the usage of external tools) and their order. One possible action could be retrieving data from an external knowledge base in response to a user's query. In this tutorial, we will develop a simple Agent that accesses multiple data sources and invokes data retrieval when needed.","As an example of external knowledge bases, we will use three webpages containing release announcement posts about recently released generative models:","Since all of these models were released recently and this information was not included in GPT-4's training data, GPT can either provide no information about these topics, or may hallucinate and generate incorrect responses (see example in my previous article here). By creating an agent that is able to retrieve data from external datasources (such as webpages linked above), we will provide an LLM with relevant contextual information that will be used for generating responses."]],["RAG Agent Architecture and Technical Stack","rag-agent-architecture-and-technical-stack",["","The application will consist of the following components:","Streamlit application: provides a frontend interface for users to interact with a chatbot.",": facilitates communication between the frontend and backend.",":  model from OpenAI that has access to provided knowledge bases and invokes data retrieval from them if needed.",": a vector store containing documentation about two recently released Phi-3 family of models and Llama 3.",": a vector store containing documentation about recently released OpenVoice model.","model from OpenAI: computes text embeddings.","QDrant: vector database that stores embedded chunks of text."]],["Implementation","implementation",["Indexing","Step 1:","As the first step, we need to initialize an embedding model, a chat model, and two vector stores: one for storing documentation for Llama 3 and Phi-3, and another for storing documentation for OpenVoice.","It is needed to set OPENAI_API_KEY environment variable.","Step 2:","Then, the above-mentioned websites have to be parsed, chunked into smaller pieces, and embedded. The embedded chunks are used to populate the corresponding vector stores.","Run this script:","Step 3:","Once the vector store is created, we can create a RAG pipeline. To access the pipeline from the streamlit application, we can serve it using the  function, which provides a REST API compatible with the OpenAI API (this means that we can use an official OpenAI Python client to interact with the pipeline).","Run the script:","At this stage, we have an openai-compatible compatible backend with a model named , running on . The Streamlit application will send requests to this backend.","Step 4:","Finally, we can proceed with building a chatbot UI:","Run the application:","\uD83C\uDF89 We have successfully developed an agent that is augmented with the technical documentation of several newly released generative models, and can retrieve information from these documents if necessary. To assess the agent's ability to decide when to call the  function and its effectiveness in retrieving data from external sources, we can pose some questions about the documents provided. As you can see below, the agent generated correct responses to these questions:",""]],["Conclusion","conclusion",['In this tutorial, we have developed a RAG agent that can access external knowledge bases and retrieve data from them if needed. Unlike a "naive" RAG pipeline, the agent can selectively decide whether to access the external data, which data source to use (and how many times), and how to rewrite the user\'s query before retrieving the data. This approach allows the agent to provide more accurate and relevant responses, while the high-level pipeline logic remains as simple as of a "naive" RAG pipeline.']]]},{url:"/docs/use-cases-rag-chatbot",sections:[["Building a RAG chatbot",null,[]],["Overview","overview",["Chatbots are among the most popular use cases for large language models (LLMs). They are designed to understand and respond to user inquiries, provide answers, perform tasks, or direct users to resources. Utilizing chatbots can significantly decrease customer support costs and improve response times to user requests. However, a common issue with chatbots is their tendency to deliver generic information when users expect domain-specific responses. Additionally, they may generate outdated information when users need current updates.","For demonstrations, I have chosen the webpage about Phi-3 — a family of open AI models by Microsoft released in April 2024.","If we ask how many parameters Phi-3-mini model has, GPT-4 will generate a response indicating that it does not know the answer:","If we ask GPT-3.5 the same question, it will hallucinate and provide incorrect information:","These problems can be addressed by using the retrieval-augmented generation (RAG) technique. This technique supplements the LLM with a knowledge base external to its training data sources. For instance, an organization's internal knowledge base, such as a Wiki or internal PDFs, can be provided.","The tutorial below will demonstrate how to build a simple chatbot that utilizes RAG technique and can retrieve information about a recently released family of Phi-3 models."]],["RAG Architecture","rag-architecture",["","The basic steps of the Naive RAG include:","1. Indexing","Indexing starts with extraction of raw data from various formats such as webpage, PDF, etc. To manage the context restrictions of language models and increase the response accuracy, the extracted text is broken down into smaller, manageable chunks. For now, Dingo supports a recursive chunking that involves breaking down a large text input into smaller segments recursively until the chunks are of a desired size. The choice of the chunking size is heavily dependent on the needs of RAG application. Thus, it is recommeded to experiment with different sizes to select the best one that will allow preserving the context and maintaining the accuracy. The extracted chunks are encoded into vector representations using an embedding model and stored in a vector database.","2. Retrieval","When a user submits a query, the RAG system uses the encoding model from the indexing phase to convert the query into a vector representation. It then calculates similarity scores between the query vector and the vectors of chunks in the vector database. The system identifies and retrieves the top K chunks with the highest similarity to the query. These chunks serve as the expanded context for the prompt.","3. Generation","The users query and selected chunks are combined into a single prompt and passed to LLM. Thus, the model is provided with the necessary contextual information to formulate and deliver a response."]],["Chatbot Architecture and Technical Stack","chatbot-architecture-and-technical-stack",["","On a high level, the application will consist of the following components:","Streamlit application: provides a frontend interface for users to interact with a chatbot.",": facilitates communication between the frontend and backend.","model from OpenAI: LLM that generates responses upon receiving user queries.","model from OpenAI: computes text embeddings.","QDrant: vector database that stores embedded chunks of text."]],["Implementation","implementation",["Indexing","Step 1:","As the first step, we need to initialize an embedding model, a chat model and a vector store that will be populated with embedded chunks in the next step.","It is needed to set OPENAI_API_KEY environment variable.","Step 2:","Then, the website about Phi-3 family of models has to be parsed, chunked into smaller pieces, and embedded. The embedded chunks are used to populate a vector store.","Run this script:","At this stage, the vector store is created, allowing chunks to be retrieved and incorporated into the prompt based on a user's query.","[Optional Step]","It is also possible to identify which chunks are retrieved and check their similarity scores to the user's query:","We can see that the correct chunk was retrieved, which indeed contains information about the number of parameters in the Phi-3-mini model.","Retrieval and Augmentation","Step 3:","Once the vector store is created, we can create a RAG pipeline and serve it.","Streamlit only supports two types of messages:  and . However, it us often more appropriate to include the retrieved data into the  message. Therefore, we use a custom block that injects a  message into the chat prompt before passing it to the RAG modifier.","Run the script:","At this stage, we have a RAG pipeline compatible with the OpenAI API, named , running on . The Streamlit application will send requests to this backend.","Step 4:","Finally, we can proceed with building a chatbot UI:","Run the application:","\uD83C\uDF89 We have successfully developed a chatbot that is augmented with the technical documentation of Phi-3 family of models.If we pose the same question to this chatbot as we previously did to GPT-4 and GPT-3.5 models, we will observe that it correctly answers the question:",""]],["Conclusion","conclusion",["In this tutorial we have built a simple chatbot that utilizes RAG technique and successfully retrieves information from a vector store to generate up-to-date responses. It can be seen that Dingo enhances the development of LLM-based applications by offering essential (core) features and flexibility. That allows developers to quickly and easily create application prototypes."]]]},{url:"/docs/zero-shot-text-classification",sections:[["Zero-shot text classification",null,[]],["Overview","overview",["One of the powerful features of LLMs is the ability to perform text classification without being re-trained. For that, the only requirement is that the labels must be descriptive.","For example, let's consider a task of classifying a text into one of the following categories: [positive, negative, neutral]. We will use a class  and a regular scikit-learn API to perform the classification:","However, in the zero-shot setting, the training data is not required as it is only used for the extraction of the candidate labels. Therefore, it is sufficient to manually provide a list of candidate labels:","Additionally, it is possible to perform the classification in a multi-label setting, where multiple labels can be assigned to a single text at a same time:","Unlike in a typical supervised setting, the performance of a zero-shot classifier greatly depends on how the label itself is structured. It has to be expressed in natural language, be descriptive and self-explanatory. For example, in the previous semantic classification task, it could be beneficial to transform a label from  to ."]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","ZeroShotGPTClassifier","MultiLabelZeroShotGPTClassifier","ZeroShotVertexClassifier","MultiLabelZeroShotVertexClassifier"]]]}])for(let[i,n,a]of t)X.add({url:e+(n?"#"+n:""),title:i,content:[i,...a].join("\n"),pageTitle:n?t[0][0]:void 0});function $(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},i=X.search(e,{...t,enrich:!0});return 0===i.length?[]:i[0].result.map(e=>({url:e.id,title:e.doc.title,pageTitle:e.doc.pageTitle}))}}}]);