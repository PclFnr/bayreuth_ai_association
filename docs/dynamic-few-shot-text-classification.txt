3:I[9275,[],""]
4:I[1343,[],""]
5:I[1747,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","472","static/chunks/472-cd817ab664ce0632.js","185","static/chunks/app/layout-1cfcfb656b6cf4a5.js"],"Providers"]
6:I[1227,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","472","static/chunks/472-cd817ab664ce0632.js","185","static/chunks/app/layout-1cfcfb656b6cf4a5.js"],"Layout"]
7:I[231,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","613","static/chunks/app/docs/dynamic-few-shot-text-classification/page-dea00a39e69c61bf.js"],""]
0:["KteObhGWKN4CvpFO1QXQU",[[["",{"children":["docs",{"children":["dynamic-few-shot-text-classification",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",{"children":["docs",{"children":["dynamic-few-shot-text-classification",{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","docs","children","dynamic-few-shot-text-classification","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","docs","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"en","className":"h-full antialiased __variable_01f60e __variable_a0637f","suppressHydrationWarning":true,"children":["$","body",null,{"className":"flex min-h-full bg-white dark:bg-zinc-900","children":["$","$L5",null,{"children":["$","$L6",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16","children":["$","div",null,{"className":"flex h-full flex-col items-center justify-center text-center","children":[["$","p",null,{"className":"font-display text-sm font-medium text-zinc-900 dark:text-white","children":"404"}],["$","h1",null,{"className":"mt-3 font-display text-3xl tracking-tight text-zinc-900 dark:text-white","children":"Page not found"}],["$","p",null,{"className":"mt-2 text-sm text-zinc-500 dark:text-zinc-400","children":"Sorry, we couldn’t find the page you’re looking for."}],["$","$L7",null,{"href":"/","className":"mt-8 text-sm font-medium text-zinc-900 dark:text-white","children":"Go back home"}]]}]}],"notFoundStyles":[],"styles":null}]}]}]}]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css","precedence":"next","crossOrigin":"$undefined"}]],"$L8"]]]]
9:I[4456,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","613","static/chunks/app/docs/dynamic-few-shot-text-classification/page-dea00a39e69c61bf.js"],"DocsHeader"]
a:I[7408,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","613","static/chunks/app/docs/dynamic-few-shot-text-classification/page-dea00a39e69c61bf.js"],"Fence"]
b:I[2553,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","613","static/chunks/app/docs/dynamic-few-shot-text-classification/page-dea00a39e69c61bf.js"],"PrevNextLinks"]
c:I[817,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","613","static/chunks/app/docs/dynamic-few-shot-text-classification/page-dea00a39e69c61bf.js"],"TableOfContents"]
2:[["$","div",null,{"className":"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16","children":[["$","article",null,{"children":[["$","$L9",null,{"title":"Dynamic few-shot text classification"}],["$","div",null,{"className":"prose prose-teal max-w-none dark:prose-invert dark:text-teal-200 prose-headings:scroll-mt-28 prose-headings:font-display prose-headings:font-normal lg:prose-headings:scroll-mt-[8.5rem] prose-lead:text-teal-500 dark:prose-lead:text-teal-400 prose-a:font-semibold dark:prose-a:text-teal-400 prose-a:no-underline prose-a:shadow-[inset_0_-2px_0_0_var(--tw-prose-background,#fff),inset_0_calc(-1*(var(--tw-prose-underline-size,4px)+2px))_0_0_var(--tw-prose-underline,theme(colors.teal.300))] hover:prose-a:[--tw-prose-underline-size:6px] dark:[--tw-prose-background:theme(colors.teal.900)] dark:prose-a:shadow-[inset_0_calc(-1*var(--tw-prose-underline-size,2px))_0_0_var(--tw-prose-underline,theme(colors.teal.800))] dark:hover:prose-a:[--tw-prose-underline-size:6px] prose-pre:rounded-xl prose-pre:bg-zinc-900 prose-pre:shadow-lg dark:prose-pre:bg-zinc-700/40 dark:prose-pre:shadow-none dark:prose-pre:ring-1 dark:prose-pre:ring-zinc-100/10 dark:prose-hr:border-teal-800","children":[["$","h2",null,{"id":"overview","children":"Overview"}],["$","p",null,{"children":["Dynamic Few-Shot Classification is an extension of ",["$","a",null,{"href":"/docs/few-shot-text-classification","children":"Few-Shot Text Classification"}]," that is more suitable for larger datasets. Instead of using a fixed set of examples for each class, it constructs a dynamic subset for each sample on the fly. This allows to efficiently utilize the limited contex window of the model and save the number of consumed tokens."]}],["$","p",null,{"children":["Dynamic Few-Shot Classification can be motivated from a variety of ways, from prior studies show-casing how kNN-Pretraining improves question answering capabilities (",["$","a",null,{"href":"https://arxiv.org/pdf/2110.04541","children":"[1]"}],"), or literature that shows, how dynamically adjusting the prompt template can improve performance (",["$","a",null,{"href":"https://arxiv.org/pdf/2108.13161","children":"[2]"}],"). In this sense it would also make sense, that dynamically adjusting the examples may be better for performance as well. However, hand-crafting each prompt is an exhaustive task (",["$","a",null,{"href":"https://arxiv.org/pdf/2104.08786","children":"[3]"}],") and not in the spirit of automated classification."]}],["$","p",null,{"children":"Let's consider a toy example, where the goal is to determine whether the review is about a book or a movie. The training dataset consists of 6 samples, 3 for each class:"}],["$","$La",null,{"language":"python","children":"X = [\n    \"I love reading science fiction novels, they transport me to other worlds.\", # example 1 - book - sci-fi\n    \"A good mystery novel keeps me guessing until the very end.\", # example 2 - book - mystery\n    \"Historical novels give me a sense of different times and places.\", # example 3 - book - historical\n    \"I love watching science fiction movies, they transport me to other galaxies.\", # example 4 - movie - sci-fi\n    \"A good mystery movie keeps me on the edge of my seat.\", # example 5 - movie - mystery\n    \"Historical movies offer a glimpse into the past.\", # example 6 - movie - historical\n]\n\ny = [\"books\", \"books\", \"books\", \"movies\", \"movies\", \"movies\"]\n"}],["$","p",null,{"children":"Now let's say we want to classify the following review:"}],["$","$La",null,{"language":"bash","children":"I have fallen deeply in love with this sci-fi book; its unique blend of science and fiction has me spellbound.\n"}],["$","p",null,{"children":"Since the query is about a sci-fi book, we would like to only examples 1 and 4 to be used for classification, since they are the most relevant. If we use the dynamic few-shot classifier with 1 example per class, and investigate which examples were selected, we can see that the model successfully identified examples 1 and 4 as the most relevant ones:"}],["$","$La",null,{"language":"python","children":"from skollama.models.ollama.classification.few_shot import DynamicFewShotOllamaClassifier\n\nquery = \"I have fallen deeply in love with this sci-fi book; its unique blend of science and fiction has me spellbound.\"\n\nclf = DynamicFewShotOllamaClassifier(n_examples=1).fit(X,y)\n\nprompt = clf._get_prompt(query)\nprint(prompt)\n"}],["$","$La",null,{"language":"bash","children":"...\n\nSample input:\n\"I love reading science fiction novels, they transport me to other worlds.\"\n\nSample target: books\n\n\nSample input:\n\"I love watching science fiction movies, they transport me to other galaxies.\"\n\nSample target: movies\n\n...\n"}],["$","p",null,{"children":"This is achieved by adding a KNN search algorithm as an additional preprocessor. If we assume that the most relevant examples are the closest ones in space, then the problem reduces to a nearest neighbors search and can be tackled in three steps:"}],["$","ol",null,{"children":[["$","li",null,{"children":[["$","strong",null,{"children":"Vectorization:"}]," ","Before doing the nearest neighbors search, the training set must be vectorized using an embedding model. By default ",["$","code",null,{"children":"Scikit-Ollama"}]," uses ",["$","code",null,{"children":"nomic-embed-text"}]," for embedding and ",["$","code",null,{"children":"llama3"}]," for chat completion."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Index construction"}]," using an arbitrary nearest neighbors search algorithm. The index allows to efficiently retrieve the nearest neighbors from each class. Currently ",["$","a",null,{"href":"https://scikit-learn.org/stable/modules/neighbors.html","children":"Scikit-Learn KNN"}]," and ",["$","a",null,{"href":"https://github.com/spotify/annoy","children":"Annoy"}]," are supported, but it is possible to add a custom index as well."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Balanced sampling:"}]," ","The last thing to be accounted for is a class balancing. If only N nearest neighbors are selected for a few-shot prompting, there is a very high risk that some of the classes will be underrepresented or missing completely. To mitigate this issue, instead of creating a single index, the training data is partitioned by class. In this way, we are able to sample N examples from each class, ensuring the equal representation of each class."]}]]}],["$","hr",null,{}],["$","h2",null,{"id":"api-reference","children":"API Reference"}],["$","p",null,{"children":"The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier."}],["$","h3",null,{"id":"dynamic-few-shot-ollama-classifier","children":"DynamicFewShotOllamaClassifier"}],["$","$La",null,{"language":"python","children":"from skollama.models.ollama.classification.few_shot import DynamicFewShotOllamaClassifier\n"}],["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"scope":"col","children":["$","strong",null,{"children":"Parameter"}]}],["$","th",null,{"scope":"col","children":["$","strong",null,{"children":"Type"}]}],["$","th",null,{"scope":"col","children":["$","strong",null,{"children":"Description"}]}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","code",null,{"children":"model"}]}],["$","td",null,{"children":["$","code",null,{"children":"str"}]}],["$","td",null,{"children":"Model to use, by default \"gpt-3.5-turbo\"."}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","code",null,{"children":"default_label"}]}],["$","td",null,{"children":["$","code",null,{"children":"str"}]}],["$","td",null,{"children":"Default label for failed prediction; if \"Random\" -> selects randomly based on class frequencies, by default \"Random\"."}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","code",null,{"children":"prompt_template"}]}],["$","td",null,{"children":["$","code",null,{"children":"Optional[str]"}]}],["$","td",null,{"children":"Custom prompt template to use, by default None."}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","code",null,{"children":"key"}]}],["$","td",null,{"children":["$","code",null,{"children":"Optional[str]"}]}],["$","td",null,{"children":"Estimator-specific API key; if None, retrieved from the global config, by default None."}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","code",null,{"children":"n_examples"}]}],["$","td",null,{"children":["$","code",null,{"children":"int"}]}],["$","td",null,{"children":"Number of closest examples per class to be retrieved, by default 3."}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","code",null,{"children":"memory_index"}]}],["$","td",null,{"children":["$","code",null,{"children":"Optional[IndexConstructor]"}]}],["$","td",null,{"children":["Custom memory index, for details check ",["$","code",null,{"children":"skllm.memory"}]," submodule, by default None."]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","code",null,{"children":"vectorizer"}]}],["$","td",null,{"children":["$","code",null,{"children":"Optional[BaseVectorizer]"}]}],["$","td",null,{"children":["Scikit-LLM vectorizer; if None, ",["$","code",null,{"children":"GPTVectorizer"}]," is used, by default None."]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","code",null,{"children":"metric"}]}],["$","td",null,{"children":["$","code",null,{"children":"Optional[str]"}]}],["$","td",null,{"children":"Metric used for similarity search by the memory_index, by default \"euclidean\""}]]}]]}]]}]]}]]}],["$","$Lb",null,{}]]}],["$","$Lc",null,{"tableOfContents":[{"level":2,"id":"overview","title":"Overview","children":[]},{"level":2,"id":"api-reference","title":"API Reference","children":[{"level":3,"id":"dynamic-few-shot-ollama-classifier","title":"DynamicFewShotOllamaClassifier"}]}]}]]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Dynamic few-shot text classification - Docs"}],["$","meta","3",{"name":"description","content":"Learn about dynamic few-shot text classification."}],["$","link","4",{"rel":"icon","href":"/bayreuth_ai_association/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","5",{"name":"next-size-adjust"}]]
1:null
