<!DOCTYPE html><html lang="en" class="h-full antialiased __variable_01f60e __variable_a0637f"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/bayreuth_ai_association/_next/static/media/8935352d0bfcf3a9-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/bayreuth_ai_association/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/bayreuth_ai_association/_next/static/chunks/webpack-ce4d567c9f2a94d3.js"/><script src="/bayreuth_ai_association/_next/static/chunks/fd9d1056-96f96ef28edce221.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/23-80ca076b01c7bb1f.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/main-app-28cdedace4fab0f2.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/231-829f56fdb7c75283.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/55-c141fc066ff4a19d.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/472-cd817ab664ce0632.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/app/layout-1cfcfb656b6cf4a5.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/app/docs/dynamic-few-shot-text-classification/page-dea00a39e69c61bf.js" async=""></script><title>Dynamic few-shot text classification - Docs</title><meta name="description" content="Learn about dynamic few-shot text classification."/><link rel="icon" href="/bayreuth_ai_association/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/bayreuth_ai_association/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="flex min-h-full bg-white dark:bg-zinc-900"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="flex w-full flex-col"><header class="sticky top-0 z-50 flex flex-none flex-wrap items-center justify-between bg-white px-4 py-5 shadow-md shadow-zinc-900/5 transition duration-500 sm:px-6 lg:px-8 dark:shadow-none dark:bg-transparent"><div class="mr-6 flex lg:hidden"><button type="button" class="relative" aria-label="Open navigation"><svg aria-hidden="true" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke-linecap="round" class="h-6 w-6 stroke-zinc-500"><path d="M4 7h16M4 12h16M4 17h16"></path></svg></button><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div></div><div class="relative flex flex-grow basis-0 items-center"><a aria-label="Home page" href="/bayreuth_ai_association"><p>scikit-ollama</p></a></div><div class="-my-5 mr-6 sm:mr-8 md:mr-0"><button type="button" class="group flex h-6 w-6 items-center justify-center sm:justify-start md:h-auto md:w-80 md:flex-none md:rounded-lg md:py-2.5 md:pl-4 md:pr-3.5 md:text-sm md:ring-1 md:ring-zinc-200 md:hover:ring-zinc-300 lg:w-96 dark:md:bg-zinc-800/75 dark:md:ring-inset dark:md:ring-white/5 dark:md:hover:bg-zinc-700/40 dark:md:hover:ring-zinc-500"><svg aria-hidden="true" viewBox="0 0 20 20" class="h-5 w-5 flex-none fill-zinc-400 group-hover:fill-zinc-500 md:group-hover:fill-zinc-400 dark:fill-zinc-500"><path d="M16.293 17.707a1 1 0 0 0 1.414-1.414l-1.414 1.414ZM9 14a5 5 0 0 1-5-5H2a7 7 0 0 0 7 7v-2ZM4 9a5 5 0 0 1 5-5V2a7 7 0 0 0-7 7h2Zm5-5a5 5 0 0 1 5 5h2a7 7 0 0 0-7-7v2Zm8.707 12.293-3.757-3.757-1.414 1.414 3.757 3.757 1.414-1.414ZM14 9a4.98 4.98 0 0 1-1.464 3.536l1.414 1.414A6.98 6.98 0 0 0 16 9h-2Zm-1.464 3.536A4.98 4.98 0 0 1 9 14v2a6.98 6.98 0 0 0 4.95-2.05l-1.414-1.414Z"></path></svg><span class="sr-only md:not-sr-only md:ml-2 md:text-zinc-500 md:dark:text-zinc-400">Search docs</span></button><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div></div><div class="relative flex basis-0 justify-end gap-6 sm:gap-8 md:flex-grow"><div class="h-6 w-6"></div><a class="group" aria-label="GitHub" href="https://github.com/AndreasKarasenko/scikit-ollama"><svg aria-hidden="true" viewBox="0 0 16 16" class="h-6 w-6 fill-zinc-400 group-hover:fill-zinc-500 dark:group-hover:fill-zinc-300"><path d="M8 0C3.58 0 0 3.58 0 8C0 11.54 2.29 14.53 5.47 15.59C5.87 15.66 6.02 15.42 6.02 15.21C6.02 15.02 6.01 14.39 6.01 13.72C4 14.09 3.48 13.23 3.32 12.78C3.23 12.55 2.84 11.84 2.5 11.65C2.22 11.5 1.82 11.13 2.49 11.12C3.12 11.11 3.57 11.7 3.72 11.94C4.44 13.15 5.59 12.81 6.05 12.6C6.12 12.08 6.33 11.73 6.56 11.53C4.78 11.33 2.92 10.64 2.92 7.58C2.92 6.71 3.23 5.99 3.74 5.43C3.66 5.23 3.38 4.41 3.82 3.31C3.82 3.31 4.49 3.1 6.02 4.13C6.66 3.95 7.34 3.86 8.02 3.86C8.7 3.86 9.38 3.95 10.02 4.13C11.55 3.09 12.22 3.31 12.22 3.31C12.66 4.41 12.38 5.23 12.3 5.43C12.81 5.99 13.12 6.7 13.12 7.58C13.12 10.65 11.25 11.33 9.47 11.53C9.76 11.78 10.01 12.26 10.01 13.01C10.01 14.08 10 14.94 10 15.21C10 15.42 10.15 15.67 10.55 15.59C13.71 14.53 16 11.53 16 8C16 3.58 12.42 0 8 0Z"></path></svg></a></div></header><div id="cde"></div><div class="relative mx-auto flex w-full max-w-8xl flex-auto justify-center sm:px-2 lg:px-8 xl:px-12"><div class="hidden lg:relative lg:block lg:flex-none"><div class="absolute inset-y-0 right-0 w-[50vw] bg-zinc-50 dark:hidden"></div><div class="absolute bottom-0 right-0 top-16 hidden h-12 w-px bg-gradient-to-t from-zinc-800 dark:block"></div><div class="absolute bottom-0 right-0 top-28 hidden w-px bg-zinc-800 dark:block"></div><div class="sticky top-[4.75rem] -ml-0.5 h-[calc(100vh-4.75rem)] w-64 overflow-y-auto overflow-x-hidden py-16 pl-0.5 pr-8 xl:w-72 xl:pr-16"><nav class="text-base lg:text-sm"><ul role="list" class="space-y-9"><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Introduction</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association">Home</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/about-our-meetings">About our meetings</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Text classification</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/zero-shot-text-classification">Zero-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/few-shot-text-classification">Few-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full font-semibold text-teal-500 before:bg-teal-500 dark:text-teal-200" href="/bayreuth_ai_association/docs/dynamic-few-shot-text-classification">Dynamic few-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/tunable-text-classification">Tunable text classification</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Text-to-text modelling</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-summarization">Text summarization</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-translation">Text translation</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/tunable-text-to-text">Tunable text-to-text</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Resources</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-vectorization">Overview</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Contributing</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/how-to-contribute">How to contribute</a></li></ul></li></ul></nav></div></div><div class="min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16"><article><header class="mb-9 space-y-1"><p class="font-display text-sm font-medium text-teal-800 dark:text-teal-200">Text classification</p><h1 class="font-display text-3xl tracking-tight text-zinc-900 dark:text-white">Dynamic few-shot text classification</h1></header><div class="prose prose-teal max-w-none dark:prose-invert dark:text-teal-200 prose-headings:scroll-mt-28 prose-headings:font-display prose-headings:font-normal lg:prose-headings:scroll-mt-[8.5rem] prose-lead:text-teal-500 dark:prose-lead:text-teal-400 prose-a:font-semibold dark:prose-a:text-teal-400 prose-a:no-underline prose-a:shadow-[inset_0_-2px_0_0_var(--tw-prose-background,#fff),inset_0_calc(-1*(var(--tw-prose-underline-size,4px)+2px))_0_0_var(--tw-prose-underline,theme(colors.teal.300))] hover:prose-a:[--tw-prose-underline-size:6px] dark:[--tw-prose-background:theme(colors.teal.900)] dark:prose-a:shadow-[inset_0_calc(-1*var(--tw-prose-underline-size,2px))_0_0_var(--tw-prose-underline,theme(colors.teal.800))] dark:hover:prose-a:[--tw-prose-underline-size:6px] prose-pre:rounded-xl prose-pre:bg-zinc-900 prose-pre:shadow-lg dark:prose-pre:bg-zinc-700/40 dark:prose-pre:shadow-none dark:prose-pre:ring-1 dark:prose-pre:ring-zinc-100/10 dark:prose-hr:border-teal-800"><h2 id="overview">Overview</h2><p>Dynamic Few-Shot Classification is an extension of <a href="/docs/few-shot-text-classification">Few-Shot Text Classification</a> that is more suitable for larger datasets. Instead of using a fixed set of examples for each class, it constructs a dynamic subset for each sample on the fly. This allows to efficiently utilize the limited contex window of the model and save the number of consumed tokens.</p><p>Dynamic Few-Shot Classification can be motivated from a variety of ways, from prior studies show-casing how kNN-Pretraining improves question answering capabilities (<a href="https://arxiv.org/pdf/2110.04541">[1]</a>), or literature that shows, how dynamically adjusting the prompt template can improve performance (<a href="https://arxiv.org/pdf/2108.13161">[2]</a>). In this sense it would also make sense, that dynamically adjusting the examples may be better for performance as well. However, hand-crafting each prompt is an exhaustive task (<a href="https://arxiv.org/pdf/2104.08786">[3]</a>) and not in the spirit of automated classification.</p><p>Let&#x27;s consider a toy example, where the goal is to determine whether the review is about a book or a movie. The training dataset consists of 6 samples, 3 for each class:</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token plain">X </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token string" style="color:#fdba74">&quot;I love reading science fiction novels, they transport me to other worlds.&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token comment" style="color:#6a9955"># example 1 - book - sci-fi</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token string" style="color:#fdba74">&quot;A good mystery novel keeps me guessing until the very end.&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token comment" style="color:#6a9955"># example 2 - book - mystery</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token string" style="color:#fdba74">&quot;Historical novels give me a sense of different times and places.&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token comment" style="color:#6a9955"># example 3 - book - historical</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token string" style="color:#fdba74">&quot;I love watching science fiction movies, they transport me to other galaxies.&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token comment" style="color:#6a9955"># example 4 - movie - sci-fi</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token string" style="color:#fdba74">&quot;A good mystery movie keeps me on the edge of my seat.&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token comment" style="color:#6a9955"># example 5 - movie - mystery</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token string" style="color:#fdba74">&quot;Historical movies offer a glimpse into the past.&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token comment" style="color:#6a9955"># example 6 - movie - historical</span><span class="token plain"></span>
<span class="token plain"></span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token plain"></span>
<!-- -->
<span class="token plain">y </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;books&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;books&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;books&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;movies&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;movies&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;movies&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span>
</code></pre><p>Now let&#x27;s say we want to classify the following review:</p><pre class="prism-code language-bash" style="color:#e4e4e7;font-style:normal"><code><span class="token plain">I have fallen deeply in love with this sci-fi book; its unique blend of science and fiction has me spellbound.</span>
</code></pre><p>Since the query is about a sci-fi book, we would like to only examples 1 and 4 to be used for classification, since they are the most relevant. If we use the dynamic few-shot classifier with 1 example per class, and investigate which examples were selected, we can see that the model successfully identified examples 1 and 4 as the most relevant ones:</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> skollama</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">models</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">ollama</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">classification</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">few_shot </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> DynamicFewShotOllamaClassifier</span>
<!-- -->
<span class="token plain">query </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;I have fallen deeply in love with this sci-fi book; its unique blend of science and fiction has me spellbound.&quot;</span><span class="token plain"></span>
<!-- -->
<span class="token plain">clf </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> DynamicFewShotOllamaClassifier</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">n_examples</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">1</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">fit</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">X</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain">y</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain">prompt </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> clf</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">_get_prompt</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">query</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">print</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">prompt</span><span class="token punctuation" style="color:#a1a1aa">)</span>
</code></pre><pre class="prism-code language-bash" style="color:#e4e4e7;font-style:normal"><code><span class="token plain">...</span>
<!-- -->
<span class="token plain">Sample input:</span>
<span class="token plain">&quot;I love reading science fiction novels, they transport me to other worlds.&quot;</span>
<!-- -->
<span class="token plain">Sample target: books</span>
<!-- -->
<!-- -->
<span class="token plain">Sample input:</span>
<span class="token plain">&quot;I love watching science fiction movies, they transport me to other galaxies.&quot;</span>
<!-- -->
<span class="token plain">Sample target: movies</span>
<!-- -->
<span class="token plain">...</span>
</code></pre><p>This is achieved by adding a KNN search algorithm as an additional preprocessor. If we assume that the most relevant examples are the closest ones in space, then the problem reduces to a nearest neighbors search and can be tackled in three steps:</p><ol><li><strong>Vectorization:</strong> <!-- -->Before doing the nearest neighbors search, the training set must be vectorized using an embedding model. By default <code>Scikit-Ollama</code> uses <code>nomic-embed-text</code> for embedding and <code>llama3</code> for chat completion.</li><li><strong>Index construction</strong> using an arbitrary nearest neighbors search algorithm. The index allows to efficiently retrieve the nearest neighbors from each class. Currently <a href="https://scikit-learn.org/stable/modules/neighbors.html">Scikit-Learn KNN</a> and <a href="https://github.com/spotify/annoy">Annoy</a> are supported, but it is possible to add a custom index as well.</li><li><strong>Balanced sampling:</strong> <!-- -->The last thing to be accounted for is a class balancing. If only N nearest neighbors are selected for a few-shot prompting, there is a very high risk that some of the classes will be underrepresented or missing completely. To mitigate this issue, instead of creating a single index, the training data is partitioned by class. In this way, we are able to sample N examples from each class, ensuring the equal representation of each class.</li></ol><hr/><h2 id="api-reference">API Reference</h2><p>The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.</p><h3 id="dynamic-few-shot-ollama-classifier">DynamicFewShotOllamaClassifier</h3><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> skollama</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">models</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">ollama</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">classification</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">few_shot </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> DynamicFewShotOllamaClassifier</span>
</code></pre><table><thead><tr><th scope="col"><strong>Parameter</strong></th><th scope="col"><strong>Type</strong></th><th scope="col"><strong>Description</strong></th></tr></thead><tbody><tr><td><code>model</code></td><td><code>str</code></td><td>Model to use, by default &quot;gpt-3.5-turbo&quot;.</td></tr><tr><td><code>default_label</code></td><td><code>str</code></td><td>Default label for failed prediction; if &quot;Random&quot; -&gt; selects randomly based on class frequencies, by default &quot;Random&quot;.</td></tr><tr><td><code>prompt_template</code></td><td><code>Optional[str]</code></td><td>Custom prompt template to use, by default None.</td></tr><tr><td><code>key</code></td><td><code>Optional[str]</code></td><td>Estimator-specific API key; if None, retrieved from the global config, by default None.</td></tr><tr><td><code>n_examples</code></td><td><code>int</code></td><td>Number of closest examples per class to be retrieved, by default 3.</td></tr><tr><td><code>memory_index</code></td><td><code>Optional[IndexConstructor]</code></td><td>Custom memory index, for details check <code>skllm.memory</code> submodule, by default None.</td></tr><tr><td><code>vectorizer</code></td><td><code>Optional[BaseVectorizer]</code></td><td>Scikit-LLM vectorizer; if None, <code>GPTVectorizer</code> is used, by default None.</td></tr><tr><td><code>metric</code></td><td><code>Optional[str]</code></td><td>Metric used for similarity search by the memory_index, by default &quot;euclidean&quot;</td></tr></tbody></table></div></article><dl class="mt-12 flex border-t border-zinc-200 pt-6 dark:border-zinc-800"><div><dt class="font-display text-sm font-medium text-zinc-900 dark:text-white">Previous</dt><dd class="mt-1"><a class="flex items-center gap-x-1 text-base font-semibold text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300 flex-row-reverse" href="/bayreuth_ai_association/docs/few-shot-text-classification">Few-shot text classification<svg viewBox="0 0 16 16" aria-hidden="true" class="h-4 w-4 flex-none fill-current -scale-x-100"><path d="m9.182 13.423-1.17-1.16 3.505-3.505H3V7.065h8.517l-3.506-3.5L9.181 2.4l5.512 5.511-5.511 5.512Z"></path></svg></a></dd></div><div class="ml-auto text-right"><dt class="font-display text-sm font-medium text-zinc-900 dark:text-white">Next</dt><dd class="mt-1"><a class="flex items-center gap-x-1 text-base font-semibold text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/tunable-text-classification">Tunable text classification<svg viewBox="0 0 16 16" aria-hidden="true" class="h-4 w-4 flex-none fill-current"><path d="m9.182 13.423-1.17-1.16 3.505-3.505H3V7.065h8.517l-3.506-3.5L9.181 2.4l5.512 5.511-5.511 5.512Z"></path></svg></a></dd></div></dl></div><div class="hidden xl:sticky xl:top-[4.75rem] xl:-mr-6 xl:block xl:h-[calc(100vh-4.75rem)] xl:flex-none xl:overflow-y-auto xl:py-16 xl:pr-6"><nav aria-labelledby="on-this-page-title" class="w-56"><h2 id="on-this-page-title" class="font-display text-sm font-medium text-zinc-900 dark:text-white">On this page</h2><ol role="list" class="mt-4 space-y-3 text-sm"><li><h3><a class="text-teal-500 dark:text-teal-200" href="#overview">Overview</a></h3></li><li><h3><a class="font-normal text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="#api-reference">API Reference</a></h3><ol role="list" class="mt-2 space-y-3 pl-5 text-zinc-500 dark:text-zinc-400"><li><a class="hover:text-zinc-600 dark:hover:text-zinc-300" href="#dynamic-few-shot-ollama-classifier">DynamicFewShotOllamaClassifier</a></li></ol></li></ol></nav></div></div></div><script src="/bayreuth_ai_association/_next/static/chunks/webpack-ce4d567c9f2a94d3.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/bayreuth_ai_association/_next/static/media/8935352d0bfcf3a9-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/bayreuth_ai_association/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"4:I[5751,[],\"\"]\n7:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[1747,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"472\",\"static/chunks/472-cd817ab664ce0632.js\",\"185\",\"static/chunks/app/layout-1cfcfb656b6cf4a5.js\"],\"Providers\"]\na:I[1227,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"472\",\"static/chunks/472-cd817ab664ce0632.js\",\"185\",\"static/chunks/app/layout-1cfcfb656b6cf4a5.js\"],\"Layout\"]\nb:I[231,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"613\",\"static/chunks/app/docs/dynamic-few-shot-text-classification/page-dea00a39e69c61bf.js\"],\"\"]\nd:I[6130,[],\"\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L4\",null,{\"buildId\":\"KteObhGWKN4CvpFO1QXQU\",\"assetPrefix\":\"/bayreuth_ai_association\",\"initialCanonicalUrl\":\"/docs/dynamic-few-shot-text-classification\",\"initialTree\":[\"\",{\"children\":[\"docs\",{\"children\":[\"dynamic-few-shot-text-classification\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"docs\",{\"children\":[\"dynamic-few-shot-text-classification\",{\"children\":[\"__PAGE__\",{},[[\"$L5\",\"$L6\"],null],null]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"docs\",\"children\",\"dynamic-few-shot-text-classification\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"docs\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"h-full antialiased __variable_01f60e __variable_a0637f\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"flex min-h-full bg-white dark:bg-zinc-900\",\"children\":[\"$\",\"$L9\",null,{\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex h-full flex-col items-center justify-center text-center\",\"children\":[[\"$\",\"p\",null,{\"className\":\"font-display text-sm font-medium text-zinc-900 dark:text-white\",\"children\":\"404\"}],[\"$\",\"h1\",null,{\"className\":\"mt-3 font-display text-3xl tracking-tight text-zinc-900 dark:text-white\",\"children\":\"Page not found\"}],[\"$\",\"p\",null,{\"className\":\"mt-2 text-sm text-zinc-500 dark:text-zinc-400\",\"children\":\"Sorry, we couldn’t find the page you’re looking for.\"}],[\"$\",\"$Lb\",null,{\"href\":\"/\",\"className\":\"mt-8 text-sm font-medium text-zinc-900 dark:text-white\",\"children\":\"Go back home\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}]}]}]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"f:I[4456,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"613\",\"static/chunks/app/docs/dynamic-few-shot-text-classification/page-dea00a39e69c61bf.js\"],\"DocsHeader\"]\n10:I[7408,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"613\",\"static/chunks/app/docs/dynamic-few-shot-text-classification/page-dea00a39e69c61bf.js\"],\"Fence\"]\n11:I[2553,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"613\",\"static/chunks/app/docs/dynamic-few-shot-text-classification/page-dea00a39e69c61bf.js\"],\"PrevNextLinks\"]\n12:I[817,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"613\",\"static/chunks/app/docs/dynamic-few-shot-text-classification/page-dea00a39e69c61bf.js\"],\"TableOfContents\"]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"div\",null,{\"className\":\"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16\",\"children\":[[\"$\",\"article\",null,{\"children\":[[\"$\",\"$Lf\",null,{\"title\":\"Dynamic few-shot text classification\"}],[\"$\",\"div\",null,{\"className\":\"prose prose-teal max-w-none dark:prose-invert dark:text-teal-200 prose-headings:scroll-mt-28 prose-headings:font-display prose-headings:font-normal lg:prose-headings:scroll-mt-[8.5rem] prose-lead:text-teal-500 dark:prose-lead:text-teal-400 prose-a:font-semibold dark:prose-a:text-teal-400 prose-a:no-underline prose-a:shadow-[inset_0_-2px_0_0_var(--tw-prose-background,#fff),inset_0_calc(-1*(var(--tw-prose-underline-size,4px)+2px))_0_0_var(--tw-prose-underline,theme(colors.teal.300))] hover:prose-a:[--tw-prose-underline-size:6px] dark:[--tw-prose-background:theme(colors.teal.900)] dark:prose-a:shadow-[inset_0_calc(-1*var(--tw-prose-underline-size,2px))_0_0_var(--tw-prose-underline,theme(colors.teal.800))] dark:hover:prose-a:[--tw-prose-underline-size:6px] prose-pre:rounded-xl prose-pre:bg-zinc-900 prose-pre:shadow-lg dark:prose-pre:bg-zinc-700/40 dark:prose-pre:shadow-none dark:prose-pre:ring-1 dark:prose-pre:ring-zinc-100/10 dark:prose-hr:border-teal-800\",\"children\":[[\"$\",\"h2\",null,{\"id\":\"overview\",\"children\":\"Overview\"}],[\"$\",\"p\",null,{\"children\":[\"Dynamic Few-Shot Classification is an extension of \",[\"$\",\"a\",null,{\"href\":\"/docs/few-shot-text-classification\",\"children\":\"Few-Shot Text Classification\"}],\" that is more suitable for larger datasets. Instead of using a fixed set of examples for each class, it constructs a dynamic subset for each sample on the fly. This allows to efficiently utilize the limited contex window of the model and save the number of consumed tokens.\"]}],[\"$\",\"p\",null,{\"children\":[\"Dynamic Few-Shot Classification can be motivated from a variety of ways, from prior studies show-casing how kNN-Pretraining improves question answering capabilities (\",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2110.04541\",\"children\":\"[1]\"}],\"), or literature that shows, how dynamically adjusting the prompt template can improve performance (\",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2108.13161\",\"children\":\"[2]\"}],\"). In this sense it would also make sense, that dynamically adjusting the examples may be better for performance as well. However, hand-crafting each prompt is an exhaustive task (\",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2104.08786\",\"children\":\"[3]\"}],\") and not in the spirit of automated classification.\"]}],[\"$\",\"p\",null,{\"children\":\"Let's consider a toy example, where the goal is to determine whether the review is about a book or a movie. The training dataset consists of 6 samples, 3 for each class:\"}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"X = [\\n    \\\"I love reading science fiction novels, they transport me to other worlds.\\\", # example 1 - book - sci-fi\\n    \\\"A good mystery novel keeps me guessing until the very end.\\\", # example 2 - book - mystery\\n    \\\"Historical novels give me a sense of different times and places.\\\", # example 3 - book - historical\\n    \\\"I love watching science fiction movies, they transport me to other galaxies.\\\", # example 4 - movie - sci-fi\\n    \\\"A good mystery movie keeps me on the edge of my seat.\\\", # example 5 - movie - mystery\\n    \\\"Historical movies offer a glimpse into the past.\\\", # example 6 - movie - historical\\n]\\n\\ny = [\\\"books\\\", \\\"books\\\", \\\"books\\\", \\\"movies\\\", \\\"movies\\\", \\\"movies\\\"]\\n\"}],[\"$\",\"p\",null,{\"children\":\"Now let's say we want to classify the following review:\"}],[\"$\",\"$L10\",null,{\"language\":\"bash\",\"children\":\"I have fallen deeply in love with this sci-fi book; its unique blend of science and fiction has me spellbound.\\n\"}],[\"$\",\"p\",null,{\"children\":\"Since the query is about a sci-fi book, we would like to only examples 1 and 4 to be used for classification, since they are the most relevant. If we use the dynamic few-shot classifier with 1 example per class, and investigate which examples were selected, we can see that the model successfully identified examples 1 and 4 as the most relevant ones:\"}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"from skollama.models.ollama.classification.few_shot import DynamicFewShotOllamaClassifier\\n\\nquery = \\\"I have fallen deeply in love with this sci-fi book; its unique blend of science and fiction has me spellbound.\\\"\\n\\nclf = DynamicFewShotOllamaClassifier(n_examples=1).fit(X,y)\\n\\nprompt = clf._get_prompt(query)\\nprint(prompt)\\n\"}],[\"$\",\"$L10\",null,{\"language\":\"bash\",\"children\":\"...\\n\\nSample input:\\n\\\"I love reading science fiction novels, they transport me to other worlds.\\\"\\n\\nSample target: books\\n\\n\\nSample input:\\n\\\"I love watching science fiction movies, they transport me to other galaxies.\\\"\\n\\nSample target: movies\\n\\n...\\n\"}],[\"$\",\"p\",null,{\"children\":\"This is achieved by adding a KNN search algorithm as an additional preprocessor. If we assume that the most relevant examples are the closest ones in space, then the problem reduces to a nearest neighbors search and can be tackled in three steps:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Vectorization:\"}],\" \",\"Before doing the nearest neighbors search, the training set must be vectorized using an embedding model. By default \",[\"$\",\"code\",null,{\"children\":\"Scikit-Ollama\"}],\" uses \",[\"$\",\"code\",null,{\"children\":\"nomic-embed-text\"}],\" for embedding and \",[\"$\",\"code\",null,{\"children\":\"llama3\"}],\" for chat completion.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Index construction\"}],\" using an arbitrary nearest neighbors search algorithm. The index allows to efficiently retrieve the nearest neighbors from each class. Currently \",[\"$\",\"a\",null,{\"href\":\"https://scikit-learn.org/stable/modules/neighbors.html\",\"children\":\"Scikit-Learn KNN\"}],\" and \",[\"$\",\"a\",null,{\"href\":\"https://github.com/spotify/annoy\",\"children\":\"Annoy\"}],\" are supported, but it is possible to add a custom index as well.\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Balanced sampling:\"}],\" \",\"The last thing to be accounted for is a class balancing. If only N nearest neighbors are selected for a few-shot prompting, there is a very high risk that some of the classes will be underrepresented or missing completely. To mitigate this issue, instead of creating a single index, the training data is partitioned by class. In this way, we are able to sample N examples from each class, ensuring the equal representation of each class.\"]}]]}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"id\":\"api-reference\",\"children\":\"API Reference\"}],[\"$\",\"p\",null,{\"children\":\"The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.\"}],[\"$\",\"h3\",null,{\"id\":\"dynamic-few-shot-ollama-classifier\",\"children\":\"DynamicFewShotOllamaClassifier\"}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"from skollama.models.ollama.classification.few_shot import DynamicFewShotOllamaClassifier\\n\"}],[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"scope\":\"col\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Parameter\"}]}],[\"$\",\"th\",null,{\"scope\":\"col\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Type\"}]}],[\"$\",\"th\",null,{\"scope\":\"col\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Description\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"model\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"str\"}]}],[\"$\",\"td\",null,{\"children\":\"Model to use, by default \\\"gpt-3.5-turbo\\\".\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"default_label\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"str\"}]}],[\"$\",\"td\",null,{\"children\":\"Default label for failed prediction; if \\\"Random\\\" -\u003e selects randomly based on class frequencies, by default \\\"Random\\\".\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"prompt_template\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Optional[str]\"}]}],[\"$\",\"td\",null,{\"children\":\"Custom prompt template to use, by default None.\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"key\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Optional[str]\"}]}],[\"$\",\"td\",null,{\"children\":\"Estimator-specific API key; if None, retrieved from the global config, by default None.\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"n_examples\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"int\"}]}],[\"$\",\"td\",null,{\"children\":\"Number of closest examples per class to be retrieved, by default 3.\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"memory_index\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Optional[IndexConstructor]\"}]}],[\"$\",\"td\",null,{\"children\":[\"Custom memory index, for details check \",[\"$\",\"code\",null,{\"children\":\"skllm.memory\"}],\" submodule, by default None.\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"vectorizer\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Optional[BaseVectorizer]\"}]}],[\"$\",\"td\",null,{\"children\":[\"Scikit-LLM vectorizer; if None, \",[\"$\",\"code\",null,{\"children\":\"GPTVectorizer\"}],\" is used, by default None.\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"metric\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Optional[str]\"}]}],[\"$\",\"td\",null,{\"children\":\"Metric used for similarity search by the memory_index, by default \\\"euclidean\\\"\"}]]}]]}]]}]]}]]}],[\"$\",\"$L11\",null,{}]]}],[\"$\",\"$L12\",null,{\"tableOfContents\":[{\"level\":2,\"id\":\"overview\",\"title\":\"Overview\",\"children\":[]},{\"level\":2,\"id\":\"api-reference\",\"title\":\"API Reference\",\"children\":[{\"level\":3,\"id\":\"dynamic-few-shot-ollama-classifier\",\"title\":\"DynamicFewShotOllamaClassifier\"}]}]}]]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Dynamic few-shot text classification - Docs\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Learn about dynamic few-shot text classification.\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/bayreuth_ai_association/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\n5:null\n"])</script></body></html>