<!DOCTYPE html><html lang="en" class="h-full antialiased __variable_01f60e __variable_a0637f"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/bayreuth_ai_association/_next/static/media/8935352d0bfcf3a9-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/bayreuth_ai_association/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/ce8e33447a34db0259f888d39c58256c2cbf43b1/dingo_rag_use_case.svg"/><link rel="preload" as="image" href="https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/7f8f41d5bf00a23638b8958cc970281857a43a6f/dingo_app_architecture.svg"/><link rel="preload" as="image" href="https://i.ibb.co/rQm0m41/Dingo-Chatbot.png"/><link rel="stylesheet" href="/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/bayreuth_ai_association/_next/static/chunks/webpack-ce4d567c9f2a94d3.js"/><script src="/bayreuth_ai_association/_next/static/chunks/fd9d1056-96f96ef28edce221.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/23-80ca076b01c7bb1f.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/main-app-28cdedace4fab0f2.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/231-829f56fdb7c75283.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/55-c141fc066ff4a19d.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/472-cd817ab664ce0632.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/app/layout-1cfcfb656b6cf4a5.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/app/docs/use-cases-rag-chatbot/page-4b9d8097182892f7.js" async=""></script><title>Building a RAG chatbot - Docs</title><meta name="description" content="Learn how to build a RAG chat bot."/><link rel="icon" href="/bayreuth_ai_association/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/bayreuth_ai_association/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="flex min-h-full bg-white dark:bg-zinc-900"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="flex w-full flex-col"><header class="sticky top-0 z-50 flex flex-none flex-wrap items-center justify-between bg-white px-4 py-5 shadow-md shadow-zinc-900/5 transition duration-500 sm:px-6 lg:px-8 dark:shadow-none dark:bg-transparent"><div class="mr-6 flex lg:hidden"><button type="button" class="relative" aria-label="Open navigation"><svg aria-hidden="true" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke-linecap="round" class="h-6 w-6 stroke-zinc-500"><path d="M4 7h16M4 12h16M4 17h16"></path></svg></button><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div></div><div class="relative flex flex-grow basis-0 items-center"><a aria-label="Home page" href="/bayreuth_ai_association"><p>scikit-ollama</p></a></div><div class="-my-5 mr-6 sm:mr-8 md:mr-0"><button type="button" class="group flex h-6 w-6 items-center justify-center sm:justify-start md:h-auto md:w-80 md:flex-none md:rounded-lg md:py-2.5 md:pl-4 md:pr-3.5 md:text-sm md:ring-1 md:ring-zinc-200 md:hover:ring-zinc-300 lg:w-96 dark:md:bg-zinc-800/75 dark:md:ring-inset dark:md:ring-white/5 dark:md:hover:bg-zinc-700/40 dark:md:hover:ring-zinc-500"><svg aria-hidden="true" viewBox="0 0 20 20" class="h-5 w-5 flex-none fill-zinc-400 group-hover:fill-zinc-500 md:group-hover:fill-zinc-400 dark:fill-zinc-500"><path d="M16.293 17.707a1 1 0 0 0 1.414-1.414l-1.414 1.414ZM9 14a5 5 0 0 1-5-5H2a7 7 0 0 0 7 7v-2ZM4 9a5 5 0 0 1 5-5V2a7 7 0 0 0-7 7h2Zm5-5a5 5 0 0 1 5 5h2a7 7 0 0 0-7-7v2Zm8.707 12.293-3.757-3.757-1.414 1.414 3.757 3.757 1.414-1.414ZM14 9a4.98 4.98 0 0 1-1.464 3.536l1.414 1.414A6.98 6.98 0 0 0 16 9h-2Zm-1.464 3.536A4.98 4.98 0 0 1 9 14v2a6.98 6.98 0 0 0 4.95-2.05l-1.414-1.414Z"></path></svg><span class="sr-only md:not-sr-only md:ml-2 md:text-zinc-500 md:dark:text-zinc-400">Search docs</span></button><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div></div><div class="relative flex basis-0 justify-end gap-6 sm:gap-8 md:flex-grow"><div class="h-6 w-6"></div><a class="group" aria-label="GitHub" href="https://github.com/AndreasKarasenko/scikit-ollama"><svg aria-hidden="true" viewBox="0 0 16 16" class="h-6 w-6 fill-zinc-400 group-hover:fill-zinc-500 dark:group-hover:fill-zinc-300"><path d="M8 0C3.58 0 0 3.58 0 8C0 11.54 2.29 14.53 5.47 15.59C5.87 15.66 6.02 15.42 6.02 15.21C6.02 15.02 6.01 14.39 6.01 13.72C4 14.09 3.48 13.23 3.32 12.78C3.23 12.55 2.84 11.84 2.5 11.65C2.22 11.5 1.82 11.13 2.49 11.12C3.12 11.11 3.57 11.7 3.72 11.94C4.44 13.15 5.59 12.81 6.05 12.6C6.12 12.08 6.33 11.73 6.56 11.53C4.78 11.33 2.92 10.64 2.92 7.58C2.92 6.71 3.23 5.99 3.74 5.43C3.66 5.23 3.38 4.41 3.82 3.31C3.82 3.31 4.49 3.1 6.02 4.13C6.66 3.95 7.34 3.86 8.02 3.86C8.7 3.86 9.38 3.95 10.02 4.13C11.55 3.09 12.22 3.31 12.22 3.31C12.66 4.41 12.38 5.23 12.3 5.43C12.81 5.99 13.12 6.7 13.12 7.58C13.12 10.65 11.25 11.33 9.47 11.53C9.76 11.78 10.01 12.26 10.01 13.01C10.01 14.08 10 14.94 10 15.21C10 15.42 10.15 15.67 10.55 15.59C13.71 14.53 16 11.53 16 8C16 3.58 12.42 0 8 0Z"></path></svg></a></div></header><div id="cde"></div><div class="relative mx-auto flex w-full max-w-8xl flex-auto justify-center sm:px-2 lg:px-8 xl:px-12"><div class="hidden lg:relative lg:block lg:flex-none"><div class="absolute inset-y-0 right-0 w-[50vw] bg-zinc-50 dark:hidden"></div><div class="absolute bottom-0 right-0 top-16 hidden h-12 w-px bg-gradient-to-t from-zinc-800 dark:block"></div><div class="absolute bottom-0 right-0 top-28 hidden w-px bg-zinc-800 dark:block"></div><div class="sticky top-[4.75rem] -ml-0.5 h-[calc(100vh-4.75rem)] w-64 overflow-y-auto overflow-x-hidden py-16 pl-0.5 pr-8 xl:w-72 xl:pr-16"><nav class="text-base lg:text-sm"><ul role="list" class="space-y-9"><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Introduction</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association">Home</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/about-our-meetings">About our meetings</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Text classification</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/zero-shot-text-classification">Zero-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/few-shot-text-classification">Few-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/dynamic-few-shot-text-classification">Dynamic few-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/tunable-text-classification">Tunable text classification</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Text-to-text modelling</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-summarization">Text summarization</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-translation">Text translation</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/tunable-text-to-text">Tunable text-to-text</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Resources</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-vectorization">Overview</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Contributing</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/how-to-contribute">How to contribute</a></li></ul></li></ul></nav></div></div><div class="min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16"><article><header class="mb-9 space-y-1"><h1 class="font-display text-3xl tracking-tight text-zinc-900 dark:text-white">Building a RAG chatbot</h1></header><div class="prose prose-teal max-w-none dark:prose-invert dark:text-teal-200 prose-headings:scroll-mt-28 prose-headings:font-display prose-headings:font-normal lg:prose-headings:scroll-mt-[8.5rem] prose-lead:text-teal-500 dark:prose-lead:text-teal-400 prose-a:font-semibold dark:prose-a:text-teal-400 prose-a:no-underline prose-a:shadow-[inset_0_-2px_0_0_var(--tw-prose-background,#fff),inset_0_calc(-1*(var(--tw-prose-underline-size,4px)+2px))_0_0_var(--tw-prose-underline,theme(colors.teal.300))] hover:prose-a:[--tw-prose-underline-size:6px] dark:[--tw-prose-background:theme(colors.teal.900)] dark:prose-a:shadow-[inset_0_calc(-1*var(--tw-prose-underline-size,2px))_0_0_var(--tw-prose-underline,theme(colors.teal.800))] dark:hover:prose-a:[--tw-prose-underline-size:6px] prose-pre:rounded-xl prose-pre:bg-zinc-900 prose-pre:shadow-lg dark:prose-pre:bg-zinc-700/40 dark:prose-pre:shadow-none dark:prose-pre:ring-1 dark:prose-pre:ring-zinc-100/10 dark:prose-hr:border-teal-800"><h2 id="overview">Overview</h2><p>Chatbots are among the most popular use cases for large language models (LLMs). They are designed to understand and respond to user inquiries, provide answers, perform tasks, or direct users to resources. Utilizing chatbots can significantly decrease customer support costs and improve response times to user requests. However, a common issue with chatbots is their tendency to deliver generic information when users expect domain-specific responses. Additionally, they may generate outdated information when users need current updates.</p><p>For demonstrations, I have chosen the webpage about <a href="https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/">Phi-3</a> â€” a family of open AI models by Microsoft released in April 2024.</p><p>If we ask how many parameters Phi-3-mini model has, GPT-4 will generate a response indicating that it does not know the answer:</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> openai </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> OpenAI</span>
<span class="token plain">client </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> OpenAI</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain">completion </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> client</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">completions</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">create</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain"></span>
<span class="token plain">  model</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;gpt-4-turbo&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">  messages</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;system&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;You are a helpful assistant.&quot;</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;user&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;How many parameters does the Phi-3-mini model from Microsoft have?&quot;</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token plain"></span>
<span class="token plain">  </span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token plain"></span>
<span class="token plain"></span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">print</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">completion</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">choices</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token number" style="color:#fdba74">0</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">message</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># As of the last update, the Phi model variants by Microsoft, including the Phi-3-mini, are not explicitly defined in publicly available resources. There has been no detailed information released about a specific &quot;Phi-3-mini&quot; model.</span>
</code></pre><p>If we ask GPT-3.5 the same question, it will hallucinate and provide incorrect information:</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token plain">completion </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> client</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">completions</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">create</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain"></span>
<span class="token plain">  model</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;gpt-3.5-turbo&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">  messages</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;system&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;You are a helpful assistant.&quot;</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;user&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;How many parameters does the Phi-3-mini model from Microsoft have?&quot;</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token plain"></span>
<span class="token plain">  </span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token plain"></span>
<span class="token plain"></span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">print</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">completion</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">choices</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token number" style="color:#fdba74">0</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">message</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># The Phi-3-mini model from Microsoft has 121 million parameters.</span>
</code></pre><p>These problems can be addressed by using the retrieval-augmented generation (RAG) technique. This technique supplements the LLM with a knowledge base external to its training data sources. For instance, an organization&#x27;s internal knowledge base, such as a Wiki or internal PDFs, can be provided.</p><p>The tutorial below will demonstrate how to build a simple chatbot that utilizes RAG technique and can retrieve information about a recently released family of Phi-3 models.</p><hr/><h2 id="rag-architecture">RAG Architecture</h2><p><img src="https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/ce8e33447a34db0259f888d39c58256c2cbf43b1/dingo_rag_use_case.svg" alt="RAG Architecture"/></p><p>The basic steps of the Naive RAG include:</p><p><strong>1. Indexing</strong></p><p>Indexing starts with extraction of raw data from various formats such as webpage, PDF, etc. To manage the context restrictions of language models and increase the response accuracy, the extracted text is broken down into smaller, manageable chunks. For now, Dingo supports a recursive chunking that involves breaking down a large text input into smaller segments recursively until the chunks are of a desired size. The choice of the chunking size is heavily dependent on the needs of RAG application. Thus, it is recommeded to experiment with different sizes to select the best one that will allow preserving the context and maintaining the accuracy. The extracted chunks are encoded into vector representations using an embedding model and stored in a vector database.</p><p><strong>2. Retrieval</strong></p><p>When a user submits a query, the RAG system uses the encoding model from the indexing phase to convert the query into a vector representation. It then calculates similarity scores between the query vector and the vectors of chunks in the vector database. The system identifies and retrieves the top K chunks with the highest similarity to the query. These chunks serve as the expanded context for the prompt.</p><p><strong>3. Generation</strong></p><p>The users query and selected chunks are combined into a single prompt and passed to LLM. Thus, the model is provided with the necessary contextual information to formulate and deliver a response.</p><hr/><h2 id="chatbot-architecture-and-technical-stack">Chatbot Architecture and Technical Stack</h2><p><img src="https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/7f8f41d5bf00a23638b8958cc970281857a43a6f/dingo_app_architecture.svg" alt="App Architecture"/></p><p>On a high level, the application will consist of the following components:</p><ol><li><p><a href="https://streamlit.io/">Streamlit</a> application: provides a frontend interface for users to interact with a chatbot.</p></li><li><p><code>FastAPI</code>: facilitates communication between the frontend and backend.</p></li><li><p><code>GPT-4 Turbo</code> model from OpenAI: LLM that generates responses upon receiving user queries.</p></li><li><p><code>Embedding V3 small</code> model from OpenAI: computes text embeddings.</p></li><li><p><a href="https://qdrant.tech/">QDrant</a>: vector database that stores embedded chunks of text.</p></li></ol><hr/><h2 id="implementation">Implementation</h2><h3 id="indexing">Indexing</h3><h4 id="step-1">Step 1:</h4><p>As the first step, we need to initialize an embedding model, a chat model and a vector store that will be populated with embedded chunks in the next step.</p><div class="my-8 flex rounded-3xl p-6 bg-amber-50 dark:bg-zinc-800/60 dark:ring-1 dark:ring-zinc-300/10"><svg aria-hidden="true" viewBox="0 0 32 32" fill="none" class="h-8 w-8 flex-none [--icon-foreground:theme(colors.zinc.900)] [--icon-background:theme(colors.white)]"><defs><radialGradient cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" id=":S1:-gradient" gradientTransform="matrix(0 21 -21 0 20 11)"><stop stop-color="#F53803"></stop><stop stop-color="#FF7500" offset=".527"></stop><stop stop-color="#FDBA74" offset="1"></stop></radialGradient><radialGradient cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" id=":S1:-gradient-dark" gradientTransform="matrix(0 24.5001 -19.2498 0 16 5.5)"><stop stop-color="#F53803"></stop><stop stop-color="#FF7500" offset=".527"></stop><stop stop-color="#FDBA74" offset="1"></stop></radialGradient></defs><g class="dark:hidden"><circle cx="20" cy="20" r="12" fill="url(#:S1:-gradient)"></circle><path fill-rule="evenodd" clip-rule="evenodd" d="M20 24.995c0-1.855 1.094-3.501 2.427-4.792C24.61 18.087 26 15.07 26 12.231 26 7.133 21.523 3 16 3S6 7.133 6 12.23c0 2.84 1.389 5.857 3.573 7.973C10.906 21.494 12 23.14 12 24.995V27a2 2 0 0 0 2 2h4a2 2 0 0 0 2-2v-2.005Z" class="fill-[var(--icon-background)]" fill-opacity="0.5"></path><path d="M25 12.23c0 2.536-1.254 5.303-3.269 7.255l1.391 1.436c2.354-2.28 3.878-5.547 3.878-8.69h-2ZM16 4c5.047 0 9 3.759 9 8.23h2C27 6.508 21.998 2 16 2v2Zm-9 8.23C7 7.76 10.953 4 16 4V2C10.002 2 5 6.507 5 12.23h2Zm3.269 7.255C8.254 17.533 7 14.766 7 12.23H5c0 3.143 1.523 6.41 3.877 8.69l1.392-1.436ZM13 27v-2.005h-2V27h2Zm1 1a1 1 0 0 1-1-1h-2a3 3 0 0 0 3 3v-2Zm4 0h-4v2h4v-2Zm1-1a1 1 0 0 1-1 1v2a3 3 0 0 0 3-3h-2Zm0-2.005V27h2v-2.005h-2ZM8.877 20.921C10.132 22.136 11 23.538 11 24.995h2c0-2.253-1.32-4.143-2.731-5.51L8.877 20.92Zm12.854-1.436C20.32 20.852 19 22.742 19 24.995h2c0-1.457.869-2.859 2.122-4.074l-1.391-1.436Z" class="fill-[var(--icon-foreground)]"></path><path d="M20 26a1 1 0 1 0 0-2v2Zm-8-2a1 1 0 1 0 0 2v-2Zm2 0h-2v2h2v-2Zm1 1V13.5h-2V25h2Zm-5-11.5v1h2v-1h-2Zm3.5 4.5h5v-2h-5v2Zm8.5-3.5v-1h-2v1h2ZM20 24h-2v2h2v-2Zm-2 0h-4v2h4v-2Zm-1-10.5V25h2V13.5h-2Zm2.5-2.5a2.5 2.5 0 0 0-2.5 2.5h2a.5.5 0 0 1 .5-.5v-2Zm2.5 2.5a2.5 2.5 0 0 0-2.5-2.5v2a.5.5 0 0 1 .5.5h2ZM18.5 18a3.5 3.5 0 0 0 3.5-3.5h-2a1.5 1.5 0 0 1-1.5 1.5v2ZM10 14.5a3.5 3.5 0 0 0 3.5 3.5v-2a1.5 1.5 0 0 1-1.5-1.5h-2Zm2.5-3.5a2.5 2.5 0 0 0-2.5 2.5h2a.5.5 0 0 1 .5-.5v-2Zm2.5 2.5a2.5 2.5 0 0 0-2.5-2.5v2a.5.5 0 0 1 .5.5h2Z" class="fill-[var(--icon-foreground)]"></path></g><g class="hidden dark:inline"><path fill-rule="evenodd" clip-rule="evenodd" d="M16 2C10.002 2 5 6.507 5 12.23c0 3.144 1.523 6.411 3.877 8.691.75.727 1.363 1.52 1.734 2.353.185.415.574.726 1.028.726H12a1 1 0 0 0 1-1v-4.5a.5.5 0 0 0-.5-.5A3.5 3.5 0 0 1 9 14.5V14a3 3 0 1 1 6 0v9a1 1 0 1 0 2 0v-9a3 3 0 1 1 6 0v.5a3.5 3.5 0 0 1-3.5 3.5.5.5 0 0 0-.5.5V23a1 1 0 0 0 1 1h.36c.455 0 .844-.311 1.03-.726.37-.833.982-1.626 1.732-2.353 2.354-2.28 3.878-5.547 3.878-8.69C27 6.507 21.998 2 16 2Zm5 25a1 1 0 0 0-1-1h-8a1 1 0 0 0-1 1 3 3 0 0 0 3 3h4a3 3 0 0 0 3-3Zm-8-13v1.5a.5.5 0 0 1-.5.5 1.5 1.5 0 0 1-1.5-1.5V14a1 1 0 1 1 2 0Zm6.5 2a.5.5 0 0 1-.5-.5V14a1 1 0 1 1 2 0v.5a1.5 1.5 0 0 1-1.5 1.5Z" fill="url(#:S1:-gradient-dark)"></path></g></svg><div class="ml-4 flex-auto"><p class="m-0 font-display text-xl text-amber-900 dark:text-amber-500">Note</p><div class="prose mt-2.5 text-amber-800 [--tw-prose-underline:theme(colors.amber.400)] [--tw-prose-background:theme(colors.amber.50)] prose-a:text-amber-900 prose-code:text-amber-900 dark:text-zinc-300 dark:[--tw-prose-underline:theme(colors.amber.700)] dark:prose-code:text-zinc-300"><p>It is needed to set OPENAI_API_KEY environment variable.</p></div></div></div><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token comment" style="color:#6a9955"># components.py</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">rag</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">embedders</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">openai </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> OpenAIEmbedder</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">rag</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">vector_stores</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">qdrant </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> Qdrant</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">llm</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">openai </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> OpenAI</span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Initialize an embedding model</span><span class="token plain"></span>
<span class="token plain">embedder </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> OpenAIEmbedder</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">model</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;text-embedding-3-small&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Initialize a vector store</span><span class="token plain"></span>
<span class="token plain">vector_store </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> Qdrant</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">collection_name</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;phi_3_docs&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> embedding_size</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">1536</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> path</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;./qdrant_db&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Initialize an LLM</span><span class="token plain"></span>
<span class="token plain">llm </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> OpenAI</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">model</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;gpt-4-turbo&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span>
</code></pre><h4 id="step-2">Step 2:</h4><p>Then, the website about <a href="https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/">Phi-3</a> family of models has to be parsed, chunked into smaller pieces, and embedded. The embedded chunks are used to populate a vector store.</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token comment" style="color:#6a9955"># build.py</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> components </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> vector_store</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> embedder</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">rag</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">readers</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">web </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> WebpageReader</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">rag</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chunkers</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">recursive </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> RecursiveChunker</span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Read the content of the website</span><span class="token plain"></span>
<span class="token plain">reader </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> WebpageReader</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">docs </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> reader</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Chunk the document</span><span class="token plain"></span>
<span class="token plain">chunker </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> RecursiveChunker</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">chunk_size</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">512</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">chunks </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> chunker</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chunk</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">docs</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Embed the chunks</span><span class="token plain"></span>
<span class="token plain">embedder</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">embed_chunks</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">chunks</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Populate vector store with embedded chunks</span><span class="token plain"></span>
<span class="token plain">vector_store</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">upsert_chunks</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">chunks</span><span class="token punctuation" style="color:#a1a1aa">)</span>
</code></pre><p>Run this script:</p><pre class="prism-code language-bash" style="color:#e4e4e7;font-style:normal"><code><span class="token plain">python build.py</span>
</code></pre><p>At this stage, the vector store is created, allowing chunks to be retrieved and incorporated into the prompt based on a user&#x27;s query.</p><h4 id="optional-step">[Optional Step]</h4><p>It is also possible to identify which chunks are retrieved and check their similarity scores to the user&#x27;s query:</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token comment" style="color:#6a9955"># test.py</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> components </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> vector_store</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> embedder</span>
<span class="token plain">query </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;How many parameters does Phi-3-mini model from Microsoft have?&quot;</span><span class="token plain"></span>
<span class="token plain">query_embedding </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> embedder</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">embed</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token plain">query</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token number" style="color:#fdba74">0</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token plain"></span>
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># select a single chunk (k=1) with the highest similarity to the query</span><span class="token plain"></span>
<span class="token plain">retrieved_chunks </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> vector_store</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">retrieve</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">k</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">1</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> query</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain">query_embedding</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">print</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">retrieved_chunks</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain"></span><span class="token comment" style="color:#6a9955">#[RetrievedChunk(content=&#x27; Starting today,  Phi-3-mini , a 3.8B language model is available...&#x27;, document_metadata={&#x27;source&#x27;: &#x27;https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/&#x27;}, score=0.7154231207501476)]</span>
</code></pre><p>We can see that the correct chunk was retrieved, which indeed contains information about the number of parameters in the Phi-3-mini model.</p><h3 id="retrieval-and-augmentation">Retrieval and Augmentation</h3><h4 id="step-3">Step 3:</h4><p>Once the vector store is created, we can create a RAG pipeline and serve it.</p><p>Streamlit <a href="https://docs.streamlit.io/develop/api-reference/chat/st.chat_message">only supports</a> two types of messages: <code>User</code> and <code>Assistant</code>. However, it us often more appropriate to include the retrieved data into the <code>System</code> message. Therefore, we use a custom block that injects a <code>System</code> message into the chat prompt before passing it to the RAG modifier.</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token comment" style="color:#6a9955"># serve.py</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">rag</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">prompt_modifiers </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> RAGPromptModifier</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">serve </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> serve_pipeline</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">core</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">blocks </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> InlineBlock</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">core</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">state </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> ChatPrompt</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">core</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">message </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> SystemMessage</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> components </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> vector_store</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> embedder</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> llm</span>
<!-- -->
<span class="token plain"></span><span class="token decorator annotation punctuation" style="color:#a1a1aa">@InlineBlock</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">def</span><span class="token plain"> </span><span class="token function" style="color:#ff7500">inject_system_message</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">state</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> ChatPrompt</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> context</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> store</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    messages </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> state</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">messages</span>
<span class="token plain">    system_message </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> SystemMessage</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;You are a helpful assistant.&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">return</span><span class="token plain"> ChatPrompt</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token plain">system_message</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token operator" style="color:#a1a1aa">+</span><span class="token plain">messages</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain">rag </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> RAGPromptModifier</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">embedder</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> vector_store</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">pipeline </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> inject_system_message</span><span class="token operator" style="color:#a1a1aa">&gt;&gt;</span><span class="token plain">rag</span><span class="token operator" style="color:#a1a1aa">&gt;&gt;</span><span class="token plain">llm</span>
<!-- -->
<span class="token plain">serve_pipeline</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;gpt-rag&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> pipeline</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">    host</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;127.0.0.1&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">    port</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">8000</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">    is_async</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token boolean" style="color:#fdba74">False</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain"></span><span class="token punctuation" style="color:#a1a1aa">)</span>
</code></pre><p>Run the script:</p><pre class="prism-code language-bash" style="color:#e4e4e7;font-style:normal"><code><span class="token plain">python serve.py</span>
</code></pre><p>At this stage, we have a RAG pipeline compatible with the OpenAI API, named <code>gpt-rag</code>, running on <code>http://127.0.0.1:8000/</code>. The Streamlit application will send requests to this backend.</p><h4 id="step-4">Step 4:</h4><p>Finally, we can proceed with building a chatbot UI:</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token comment" style="color:#6a9955"># app.py</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> streamlit </span><span class="token keyword" style="color:#ff7500">as</span><span class="token plain"> st</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> openai </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> OpenAI</span>
<!-- -->
<span class="token plain">st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">title</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;ðŸ¦Š LLM Expert&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># provide any string as an api_key parameter</span><span class="token plain"></span>
<span class="token plain">client </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> OpenAI</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">base_url</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;http://127.0.0.1:8000&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> api_key</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;123&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">if</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;openai_model&quot;</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">not</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;openai_model&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token plain"> </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;gpt-rag&quot;</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">if</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;messages&quot;</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">not</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">messages </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">for</span><span class="token plain"> message </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">messages</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    avatar </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;ðŸ¦Š&quot;</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">if</span><span class="token plain"> message</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token plain"> </span><span class="token operator" style="color:#a1a1aa">==</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;assistant&quot;</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">else</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;ðŸ‘¤&quot;</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">with</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat_message</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">message</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> avatar</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain">avatar</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">        st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">markdown</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">message</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">if</span><span class="token plain"> prompt </span><span class="token operator" style="color:#a1a1aa">:=</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat_input</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;How can I assist you today?&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">messages</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;user&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> prompt</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">with</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat_message</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;user&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> avatar</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;ðŸ‘¤&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">        st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">markdown</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">prompt</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">with</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat_message</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;assistant&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> avatar</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;ðŸ¦Š&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">        stream </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> client</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">completions</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">create</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain"></span>
<span class="token plain">            model</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain">st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;openai_model&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">            messages</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token plain"></span>
<span class="token plain">                </span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> m</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> m</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token plain"></span>
<span class="token plain">                </span><span class="token keyword" style="color:#ff7500">for</span><span class="token plain"> m </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">messages</span>
<span class="token plain">            </span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">            stream</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token boolean" style="color:#fdba74">False</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">        </span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">        response </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">write_stream</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">i </span><span class="token keyword" style="color:#ff7500">for</span><span class="token plain"> i </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> stream</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">choices</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token number" style="color:#fdba74">0</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">message</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">content</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">    st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">messages</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;assistant&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> response</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token punctuation" style="color:#a1a1aa">)</span>
</code></pre><p>Run the application:</p><pre class="prism-code language-bash" style="color:#e4e4e7;font-style:normal"><code><span class="token plain">streamlit run app.py</span>
</code></pre><p>ðŸŽ‰ We have successfully developed a chatbot that is augmented with the technical documentation of Phi-3 family of models.<!-- --> <!-- -->If we pose the same question to this chatbot as we previously did to GPT-4 and GPT-3.5 models, we will observe that it correctly answers the question:</p><p><img src="https://i.ibb.co/rQm0m41/Dingo-Chatbot.png" alt="Dingo Chatbot"/></p><hr/><h2 id="conclusion">Conclusion</h2><p>In this tutorial we have built a simple chatbot that utilizes RAG technique and successfully retrieves information from a vector store to generate up-to-date responses. It can be seen that Dingo enhances the development of LLM-based applications by offering essential (core) features and flexibility. That allows developers to quickly and easily create application prototypes.</p></div></article></div><div class="hidden xl:sticky xl:top-[4.75rem] xl:-mr-6 xl:block xl:h-[calc(100vh-4.75rem)] xl:flex-none xl:overflow-y-auto xl:py-16 xl:pr-6"><nav aria-labelledby="on-this-page-title" class="w-56"><h2 id="on-this-page-title" class="font-display text-sm font-medium text-zinc-900 dark:text-white">On this page</h2><ol role="list" class="mt-4 space-y-3 text-sm"><li><h3><a class="text-teal-500 dark:text-teal-200" href="#overview">Overview</a></h3></li><li><h3><a class="font-normal text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="#rag-architecture">RAG Architecture</a></h3></li><li><h3><a class="font-normal text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="#chatbot-architecture-and-technical-stack">Chatbot Architecture and Technical Stack</a></h3></li><li><h3><a class="font-normal text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="#implementation">Implementation</a></h3><ol role="list" class="mt-2 space-y-3 pl-5 text-zinc-500 dark:text-zinc-400"><li><a class="hover:text-zinc-600 dark:hover:text-zinc-300" href="#indexing">Indexing</a></li><li><a class="hover:text-zinc-600 dark:hover:text-zinc-300" href="#retrieval-and-augmentation">Retrieval and Augmentation</a></li></ol></li><li><h3><a class="font-normal text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="#conclusion">Conclusion</a></h3></li></ol></nav></div></div></div><script src="/bayreuth_ai_association/_next/static/chunks/webpack-ce4d567c9f2a94d3.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/bayreuth_ai_association/_next/static/media/8935352d0bfcf3a9-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/bayreuth_ai_association/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"4:I[5751,[],\"\"]\n7:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[1747,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"472\",\"static/chunks/472-cd817ab664ce0632.js\",\"185\",\"static/chunks/app/layout-1cfcfb656b6cf4a5.js\"],\"Providers\"]\na:I[1227,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"472\",\"static/chunks/472-cd817ab664ce0632.js\",\"185\",\"static/chunks/app/layout-1cfcfb656b6cf4a5.js\"],\"Layout\"]\nb:I[231,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"515\",\"static/chunks/app/docs/use-cases-rag-chatbot/page-4b9d8097182892f7.js\"],\"\"]\nd:I[6130,[],\"\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L4\",null,{\"buildId\":\"KteObhGWKN4CvpFO1QXQU\",\"assetPrefix\":\"/bayreuth_ai_association\",\"initialCanonicalUrl\":\"/docs/use-cases-rag-chatbot\",\"initialTree\":[\"\",{\"children\":[\"docs\",{\"children\":[\"use-cases-rag-chatbot\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"docs\",{\"children\":[\"use-cases-rag-chatbot\",{\"children\":[\"__PAGE__\",{},[[\"$L5\",\"$L6\"],null],null]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"docs\",\"children\",\"use-cases-rag-chatbot\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"docs\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"h-full antialiased __variable_01f60e __variable_a0637f\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"flex min-h-full bg-white dark:bg-zinc-900\",\"children\":[\"$\",\"$L9\",null,{\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex h-full flex-col items-center justify-center text-center\",\"children\":[[\"$\",\"p\",null,{\"className\":\"font-display text-sm font-medium text-zinc-900 dark:text-white\",\"children\":\"404\"}],[\"$\",\"h1\",null,{\"className\":\"mt-3 font-display text-3xl tracking-tight text-zinc-900 dark:text-white\",\"children\":\"Page not found\"}],[\"$\",\"p\",null,{\"className\":\"mt-2 text-sm text-zinc-500 dark:text-zinc-400\",\"children\":\"Sorry, we couldnâ€™t find the page youâ€™re looking for.\"}],[\"$\",\"$Lb\",null,{\"href\":\"/\",\"className\":\"mt-8 text-sm font-medium text-zinc-900 dark:text-white\",\"children\":\"Go back home\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}]}]}]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"f:I[4456,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"515\",\"static/chunks/app/docs/use-cases-rag-chatbot/page-4b9d8097182892f7.js\"],\"DocsHeader\"]\n10:I[7408,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"515\",\"static/chunks/app/docs/use-cases-rag-chatbot/page-4b9d8097182892f7.js\"],\"Fence\"]\n12:I[2553,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"515\",\"static/chunks/app/docs/use-cases-rag-chatbot/page-4b9d8097182892f7.js\"],\"PrevNextLinks\"]\n13:I[817,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"515\",\"static/chunks/app/docs/use-cases-rag-chatbot/page-4b9d8097182892f7.js\"],\"TableOfContents\"]\n11:T50a,# app.py\nimport streamlit as st\nfrom openai import OpenAI\n\nst.title(\"ðŸ¦Š LLM Expert\")\n\n# provide any string as an api_key parameter\nclient = OpenAI(base_url=\"http://127.0.0.1:8000\", api_key=\"123\")\n\nif \"openai_model\" not in st.session_state:\n    st.session_state[\"openai_model\"] = \"gpt-rag\"\n\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\nfor message in st.session_state.messages:\n    avatar = \"ðŸ¦Š\" if message[\"role\"] == \"assistant\" else \"ðŸ‘¤\"\n    with st.chat_message(message[\"role\"], avatar=avatar):\n        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"How can I assist you today?\"):\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\", avatar=\"ðŸ‘¤\"):\n        st.markdown(prompt)\n\n    with st.chat_message(\"assistant\", avatar=\"ðŸ¦Š\"):\n        stream = client.chat.completions.create(\n            model=st.session_state[\"openai_model\"],\n            messages=[\n                {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n                for m in st.session_state.messages\n            ],\n            stream=False,\n        )\n        response = st.write_stream((i for i in stream.choices[0].message.content))\n    st.session_state.messages.append({\"role\": \"assista"])</script><script>self.__next_f.push([1,"nt\", \"content\": response})\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"div\",null,{\"className\":\"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16\",\"children\":[[\"$\",\"article\",null,{\"children\":[[\"$\",\"$Lf\",null,{\"title\":\"Building a RAG chatbot\"}],[\"$\",\"div\",null,{\"className\":\"prose prose-teal max-w-none dark:prose-invert dark:text-teal-200 prose-headings:scroll-mt-28 prose-headings:font-display prose-headings:font-normal lg:prose-headings:scroll-mt-[8.5rem] prose-lead:text-teal-500 dark:prose-lead:text-teal-400 prose-a:font-semibold dark:prose-a:text-teal-400 prose-a:no-underline prose-a:shadow-[inset_0_-2px_0_0_var(--tw-prose-background,#fff),inset_0_calc(-1*(var(--tw-prose-underline-size,4px)+2px))_0_0_var(--tw-prose-underline,theme(colors.teal.300))] hover:prose-a:[--tw-prose-underline-size:6px] dark:[--tw-prose-background:theme(colors.teal.900)] dark:prose-a:shadow-[inset_0_calc(-1*var(--tw-prose-underline-size,2px))_0_0_var(--tw-prose-underline,theme(colors.teal.800))] dark:hover:prose-a:[--tw-prose-underline-size:6px] prose-pre:rounded-xl prose-pre:bg-zinc-900 prose-pre:shadow-lg dark:prose-pre:bg-zinc-700/40 dark:prose-pre:shadow-none dark:prose-pre:ring-1 dark:prose-pre:ring-zinc-100/10 dark:prose-hr:border-teal-800\",\"children\":[[\"$\",\"h2\",null,{\"id\":\"overview\",\"children\":\"Overview\"}],[\"$\",\"p\",null,{\"children\":\"Chatbots are among the most popular use cases for large language models (LLMs). They are designed to understand and respond to user inquiries, provide answers, perform tasks, or direct users to resources. Utilizing chatbots can significantly decrease customer support costs and improve response times to user requests. However, a common issue with chatbots is their tendency to deliver generic information when users expect domain-specific responses. Additionally, they may generate outdated information when users need current updates.\"}],[\"$\",\"p\",null,{\"children\":[\"For demonstrations, I have chosen the webpage about \",[\"$\",\"a\",null,{\"href\":\"https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/\",\"children\":\"Phi-3\"}],\" â€” a family of open AI models by Microsoft released in April 2024.\"]}],[\"$\",\"p\",null,{\"children\":\"If we ask how many parameters Phi-3-mini model has, GPT-4 will generate a response indicating that it does not know the answer:\"}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"from openai import OpenAI\\nclient = OpenAI()\\n\\ncompletion = client.chat.completions.create(\\n  model=\\\"gpt-4-turbo\\\",\\n  messages=[\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant.\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"How many parameters does the Phi-3-mini model from Microsoft have?\\\"}\\n  ]\\n)\\n\\nprint(completion.choices[0].message)\\n\\n# As of the last update, the Phi model variants by Microsoft, including the Phi-3-mini, are not explicitly defined in publicly available resources. There has been no detailed information released about a specific \\\"Phi-3-mini\\\" model.\\n\"}],[\"$\",\"p\",null,{\"children\":\"If we ask GPT-3.5 the same question, it will hallucinate and provide incorrect information:\"}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"completion = client.chat.completions.create(\\n  model=\\\"gpt-3.5-turbo\\\",\\n  messages=[\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant.\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"How many parameters does the Phi-3-mini model from Microsoft have?\\\"}\\n  ]\\n)\\n\\nprint(completion.choices[0].message)\\n\\n# The Phi-3-mini model from Microsoft has 121 million parameters.\\n\"}],[\"$\",\"p\",null,{\"children\":\"These problems can be addressed by using the retrieval-augmented generation (RAG) technique. This technique supplements the LLM with a knowledge base external to its training data sources. For instance, an organization's internal knowledge base, such as a Wiki or internal PDFs, can be provided.\"}],[\"$\",\"p\",null,{\"children\":\"The tutorial below will demonstrate how to build a simple chatbot that utilizes RAG technique and can retrieve information about a recently released family of Phi-3 models.\"}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"id\":\"rag-architecture\",\"children\":\"RAG Architecture\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/ce8e33447a34db0259f888d39c58256c2cbf43b1/dingo_rag_use_case.svg\",\"alt\":\"RAG Architecture\"}]}],[\"$\",\"p\",null,{\"children\":\"The basic steps of the Naive RAG include:\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"1. Indexing\"}]}],[\"$\",\"p\",null,{\"children\":\"Indexing starts with extraction of raw data from various formats such as webpage, PDF, etc. To manage the context restrictions of language models and increase the response accuracy, the extracted text is broken down into smaller, manageable chunks. For now, Dingo supports a recursive chunking that involves breaking down a large text input into smaller segments recursively until the chunks are of a desired size. The choice of the chunking size is heavily dependent on the needs of RAG application. Thus, it is recommeded to experiment with different sizes to select the best one that will allow preserving the context and maintaining the accuracy. The extracted chunks are encoded into vector representations using an embedding model and stored in a vector database.\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"2. Retrieval\"}]}],[\"$\",\"p\",null,{\"children\":\"When a user submits a query, the RAG system uses the encoding model from the indexing phase to convert the query into a vector representation. It then calculates similarity scores between the query vector and the vectors of chunks in the vector database. The system identifies and retrieves the top K chunks with the highest similarity to the query. These chunks serve as the expanded context for the prompt.\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"3. Generation\"}]}],[\"$\",\"p\",null,{\"children\":\"The users query and selected chunks are combined into a single prompt and passed to LLM. Thus, the model is provided with the necessary contextual information to formulate and deliver a response.\"}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"id\":\"chatbot-architecture-and-technical-stack\",\"children\":\"Chatbot Architecture and Technical Stack\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/7f8f41d5bf00a23638b8958cc970281857a43a6f/dingo_app_architecture.svg\",\"alt\":\"App Architecture\"}]}],[\"$\",\"p\",null,{\"children\":\"On a high level, the application will consist of the following components:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://streamlit.io/\",\"children\":\"Streamlit\"}],\" application: provides a frontend interface for users to interact with a chatbot.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"FastAPI\"}],\": facilitates communication between the frontend and backend.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"GPT-4 Turbo\"}],\" model from OpenAI: LLM that generates responses upon receiving user queries.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"Embedding V3 small\"}],\" model from OpenAI: computes text embeddings.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://qdrant.tech/\",\"children\":\"QDrant\"}],\": vector database that stores embedded chunks of text.\"]}]}]]}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"id\":\"implementation\",\"children\":\"Implementation\"}],[\"$\",\"h3\",null,{\"id\":\"indexing\",\"children\":\"Indexing\"}],[\"$\",\"h4\",null,{\"id\":\"step-1\",\"children\":\"Step 1:\"}],[\"$\",\"p\",null,{\"children\":\"As the first step, we need to initialize an embedding model, a chat model and a vector store that will be populated with embedded chunks in the next step.\"}],[\"$\",\"div\",null,{\"className\":\"my-8 flex rounded-3xl p-6 bg-amber-50 dark:bg-zinc-800/60 dark:ring-1 dark:ring-zinc-300/10\",\"children\":[[\"$\",\"svg\",null,{\"aria-hidden\":\"true\",\"viewBox\":\"0 0 32 32\",\"fill\":\"none\",\"className\":\"h-8 w-8 flex-none [--icon-foreground:theme(colors.zinc.900)] [--icon-background:theme(colors.white)]\",\"children\":[[\"$\",\"defs\",null,{\"children\":[[\"$\",\"radialGradient\",null,{\"cx\":0,\"cy\":0,\"r\":1,\"gradientUnits\":\"userSpaceOnUse\",\"id\":\":S1:-gradient\",\"gradientTransform\":\"matrix(0 21 -21 0 20 11)\",\"children\":[[\"$\",\"stop\",\"0\",{\"stopColor\":\"#F53803\"}],[\"$\",\"stop\",\"1\",{\"stopColor\":\"#FF7500\",\"offset\":\".527\"}],[\"$\",\"stop\",\"2\",{\"stopColor\":\"#FDBA74\",\"offset\":1}]]}],[\"$\",\"radialGradient\",null,{\"cx\":0,\"cy\":0,\"r\":1,\"gradientUnits\":\"userSpaceOnUse\",\"id\":\":S1:-gradient-dark\",\"gradientTransform\":\"matrix(0 24.5001 -19.2498 0 16 5.5)\",\"children\":[[\"$\",\"stop\",\"0\",{\"stopColor\":\"#F53803\"}],[\"$\",\"stop\",\"1\",{\"stopColor\":\"#FF7500\",\"offset\":\".527\"}],[\"$\",\"stop\",\"2\",{\"stopColor\":\"#FDBA74\",\"offset\":1}]]}]]}],[\"$\",\"g\",null,{\"className\":\"dark:hidden\",\"children\":[[\"$\",\"circle\",null,{\"cx\":20,\"cy\":20,\"r\":12,\"fill\":\"url(#:S1:-gradient)\"}],[\"$\",\"path\",null,{\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\",\"d\":\"M20 24.995c0-1.855 1.094-3.501 2.427-4.792C24.61 18.087 26 15.07 26 12.231 26 7.133 21.523 3 16 3S6 7.133 6 12.23c0 2.84 1.389 5.857 3.573 7.973C10.906 21.494 12 23.14 12 24.995V27a2 2 0 0 0 2 2h4a2 2 0 0 0 2-2v-2.005Z\",\"className\":\"fill-[var(--icon-background)]\",\"fillOpacity\":0.5}],[\"$\",\"path\",null,{\"d\":\"M25 12.23c0 2.536-1.254 5.303-3.269 7.255l1.391 1.436c2.354-2.28 3.878-5.547 3.878-8.69h-2ZM16 4c5.047 0 9 3.759 9 8.23h2C27 6.508 21.998 2 16 2v2Zm-9 8.23C7 7.76 10.953 4 16 4V2C10.002 2 5 6.507 5 12.23h2Zm3.269 7.255C8.254 17.533 7 14.766 7 12.23H5c0 3.143 1.523 6.41 3.877 8.69l1.392-1.436ZM13 27v-2.005h-2V27h2Zm1 1a1 1 0 0 1-1-1h-2a3 3 0 0 0 3 3v-2Zm4 0h-4v2h4v-2Zm1-1a1 1 0 0 1-1 1v2a3 3 0 0 0 3-3h-2Zm0-2.005V27h2v-2.005h-2ZM8.877 20.921C10.132 22.136 11 23.538 11 24.995h2c0-2.253-1.32-4.143-2.731-5.51L8.877 20.92Zm12.854-1.436C20.32 20.852 19 22.742 19 24.995h2c0-1.457.869-2.859 2.122-4.074l-1.391-1.436Z\",\"className\":\"fill-[var(--icon-foreground)]\"}],[\"$\",\"path\",null,{\"d\":\"M20 26a1 1 0 1 0 0-2v2Zm-8-2a1 1 0 1 0 0 2v-2Zm2 0h-2v2h2v-2Zm1 1V13.5h-2V25h2Zm-5-11.5v1h2v-1h-2Zm3.5 4.5h5v-2h-5v2Zm8.5-3.5v-1h-2v1h2ZM20 24h-2v2h2v-2Zm-2 0h-4v2h4v-2Zm-1-10.5V25h2V13.5h-2Zm2.5-2.5a2.5 2.5 0 0 0-2.5 2.5h2a.5.5 0 0 1 .5-.5v-2Zm2.5 2.5a2.5 2.5 0 0 0-2.5-2.5v2a.5.5 0 0 1 .5.5h2ZM18.5 18a3.5 3.5 0 0 0 3.5-3.5h-2a1.5 1.5 0 0 1-1.5 1.5v2ZM10 14.5a3.5 3.5 0 0 0 3.5 3.5v-2a1.5 1.5 0 0 1-1.5-1.5h-2Zm2.5-3.5a2.5 2.5 0 0 0-2.5 2.5h2a.5.5 0 0 1 .5-.5v-2Zm2.5 2.5a2.5 2.5 0 0 0-2.5-2.5v2a.5.5 0 0 1 .5.5h2Z\",\"className\":\"fill-[var(--icon-foreground)]\"}]]}],[\"$\",\"g\",null,{\"className\":\"hidden dark:inline\",\"children\":[\"$\",\"path\",null,{\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\",\"d\":\"M16 2C10.002 2 5 6.507 5 12.23c0 3.144 1.523 6.411 3.877 8.691.75.727 1.363 1.52 1.734 2.353.185.415.574.726 1.028.726H12a1 1 0 0 0 1-1v-4.5a.5.5 0 0 0-.5-.5A3.5 3.5 0 0 1 9 14.5V14a3 3 0 1 1 6 0v9a1 1 0 1 0 2 0v-9a3 3 0 1 1 6 0v.5a3.5 3.5 0 0 1-3.5 3.5.5.5 0 0 0-.5.5V23a1 1 0 0 0 1 1h.36c.455 0 .844-.311 1.03-.726.37-.833.982-1.626 1.732-2.353 2.354-2.28 3.878-5.547 3.878-8.69C27 6.507 21.998 2 16 2Zm5 25a1 1 0 0 0-1-1h-8a1 1 0 0 0-1 1 3 3 0 0 0 3 3h4a3 3 0 0 0 3-3Zm-8-13v1.5a.5.5 0 0 1-.5.5 1.5 1.5 0 0 1-1.5-1.5V14a1 1 0 1 1 2 0Zm6.5 2a.5.5 0 0 1-.5-.5V14a1 1 0 1 1 2 0v.5a1.5 1.5 0 0 1-1.5 1.5Z\",\"fill\":\"url(#:S1:-gradient-dark)\"}]}]]}],[\"$\",\"div\",null,{\"className\":\"ml-4 flex-auto\",\"children\":[[\"$\",\"p\",null,{\"className\":\"m-0 font-display text-xl text-amber-900 dark:text-amber-500\",\"children\":\"Note\"}],[\"$\",\"div\",null,{\"className\":\"prose mt-2.5 text-amber-800 [--tw-prose-underline:theme(colors.amber.400)] [--tw-prose-background:theme(colors.amber.50)] prose-a:text-amber-900 prose-code:text-amber-900 dark:text-zinc-300 dark:[--tw-prose-underline:theme(colors.amber.700)] dark:prose-code:text-zinc-300\",\"children\":[\"$\",\"p\",null,{\"children\":\"It is needed to set OPENAI_API_KEY environment variable.\"}]}]]}]]}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"# components.py\\nfrom agent_dingo.rag.embedders.openai import OpenAIEmbedder\\nfrom agent_dingo.rag.vector_stores.qdrant import Qdrant\\nfrom agent_dingo.llm.openai import OpenAI\\n\\n# Initialize an embedding model\\nembedder = OpenAIEmbedder(model=\\\"text-embedding-3-small\\\")\\n\\n# Initialize a vector store\\nvector_store = Qdrant(collection_name=\\\"phi_3_docs\\\", embedding_size=1536, path=\\\"./qdrant_db\\\")\\n\\n# Initialize an LLM\\nllm = OpenAI(model=\\\"gpt-4-turbo\\\")\\n\"}],[\"$\",\"h4\",null,{\"id\":\"step-2\",\"children\":\"Step 2:\"}],[\"$\",\"p\",null,{\"children\":[\"Then, the website about \",[\"$\",\"a\",null,{\"href\":\"https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/\",\"children\":\"Phi-3\"}],\" family of models has to be parsed, chunked into smaller pieces, and embedded. The embedded chunks are used to populate a vector store.\"]}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"# build.py\\nfrom components import vector_store, embedder\\nfrom agent_dingo.rag.readers.web import WebpageReader\\nfrom agent_dingo.rag.chunkers.recursive import RecursiveChunker\\n\\n# Read the content of the website\\nreader = WebpageReader()\\ndocs = reader.read(\\\"https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/\\\")\\n\\n# Chunk the document\\nchunker = RecursiveChunker(chunk_size=512)\\nchunks = chunker.chunk(docs)\\n\\n# Embed the chunks\\nembedder.embed_chunks(chunks)\\n\\n# Populate vector store with embedded chunks\\nvector_store.upsert_chunks(chunks)\\n\"}],[\"$\",\"p\",null,{\"children\":\"Run this script:\"}],[\"$\",\"$L10\",null,{\"language\":\"bash\",\"children\":\"python build.py\\n\"}],[\"$\",\"p\",null,{\"children\":\"At this stage, the vector store is created, allowing chunks to be retrieved and incorporated into the prompt based on a user's query.\"}],[\"$\",\"h4\",null,{\"id\":\"optional-step\",\"children\":\"[Optional Step]\"}],[\"$\",\"p\",null,{\"children\":\"It is also possible to identify which chunks are retrieved and check their similarity scores to the user's query:\"}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"# test.py\\nfrom components import vector_store, embedder\\nquery = \\\"How many parameters does Phi-3-mini model from Microsoft have?\\\"\\nquery_embedding = embedder.embed([query])[0]\\n# select a single chunk (k=1) with the highest similarity to the query\\nretrieved_chunks = vector_store.retrieve(k=1, query=query_embedding)\\nprint(retrieved_chunks)\\n#[RetrievedChunk(content=' Starting today,  Phi-3-mini , a 3.8B language model is available...', document_metadata={'source': 'https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/'}, score=0.7154231207501476)]\\n\"}],[\"$\",\"p\",null,{\"children\":\"We can see that the correct chunk was retrieved, which indeed contains information about the number of parameters in the Phi-3-mini model.\"}],[\"$\",\"h3\",null,{\"id\":\"retrieval-and-augmentation\",\"children\":\"Retrieval and Augmentation\"}],[\"$\",\"h4\",null,{\"id\":\"step-3\",\"children\":\"Step 3:\"}],[\"$\",\"p\",null,{\"children\":\"Once the vector store is created, we can create a RAG pipeline and serve it.\"}],[\"$\",\"p\",null,{\"children\":[\"Streamlit \",[\"$\",\"a\",null,{\"href\":\"https://docs.streamlit.io/develop/api-reference/chat/st.chat_message\",\"children\":\"only supports\"}],\" two types of messages: \",[\"$\",\"code\",null,{\"children\":\"User\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"Assistant\"}],\". However, it us often more appropriate to include the retrieved data into the \",[\"$\",\"code\",null,{\"children\":\"System\"}],\" message. Therefore, we use a custom block that injects a \",[\"$\",\"code\",null,{\"children\":\"System\"}],\" message into the chat prompt before passing it to the RAG modifier.\"]}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"# serve.py\\nfrom agent_dingo.rag.prompt_modifiers import RAGPromptModifier\\nfrom agent_dingo.serve import serve_pipeline\\nfrom agent_dingo.core.blocks import InlineBlock\\nfrom agent_dingo.core.state import ChatPrompt\\nfrom agent_dingo.core.message import SystemMessage\\nfrom components import vector_store, embedder, llm\\n\\n@InlineBlock()\\ndef inject_system_message(state: ChatPrompt, context, store):\\n    messages = state.messages\\n    system_message = SystemMessage(\\\"You are a helpful assistant.\\\")\\n    return ChatPrompt([system_message]+messages)\\n\\nrag = RAGPromptModifier(embedder, vector_store)\\npipeline = inject_system_message\u003e\u003erag\u003e\u003ellm\\n\\nserve_pipeline(\\n    {\\\"gpt-rag\\\": pipeline},\\n    host=\\\"127.0.0.1\\\",\\n    port=8000,\\n    is_async=False,\\n)\\n\"}],[\"$\",\"p\",null,{\"children\":\"Run the script:\"}],[\"$\",\"$L10\",null,{\"language\":\"bash\",\"children\":\"python serve.py\\n\"}],[\"$\",\"p\",null,{\"children\":[\"At this stage, we have a RAG pipeline compatible with the OpenAI API, named \",[\"$\",\"code\",null,{\"children\":\"gpt-rag\"}],\", running on \",[\"$\",\"code\",null,{\"children\":\"http://127.0.0.1:8000/\"}],\". The Streamlit application will send requests to this backend.\"]}],[\"$\",\"h4\",null,{\"id\":\"step-4\",\"children\":\"Step 4:\"}],[\"$\",\"p\",null,{\"children\":\"Finally, we can proceed with building a chatbot UI:\"}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"$11\"}],[\"$\",\"p\",null,{\"children\":\"Run the application:\"}],[\"$\",\"$L10\",null,{\"language\":\"bash\",\"children\":\"streamlit run app.py\\n\"}],[\"$\",\"p\",null,{\"children\":[\"ðŸŽ‰ We have successfully developed a chatbot that is augmented with the technical documentation of Phi-3 family of models.\",\" \",\"If we pose the same question to this chatbot as we previously did to GPT-4 and GPT-3.5 models, we will observe that it correctly answers the question:\"]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"https://i.ibb.co/rQm0m41/Dingo-Chatbot.png\",\"alt\":\"Dingo Chatbot\"}]}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"id\":\"conclusion\",\"children\":\"Conclusion\"}],[\"$\",\"p\",null,{\"children\":\"In this tutorial we have built a simple chatbot that utilizes RAG technique and successfully retrieves information from a vector store to generate up-to-date responses. It can be seen that Dingo enhances the development of LLM-based applications by offering essential (core) features and flexibility. That allows developers to quickly and easily create application prototypes.\"}]]}]]}],[\"$\",\"$L12\",null,{}]]}],[\"$\",\"$L13\",null,{\"tableOfContents\":[{\"level\":2,\"id\":\"overview\",\"title\":\"Overview\",\"children\":[]},{\"level\":2,\"id\":\"rag-architecture\",\"title\":\"RAG Architecture\",\"children\":[]},{\"level\":2,\"id\":\"chatbot-architecture-and-technical-stack\",\"title\":\"Chatbot Architecture and Technical Stack\",\"children\":[]},{\"level\":2,\"id\":\"implementation\",\"title\":\"Implementation\",\"children\":[{\"level\":3,\"id\":\"indexing\",\"title\":\"Indexing\"},{\"level\":3,\"id\":\"retrieval-and-augmentation\",\"title\":\"Retrieval and Augmentation\"}]},{\"level\":2,\"id\":\"conclusion\",\"title\":\"Conclusion\",\"children\":[]}]}]]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Building a RAG chatbot - Docs\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Learn how to build a RAG chat bot.\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/bayreuth_ai_association/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\n5:null\n"])</script></body></html>