3:I[9275,[],""]
4:I[1343,[],""]
5:I[1747,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","472","static/chunks/472-cd817ab664ce0632.js","185","static/chunks/app/layout-1cfcfb656b6cf4a5.js"],"Providers"]
6:I[1227,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","472","static/chunks/472-cd817ab664ce0632.js","185","static/chunks/app/layout-1cfcfb656b6cf4a5.js"],"Layout"]
7:I[231,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","290","static/chunks/app/docs/use-cases-local-chatbot/page-512775c04cd7164a.js"],""]
0:["KteObhGWKN4CvpFO1QXQU",[[["",{"children":["docs",{"children":["use-cases-local-chatbot",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",{"children":["docs",{"children":["use-cases-local-chatbot",{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","docs","children","use-cases-local-chatbot","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","docs","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"en","className":"h-full antialiased __variable_01f60e __variable_a0637f","suppressHydrationWarning":true,"children":["$","body",null,{"className":"flex min-h-full bg-white dark:bg-zinc-900","children":["$","$L5",null,{"children":["$","$L6",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16","children":["$","div",null,{"className":"flex h-full flex-col items-center justify-center text-center","children":[["$","p",null,{"className":"font-display text-sm font-medium text-zinc-900 dark:text-white","children":"404"}],["$","h1",null,{"className":"mt-3 font-display text-3xl tracking-tight text-zinc-900 dark:text-white","children":"Page not found"}],["$","p",null,{"className":"mt-2 text-sm text-zinc-500 dark:text-zinc-400","children":"Sorry, we couldn’t find the page you’re looking for."}],["$","$L7",null,{"href":"/","className":"mt-8 text-sm font-medium text-zinc-900 dark:text-white","children":"Go back home"}]]}]}],"notFoundStyles":[],"styles":null}]}]}]}]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css","precedence":"next","crossOrigin":"$undefined"}]],"$L8"]]]]
9:I[4456,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","290","static/chunks/app/docs/use-cases-local-chatbot/page-512775c04cd7164a.js"],"DocsHeader"]
a:I[7408,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","290","static/chunks/app/docs/use-cases-local-chatbot/page-512775c04cd7164a.js"],"Fence"]
b:I[2553,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","290","static/chunks/app/docs/use-cases-local-chatbot/page-512775c04cd7164a.js"],"PrevNextLinks"]
c:I[817,["231","static/chunks/231-829f56fdb7c75283.js","55","static/chunks/55-c141fc066ff4a19d.js","290","static/chunks/app/docs/use-cases-local-chatbot/page-512775c04cd7164a.js"],"TableOfContents"]
2:[["$","div",null,{"className":"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16","children":[["$","article",null,{"children":[["$","$L9",null,{"title":"Building a local chatbot"}],["$","div",null,{"className":"prose prose-teal max-w-none dark:prose-invert dark:text-teal-200 prose-headings:scroll-mt-28 prose-headings:font-display prose-headings:font-normal lg:prose-headings:scroll-mt-[8.5rem] prose-lead:text-teal-500 dark:prose-lead:text-teal-400 prose-a:font-semibold dark:prose-a:text-teal-400 prose-a:no-underline prose-a:shadow-[inset_0_-2px_0_0_var(--tw-prose-background,#fff),inset_0_calc(-1*(var(--tw-prose-underline-size,4px)+2px))_0_0_var(--tw-prose-underline,theme(colors.teal.300))] hover:prose-a:[--tw-prose-underline-size:6px] dark:[--tw-prose-background:theme(colors.teal.900)] dark:prose-a:shadow-[inset_0_calc(-1*var(--tw-prose-underline-size,2px))_0_0_var(--tw-prose-underline,theme(colors.teal.800))] dark:hover:prose-a:[--tw-prose-underline-size:6px] prose-pre:rounded-xl prose-pre:bg-zinc-900 prose-pre:shadow-lg dark:prose-pre:bg-zinc-700/40 dark:prose-pre:shadow-none dark:prose-pre:ring-1 dark:prose-pre:ring-zinc-100/10 dark:prose-hr:border-teal-800","children":[["$","h2",null,{"id":"overview","children":"Overview"}],["$","p",null,{"children":["In the ",["$","a",null,{"href":"/docs/use-cases-rag-chatbot","children":"previous tutorial"}],", we have built a RAG chatbot using a closed-source LLM and embedding model from OpenAI. Since some users prefer running LLMs locally, this tutorial will demonstrate how to build a RAG chatbot using a fully local, open-source solution by changing just two Dingo components."]}],["$","hr",null,{}],["$","h2",null,{"id":"chatbot-architecture-and-technical-stack","children":"Chatbot Architecture and Technical Stack"}],["$","p",null,{"children":["$","img",null,{"src":"https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/4ef5627a6ce5ac37ce3ffacb786a35e49558f674/dingo_local_app_architecture.svg","alt":"Local App Architecture"}]}],["$","p",null,{"children":"The application will consist of the following components:"}],["$","ol",null,{"children":[["$","li",null,{"children":["$","p",null,{"children":[["$","a",null,{"href":"https://streamlit.io/","children":"Streamlit"}]," application: provides a frontend interface for users to interact with a chatbot."]}]}],["$","li",null,{"children":["$","p",null,{"children":"FastAPI: facilitates communication between the frontend and backend."}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","a",null,{"href":"https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF","children":"CapybaraHermes-2.5-Mistral-7B-GGUF"}],": LLM that generates responses upon receiving user queries."]}]}],["$","li",null,{"children":["$","p",null,{"children":["Embedding model from ",["$","a",null,{"href":"https://sbert.net/","children":"SentenceTransformers"}],": computes text embeddings."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","a",null,{"href":"https://qdrant.tech/","children":"QDrant"}],": vector database that stores embedded chunks of text."]}]}]]}],["$","p",null,{"children":"There are two main differences to an architecture used in the previous tutorial:"}],["$","ul",null,{"children":["$","li",null,{"children":["$","strong",null,{"children":"Usage of quantized open-source LLM:"}]}]}],["$","p",null,{"children":["For running the model locally, Dingo can use ",["$","a",null,{"href":"https://github.com/abetlen/llama-cpp-python","children":["$","code",null,{"children":"llama-cpp-python"}]}]," that is a Python binding for ",["$","a",null,{"href":"https://github.com/ggerganov/llama.cpp","children":["$","code",null,{"children":"llama.cpp"}]}]," library which allows to run models converted to GGUF, a binary file format for storing models for inference with ",["$","code",null,{"children":"llama.cpp"}],"."]}],["$","p",null,{"children":["You can find many GGUF models on ",["$","a",null,{"href":"https://huggingface.co/models?library=gguf","children":"Hugging Face Hub"}],". We have chosen ",["$","code",null,{"children":"CapybaraHermes-2.5-Mistral-7B-GGUF"}]," model ",["$","a",null,{"href":"https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF","children":"prvided by TheBloke"}]," for this tutorial."]}],["$","p",null,{"children":["In order to download the model, you must go to ",["$","a",null,{"href":"https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF/tree/main","children":["$","code",null,{"children":"Files and versions"}]}],", where you will find many different files to choose from. They correspond to different ",["$","a",null,{"href":"https://huggingface.co/docs/hub/en/gguf#quantization-types","children":"quantization types"}]," of the model. Quantization involves reducing the memory needed to store model weights by decreasing their precision (for example, from 32-bit floating points to 4-bit integers). Higher precision usually leads to a higher accuracy but also requires more computational resources, which can make the model slower and more costly to operate. Decreasing the precision allows loading large models that typically would not fit into memory, and accelerating the inference. Usually, a 4-bit quantization is considered to be an optimal balance between performance, and size/speed for LLMs."]}],["$","ul",null,{"children":["$","li",null,{"children":["$","strong",null,{"children":"Usage of open-source embedding model:"}]}]}],["$","p",null,{"children":["SentenceTransformers is a Python toolkit that is built on top of Hugging Face's transformers library. It facilitates using transformer models, like BERT, RoBERTa, and others, for generating sentence embeddings. These embeddings can be used for tasks such as clustering, semantic search, and classification of texts. You can check the provided pre-trained models tuned for specific tasks either on the page of SentenceTransformers ",["$","a",null,{"href":"https://sbert.net/docs/pretrained_models.html#model-overview","children":"here"}],", or on the ",["$","a",null,{"href":"https://huggingface.co/models?library=sentence-transformers&sort=downloads","children":"Hugging Face Hub"}],". The models on Hugging Face Hub have a ",["$","a",null,{"href":"https://huggingface.co/docs/hub/models-widgets#whats-a-widget","children":"widget"}]," that allows running inferences and playing with the model directly in the browser."]}],["$","hr",null,{}],["$","h2",null,{"id":"implementation","children":"Implementation"}],["$","p",null,{"children":"As the first step, we need to initialize an embedding model, a chat model and a vector store that will be populated with embedded chunks in the next step."}],["$","$La",null,{"language":"python","children":"# components.py\nfrom agent_dingo.rag.embedders.sentence_transformer import SentenceTransformer\nfrom agent_dingo.rag.vector_stores.qdrant import Qdrant\nfrom agent_dingo.llm.llama_cpp import LlamaCPP\n\n# Initialize an embedding model\nembedder = SentenceTransformer(model_name=\"paraphrase-MiniLM-L6-v2\")\n\n# Initialize a vector store\nvector_store = Qdrant(collection_name=\"phi_3_docs\", embedding_size=384, path=\"./qdrant_db\")\n\n# Initialize an LLM\nmodel = \"capybarahermes-2.5-mistral-7b.Q4_K_M.gguf\"\nllm = LlamaCPP(model=model, n_ctx = 2048)\n"}],["$","p",null,{"children":["The subsequent steps involve populating the vector store, creating a RAG pipeline, and building a chatbot UI. These steps are exactly the same as in the ",["$","a",null,{"href":"/docs/use-cases-rag-chatbot","children":"previous tutorial"}],"."]}],["$","p",null,{"children":"By asking a question about the Phi-3 family of models, we can verify that our local model accurately retrieves the relevant information:"}],["$","p",null,{"children":["$","img",null,{"src":"https://i.ibb.co/23VmG8Y/Screenshot-2024-05-04-at-21-12-59.png","alt":"Dingo Local Chatbot"}]}],["$","hr",null,{}],["$","h2",null,{"id":"conclusion","children":"Conclusion"}],["$","p",null,{"children":"In this tutorial we have built a simple local chatbot that utilizes RAG technique and successfully retrieves information from a vector store to generate up-to-date responses. It can be seen that Dingo provides developers with flexibility, as the components of a LLM pipeline can be easily exchanged. For example, we were able to switch from a proprietary solution to a fully open-source solution running locally by simply changing two components of the pipeline."}]]}]]}],["$","$Lb",null,{}]]}],["$","$Lc",null,{"tableOfContents":[{"level":2,"id":"overview","title":"Overview","children":[]},{"level":2,"id":"chatbot-architecture-and-technical-stack","title":"Chatbot Architecture and Technical Stack","children":[]},{"level":2,"id":"implementation","title":"Implementation","children":[]},{"level":2,"id":"conclusion","title":"Conclusion","children":[]}]}]]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Building a local chatbot - Docs"}],["$","meta","3",{"name":"description","content":"Learn how to build a local chat bot."}],["$","link","4",{"rel":"icon","href":"/bayreuth_ai_association/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","5",{"name":"next-size-adjust"}]]
1:null
