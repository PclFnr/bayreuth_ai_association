<!DOCTYPE html><html lang="en" class="h-full antialiased __variable_01f60e __variable_a0637f"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/bayreuth_ai_association/_next/static/media/8935352d0bfcf3a9-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/bayreuth_ai_association/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/4ef5627a6ce5ac37ce3ffacb786a35e49558f674/dingo_local_app_architecture.svg"/><link rel="preload" as="image" href="https://i.ibb.co/23VmG8Y/Screenshot-2024-05-04-at-21-12-59.png"/><link rel="stylesheet" href="/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/bayreuth_ai_association/_next/static/chunks/webpack-ce4d567c9f2a94d3.js"/><script src="/bayreuth_ai_association/_next/static/chunks/fd9d1056-96f96ef28edce221.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/23-80ca076b01c7bb1f.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/main-app-28cdedace4fab0f2.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/231-829f56fdb7c75283.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/55-c141fc066ff4a19d.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/472-cd817ab664ce0632.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/app/layout-1cfcfb656b6cf4a5.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/app/docs/use-cases-local-chatbot/page-512775c04cd7164a.js" async=""></script><title>Building a local chatbot - Docs</title><meta name="description" content="Learn how to build a local chat bot."/><link rel="icon" href="/bayreuth_ai_association/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/bayreuth_ai_association/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="flex min-h-full bg-white dark:bg-zinc-900"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="flex w-full flex-col"><header class="sticky top-0 z-50 flex flex-none flex-wrap items-center justify-between bg-white px-4 py-5 shadow-md shadow-zinc-900/5 transition duration-500 sm:px-6 lg:px-8 dark:shadow-none dark:bg-transparent"><div class="mr-6 flex lg:hidden"><button type="button" class="relative" aria-label="Open navigation"><svg aria-hidden="true" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke-linecap="round" class="h-6 w-6 stroke-zinc-500"><path d="M4 7h16M4 12h16M4 17h16"></path></svg></button><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div></div><div class="relative flex flex-grow basis-0 items-center"><a aria-label="Home page" href="/bayreuth_ai_association"><p>scikit-ollama</p></a></div><div class="-my-5 mr-6 sm:mr-8 md:mr-0"><button type="button" class="group flex h-6 w-6 items-center justify-center sm:justify-start md:h-auto md:w-80 md:flex-none md:rounded-lg md:py-2.5 md:pl-4 md:pr-3.5 md:text-sm md:ring-1 md:ring-zinc-200 md:hover:ring-zinc-300 lg:w-96 dark:md:bg-zinc-800/75 dark:md:ring-inset dark:md:ring-white/5 dark:md:hover:bg-zinc-700/40 dark:md:hover:ring-zinc-500"><svg aria-hidden="true" viewBox="0 0 20 20" class="h-5 w-5 flex-none fill-zinc-400 group-hover:fill-zinc-500 md:group-hover:fill-zinc-400 dark:fill-zinc-500"><path d="M16.293 17.707a1 1 0 0 0 1.414-1.414l-1.414 1.414ZM9 14a5 5 0 0 1-5-5H2a7 7 0 0 0 7 7v-2ZM4 9a5 5 0 0 1 5-5V2a7 7 0 0 0-7 7h2Zm5-5a5 5 0 0 1 5 5h2a7 7 0 0 0-7-7v2Zm8.707 12.293-3.757-3.757-1.414 1.414 3.757 3.757 1.414-1.414ZM14 9a4.98 4.98 0 0 1-1.464 3.536l1.414 1.414A6.98 6.98 0 0 0 16 9h-2Zm-1.464 3.536A4.98 4.98 0 0 1 9 14v2a6.98 6.98 0 0 0 4.95-2.05l-1.414-1.414Z"></path></svg><span class="sr-only md:not-sr-only md:ml-2 md:text-zinc-500 md:dark:text-zinc-400">Search docs</span></button><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div></div><div class="relative flex basis-0 justify-end gap-6 sm:gap-8 md:flex-grow"><div class="h-6 w-6"></div><a class="group" aria-label="GitHub" href="https://github.com/AndreasKarasenko/scikit-ollama"><svg aria-hidden="true" viewBox="0 0 16 16" class="h-6 w-6 fill-zinc-400 group-hover:fill-zinc-500 dark:group-hover:fill-zinc-300"><path d="M8 0C3.58 0 0 3.58 0 8C0 11.54 2.29 14.53 5.47 15.59C5.87 15.66 6.02 15.42 6.02 15.21C6.02 15.02 6.01 14.39 6.01 13.72C4 14.09 3.48 13.23 3.32 12.78C3.23 12.55 2.84 11.84 2.5 11.65C2.22 11.5 1.82 11.13 2.49 11.12C3.12 11.11 3.57 11.7 3.72 11.94C4.44 13.15 5.59 12.81 6.05 12.6C6.12 12.08 6.33 11.73 6.56 11.53C4.78 11.33 2.92 10.64 2.92 7.58C2.92 6.71 3.23 5.99 3.74 5.43C3.66 5.23 3.38 4.41 3.82 3.31C3.82 3.31 4.49 3.1 6.02 4.13C6.66 3.95 7.34 3.86 8.02 3.86C8.7 3.86 9.38 3.95 10.02 4.13C11.55 3.09 12.22 3.31 12.22 3.31C12.66 4.41 12.38 5.23 12.3 5.43C12.81 5.99 13.12 6.7 13.12 7.58C13.12 10.65 11.25 11.33 9.47 11.53C9.76 11.78 10.01 12.26 10.01 13.01C10.01 14.08 10 14.94 10 15.21C10 15.42 10.15 15.67 10.55 15.59C13.71 14.53 16 11.53 16 8C16 3.58 12.42 0 8 0Z"></path></svg></a></div></header><div id="cde"></div><div class="relative mx-auto flex w-full max-w-8xl flex-auto justify-center sm:px-2 lg:px-8 xl:px-12"><div class="hidden lg:relative lg:block lg:flex-none"><div class="absolute inset-y-0 right-0 w-[50vw] bg-zinc-50 dark:hidden"></div><div class="absolute bottom-0 right-0 top-16 hidden h-12 w-px bg-gradient-to-t from-zinc-800 dark:block"></div><div class="absolute bottom-0 right-0 top-28 hidden w-px bg-zinc-800 dark:block"></div><div class="sticky top-[4.75rem] -ml-0.5 h-[calc(100vh-4.75rem)] w-64 overflow-y-auto overflow-x-hidden py-16 pl-0.5 pr-8 xl:w-72 xl:pr-16"><nav class="text-base lg:text-sm"><ul role="list" class="space-y-9"><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Introduction</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association">Home</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/about-our-meetings">About our meetings</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Text classification</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/zero-shot-text-classification">Zero-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/few-shot-text-classification">Few-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/dynamic-few-shot-text-classification">Dynamic few-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/tunable-text-classification">Tunable text classification</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Text-to-text modelling</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-summarization">Text summarization</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-translation">Text translation</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/tunable-text-to-text">Tunable text-to-text</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Resources</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-vectorization">Overview</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Contributing</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/how-to-contribute">How to contribute</a></li></ul></li></ul></nav></div></div><div class="min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16"><article><header class="mb-9 space-y-1"><h1 class="font-display text-3xl tracking-tight text-zinc-900 dark:text-white">Building a local chatbot</h1></header><div class="prose prose-teal max-w-none dark:prose-invert dark:text-teal-200 prose-headings:scroll-mt-28 prose-headings:font-display prose-headings:font-normal lg:prose-headings:scroll-mt-[8.5rem] prose-lead:text-teal-500 dark:prose-lead:text-teal-400 prose-a:font-semibold dark:prose-a:text-teal-400 prose-a:no-underline prose-a:shadow-[inset_0_-2px_0_0_var(--tw-prose-background,#fff),inset_0_calc(-1*(var(--tw-prose-underline-size,4px)+2px))_0_0_var(--tw-prose-underline,theme(colors.teal.300))] hover:prose-a:[--tw-prose-underline-size:6px] dark:[--tw-prose-background:theme(colors.teal.900)] dark:prose-a:shadow-[inset_0_calc(-1*var(--tw-prose-underline-size,2px))_0_0_var(--tw-prose-underline,theme(colors.teal.800))] dark:hover:prose-a:[--tw-prose-underline-size:6px] prose-pre:rounded-xl prose-pre:bg-zinc-900 prose-pre:shadow-lg dark:prose-pre:bg-zinc-700/40 dark:prose-pre:shadow-none dark:prose-pre:ring-1 dark:prose-pre:ring-zinc-100/10 dark:prose-hr:border-teal-800"><h2 id="overview">Overview</h2><p>In the <a href="/docs/use-cases-rag-chatbot">previous tutorial</a>, we have built a RAG chatbot using a closed-source LLM and embedding model from OpenAI. Since some users prefer running LLMs locally, this tutorial will demonstrate how to build a RAG chatbot using a fully local, open-source solution by changing just two Dingo components.</p><hr/><h2 id="chatbot-architecture-and-technical-stack">Chatbot Architecture and Technical Stack</h2><p><img src="https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/4ef5627a6ce5ac37ce3ffacb786a35e49558f674/dingo_local_app_architecture.svg" alt="Local App Architecture"/></p><p>The application will consist of the following components:</p><ol><li><p><a href="https://streamlit.io/">Streamlit</a> application: provides a frontend interface for users to interact with a chatbot.</p></li><li><p>FastAPI: facilitates communication between the frontend and backend.</p></li><li><p><a href="https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF">CapybaraHermes-2.5-Mistral-7B-GGUF</a>: LLM that generates responses upon receiving user queries.</p></li><li><p>Embedding model from <a href="https://sbert.net/">SentenceTransformers</a>: computes text embeddings.</p></li><li><p><a href="https://qdrant.tech/">QDrant</a>: vector database that stores embedded chunks of text.</p></li></ol><p>There are two main differences to an architecture used in the previous tutorial:</p><ul><li><strong>Usage of quantized open-source LLM:</strong></li></ul><p>For running the model locally, Dingo can use <a href="https://github.com/abetlen/llama-cpp-python"><code>llama-cpp-python</code></a> that is a Python binding for <a href="https://github.com/ggerganov/llama.cpp"><code>llama.cpp</code></a> library which allows to run models converted to GGUF, a binary file format for storing models for inference with <code>llama.cpp</code>.</p><p>You can find many GGUF models on <a href="https://huggingface.co/models?library=gguf">Hugging Face Hub</a>. We have chosen <code>CapybaraHermes-2.5-Mistral-7B-GGUF</code> model <a href="https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF">prvided by TheBloke</a> for this tutorial.</p><p>In order to download the model, you must go to <a href="https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF/tree/main"><code>Files and versions</code></a>, where you will find many different files to choose from. They correspond to different <a href="https://huggingface.co/docs/hub/en/gguf#quantization-types">quantization types</a> of the model. Quantization involves reducing the memory needed to store model weights by decreasing their precision (for example, from 32-bit floating points to 4-bit integers). Higher precision usually leads to a higher accuracy but also requires more computational resources, which can make the model slower and more costly to operate. Decreasing the precision allows loading large models that typically would not fit into memory, and accelerating the inference. Usually, a 4-bit quantization is considered to be an optimal balance between performance, and size/speed for LLMs.</p><ul><li><strong>Usage of open-source embedding model:</strong></li></ul><p>SentenceTransformers is a Python toolkit that is built on top of Hugging Face&#x27;s transformers library. It facilitates using transformer models, like BERT, RoBERTa, and others, for generating sentence embeddings. These embeddings can be used for tasks such as clustering, semantic search, and classification of texts. You can check the provided pre-trained models tuned for specific tasks either on the page of SentenceTransformers <a href="https://sbert.net/docs/pretrained_models.html#model-overview">here</a>, or on the <a href="https://huggingface.co/models?library=sentence-transformers&amp;sort=downloads">Hugging Face Hub</a>. The models on Hugging Face Hub have a <a href="https://huggingface.co/docs/hub/models-widgets#whats-a-widget">widget</a> that allows running inferences and playing with the model directly in the browser.</p><hr/><h2 id="implementation">Implementation</h2><p>As the first step, we need to initialize an embedding model, a chat model and a vector store that will be populated with embedded chunks in the next step.</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token comment" style="color:#6a9955"># components.py</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">rag</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">embedders</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">sentence_transformer </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> SentenceTransformer</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">rag</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">vector_stores</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">qdrant </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> Qdrant</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">llm</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">llama_cpp </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> LlamaCPP</span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Initialize an embedding model</span><span class="token plain"></span>
<span class="token plain">embedder </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> SentenceTransformer</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">model_name</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;paraphrase-MiniLM-L6-v2&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Initialize a vector store</span><span class="token plain"></span>
<span class="token plain">vector_store </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> Qdrant</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">collection_name</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;phi_3_docs&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> embedding_size</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">384</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> path</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;./qdrant_db&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Initialize an LLM</span><span class="token plain"></span>
<span class="token plain">model </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;capybarahermes-2.5-mistral-7b.Q4_K_M.gguf&quot;</span><span class="token plain"></span>
<span class="token plain">llm </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> LlamaCPP</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">model</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain">model</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> n_ctx </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token number" style="color:#fdba74">2048</span><span class="token punctuation" style="color:#a1a1aa">)</span>
</code></pre><p>The subsequent steps involve populating the vector store, creating a RAG pipeline, and building a chatbot UI. These steps are exactly the same as in the <a href="/docs/use-cases-rag-chatbot">previous tutorial</a>.</p><p>By asking a question about the Phi-3 family of models, we can verify that our local model accurately retrieves the relevant information:</p><p><img src="https://i.ibb.co/23VmG8Y/Screenshot-2024-05-04-at-21-12-59.png" alt="Dingo Local Chatbot"/></p><hr/><h2 id="conclusion">Conclusion</h2><p>In this tutorial we have built a simple local chatbot that utilizes RAG technique and successfully retrieves information from a vector store to generate up-to-date responses. It can be seen that Dingo provides developers with flexibility, as the components of a LLM pipeline can be easily exchanged. For example, we were able to switch from a proprietary solution to a fully open-source solution running locally by simply changing two components of the pipeline.</p></div></article></div><div class="hidden xl:sticky xl:top-[4.75rem] xl:-mr-6 xl:block xl:h-[calc(100vh-4.75rem)] xl:flex-none xl:overflow-y-auto xl:py-16 xl:pr-6"><nav aria-labelledby="on-this-page-title" class="w-56"><h2 id="on-this-page-title" class="font-display text-sm font-medium text-zinc-900 dark:text-white">On this page</h2><ol role="list" class="mt-4 space-y-3 text-sm"><li><h3><a class="text-teal-500 dark:text-teal-200" href="#overview">Overview</a></h3></li><li><h3><a class="font-normal text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="#chatbot-architecture-and-technical-stack">Chatbot Architecture and Technical Stack</a></h3></li><li><h3><a class="font-normal text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="#implementation">Implementation</a></h3></li><li><h3><a class="font-normal text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="#conclusion">Conclusion</a></h3></li></ol></nav></div></div></div><script src="/bayreuth_ai_association/_next/static/chunks/webpack-ce4d567c9f2a94d3.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/bayreuth_ai_association/_next/static/media/8935352d0bfcf3a9-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/bayreuth_ai_association/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"4:I[5751,[],\"\"]\n7:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[1747,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"472\",\"static/chunks/472-cd817ab664ce0632.js\",\"185\",\"static/chunks/app/layout-1cfcfb656b6cf4a5.js\"],\"Providers\"]\na:I[1227,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"472\",\"static/chunks/472-cd817ab664ce0632.js\",\"185\",\"static/chunks/app/layout-1cfcfb656b6cf4a5.js\"],\"Layout\"]\nb:I[231,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"290\",\"static/chunks/app/docs/use-cases-local-chatbot/page-512775c04cd7164a.js\"],\"\"]\nd:I[6130,[],\"\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L4\",null,{\"buildId\":\"KteObhGWKN4CvpFO1QXQU\",\"assetPrefix\":\"/bayreuth_ai_association\",\"initialCanonicalUrl\":\"/docs/use-cases-local-chatbot\",\"initialTree\":[\"\",{\"children\":[\"docs\",{\"children\":[\"use-cases-local-chatbot\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"docs\",{\"children\":[\"use-cases-local-chatbot\",{\"children\":[\"__PAGE__\",{},[[\"$L5\",\"$L6\"],null],null]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"docs\",\"children\",\"use-cases-local-chatbot\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"docs\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"h-full antialiased __variable_01f60e __variable_a0637f\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"flex min-h-full bg-white dark:bg-zinc-900\",\"children\":[\"$\",\"$L9\",null,{\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex h-full flex-col items-center justify-center text-center\",\"children\":[[\"$\",\"p\",null,{\"className\":\"font-display text-sm font-medium text-zinc-900 dark:text-white\",\"children\":\"404\"}],[\"$\",\"h1\",null,{\"className\":\"mt-3 font-display text-3xl tracking-tight text-zinc-900 dark:text-white\",\"children\":\"Page not found\"}],[\"$\",\"p\",null,{\"className\":\"mt-2 text-sm text-zinc-500 dark:text-zinc-400\",\"children\":\"Sorry, we couldn’t find the page you’re looking for.\"}],[\"$\",\"$Lb\",null,{\"href\":\"/\",\"className\":\"mt-8 text-sm font-medium text-zinc-900 dark:text-white\",\"children\":\"Go back home\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}]}]}]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"f:I[4456,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"290\",\"static/chunks/app/docs/use-cases-local-chatbot/page-512775c04cd7164a.js\"],\"DocsHeader\"]\n10:I[7408,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"290\",\"static/chunks/app/docs/use-cases-local-chatbot/page-512775c04cd7164a.js\"],\"Fence\"]\n11:I[2553,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"290\",\"static/chunks/app/docs/use-cases-local-chatbot/page-512775c04cd7164a.js\"],\"PrevNextLinks\"]\n12:I[817,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"290\",\"static/chunks/app/docs/use-cases-local-chatbot/page-512775c04cd7164a.js\"],\"TableOfContents\"]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"div\",null,{\"className\":\"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16\",\"children\":[[\"$\",\"article\",null,{\"children\":[[\"$\",\"$Lf\",null,{\"title\":\"Building a local chatbot\"}],[\"$\",\"div\",null,{\"className\":\"prose prose-teal max-w-none dark:prose-invert dark:text-teal-200 prose-headings:scroll-mt-28 prose-headings:font-display prose-headings:font-normal lg:prose-headings:scroll-mt-[8.5rem] prose-lead:text-teal-500 dark:prose-lead:text-teal-400 prose-a:font-semibold dark:prose-a:text-teal-400 prose-a:no-underline prose-a:shadow-[inset_0_-2px_0_0_var(--tw-prose-background,#fff),inset_0_calc(-1*(var(--tw-prose-underline-size,4px)+2px))_0_0_var(--tw-prose-underline,theme(colors.teal.300))] hover:prose-a:[--tw-prose-underline-size:6px] dark:[--tw-prose-background:theme(colors.teal.900)] dark:prose-a:shadow-[inset_0_calc(-1*var(--tw-prose-underline-size,2px))_0_0_var(--tw-prose-underline,theme(colors.teal.800))] dark:hover:prose-a:[--tw-prose-underline-size:6px] prose-pre:rounded-xl prose-pre:bg-zinc-900 prose-pre:shadow-lg dark:prose-pre:bg-zinc-700/40 dark:prose-pre:shadow-none dark:prose-pre:ring-1 dark:prose-pre:ring-zinc-100/10 dark:prose-hr:border-teal-800\",\"children\":[[\"$\",\"h2\",null,{\"id\":\"overview\",\"children\":\"Overview\"}],[\"$\",\"p\",null,{\"children\":[\"In the \",[\"$\",\"a\",null,{\"href\":\"/docs/use-cases-rag-chatbot\",\"children\":\"previous tutorial\"}],\", we have built a RAG chatbot using a closed-source LLM and embedding model from OpenAI. Since some users prefer running LLMs locally, this tutorial will demonstrate how to build a RAG chatbot using a fully local, open-source solution by changing just two Dingo components.\"]}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"id\":\"chatbot-architecture-and-technical-stack\",\"children\":\"Chatbot Architecture and Technical Stack\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/4ef5627a6ce5ac37ce3ffacb786a35e49558f674/dingo_local_app_architecture.svg\",\"alt\":\"Local App Architecture\"}]}],[\"$\",\"p\",null,{\"children\":\"The application will consist of the following components:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://streamlit.io/\",\"children\":\"Streamlit\"}],\" application: provides a frontend interface for users to interact with a chatbot.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":\"FastAPI: facilitates communication between the frontend and backend.\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF\",\"children\":\"CapybaraHermes-2.5-Mistral-7B-GGUF\"}],\": LLM that generates responses upon receiving user queries.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[\"Embedding model from \",[\"$\",\"a\",null,{\"href\":\"https://sbert.net/\",\"children\":\"SentenceTransformers\"}],\": computes text embeddings.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://qdrant.tech/\",\"children\":\"QDrant\"}],\": vector database that stores embedded chunks of text.\"]}]}]]}],[\"$\",\"p\",null,{\"children\":\"There are two main differences to an architecture used in the previous tutorial:\"}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Usage of quantized open-source LLM:\"}]}]}],[\"$\",\"p\",null,{\"children\":[\"For running the model locally, Dingo can use \",[\"$\",\"a\",null,{\"href\":\"https://github.com/abetlen/llama-cpp-python\",\"children\":[\"$\",\"code\",null,{\"children\":\"llama-cpp-python\"}]}],\" that is a Python binding for \",[\"$\",\"a\",null,{\"href\":\"https://github.com/ggerganov/llama.cpp\",\"children\":[\"$\",\"code\",null,{\"children\":\"llama.cpp\"}]}],\" library which allows to run models converted to GGUF, a binary file format for storing models for inference with \",[\"$\",\"code\",null,{\"children\":\"llama.cpp\"}],\".\"]}],[\"$\",\"p\",null,{\"children\":[\"You can find many GGUF models on \",[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/models?library=gguf\",\"children\":\"Hugging Face Hub\"}],\". We have chosen \",[\"$\",\"code\",null,{\"children\":\"CapybaraHermes-2.5-Mistral-7B-GGUF\"}],\" model \",[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF\",\"children\":\"prvided by TheBloke\"}],\" for this tutorial.\"]}],[\"$\",\"p\",null,{\"children\":[\"In order to download the model, you must go to \",[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF/tree/main\",\"children\":[\"$\",\"code\",null,{\"children\":\"Files and versions\"}]}],\", where you will find many different files to choose from. They correspond to different \",[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/docs/hub/en/gguf#quantization-types\",\"children\":\"quantization types\"}],\" of the model. Quantization involves reducing the memory needed to store model weights by decreasing their precision (for example, from 32-bit floating points to 4-bit integers). Higher precision usually leads to a higher accuracy but also requires more computational resources, which can make the model slower and more costly to operate. Decreasing the precision allows loading large models that typically would not fit into memory, and accelerating the inference. Usually, a 4-bit quantization is considered to be an optimal balance between performance, and size/speed for LLMs.\"]}],[\"$\",\"ul\",null,{\"children\":[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Usage of open-source embedding model:\"}]}]}],[\"$\",\"p\",null,{\"children\":[\"SentenceTransformers is a Python toolkit that is built on top of Hugging Face's transformers library. It facilitates using transformer models, like BERT, RoBERTa, and others, for generating sentence embeddings. These embeddings can be used for tasks such as clustering, semantic search, and classification of texts. You can check the provided pre-trained models tuned for specific tasks either on the page of SentenceTransformers \",[\"$\",\"a\",null,{\"href\":\"https://sbert.net/docs/pretrained_models.html#model-overview\",\"children\":\"here\"}],\", or on the \",[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/models?library=sentence-transformers\u0026sort=downloads\",\"children\":\"Hugging Face Hub\"}],\". The models on Hugging Face Hub have a \",[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/docs/hub/models-widgets#whats-a-widget\",\"children\":\"widget\"}],\" that allows running inferences and playing with the model directly in the browser.\"]}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"id\":\"implementation\",\"children\":\"Implementation\"}],[\"$\",\"p\",null,{\"children\":\"As the first step, we need to initialize an embedding model, a chat model and a vector store that will be populated with embedded chunks in the next step.\"}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"# components.py\\nfrom agent_dingo.rag.embedders.sentence_transformer import SentenceTransformer\\nfrom agent_dingo.rag.vector_stores.qdrant import Qdrant\\nfrom agent_dingo.llm.llama_cpp import LlamaCPP\\n\\n# Initialize an embedding model\\nembedder = SentenceTransformer(model_name=\\\"paraphrase-MiniLM-L6-v2\\\")\\n\\n# Initialize a vector store\\nvector_store = Qdrant(collection_name=\\\"phi_3_docs\\\", embedding_size=384, path=\\\"./qdrant_db\\\")\\n\\n# Initialize an LLM\\nmodel = \\\"capybarahermes-2.5-mistral-7b.Q4_K_M.gguf\\\"\\nllm = LlamaCPP(model=model, n_ctx = 2048)\\n\"}],[\"$\",\"p\",null,{\"children\":[\"The subsequent steps involve populating the vector store, creating a RAG pipeline, and building a chatbot UI. These steps are exactly the same as in the \",[\"$\",\"a\",null,{\"href\":\"/docs/use-cases-rag-chatbot\",\"children\":\"previous tutorial\"}],\".\"]}],[\"$\",\"p\",null,{\"children\":\"By asking a question about the Phi-3 family of models, we can verify that our local model accurately retrieves the relevant information:\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"https://i.ibb.co/23VmG8Y/Screenshot-2024-05-04-at-21-12-59.png\",\"alt\":\"Dingo Local Chatbot\"}]}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"id\":\"conclusion\",\"children\":\"Conclusion\"}],[\"$\",\"p\",null,{\"children\":\"In this tutorial we have built a simple local chatbot that utilizes RAG technique and successfully retrieves information from a vector store to generate up-to-date responses. It can be seen that Dingo provides developers with flexibility, as the components of a LLM pipeline can be easily exchanged. For example, we were able to switch from a proprietary solution to a fully open-source solution running locally by simply changing two components of the pipeline.\"}]]}]]}],[\"$\",\"$L11\",null,{}]]}],[\"$\",\"$L12\",null,{\"tableOfContents\":[{\"level\":2,\"id\":\"overview\",\"title\":\"Overview\",\"children\":[]},{\"level\":2,\"id\":\"chatbot-architecture-and-technical-stack\",\"title\":\"Chatbot Architecture and Technical Stack\",\"children\":[]},{\"level\":2,\"id\":\"implementation\",\"title\":\"Implementation\",\"children\":[]},{\"level\":2,\"id\":\"conclusion\",\"title\":\"Conclusion\",\"children\":[]}]}]]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Building a local chatbot - Docs\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Learn how to build a local chat bot.\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/bayreuth_ai_association/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\n5:null\n"])</script></body></html>