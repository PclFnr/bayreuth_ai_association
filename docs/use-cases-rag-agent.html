<!DOCTYPE html><html lang="en" class="h-full antialiased __variable_01f60e __variable_a0637f"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/bayreuth_ai_association/_next/static/media/8935352d0bfcf3a9-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/bayreuth_ai_association/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/f33293fd26a27e636286b8a9285b56d120bf1cab/dingo_agent_architecture.svg"/><link rel="preload" as="image" href="https://i.ibb.co/Kh3zVGV/Screenshot-2024-05-05-at-15-33-02.png"/><link rel="stylesheet" href="/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/bayreuth_ai_association/_next/static/chunks/webpack-ce4d567c9f2a94d3.js"/><script src="/bayreuth_ai_association/_next/static/chunks/fd9d1056-96f96ef28edce221.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/23-80ca076b01c7bb1f.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/main-app-28cdedace4fab0f2.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/231-829f56fdb7c75283.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/55-c141fc066ff4a19d.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/472-cd817ab664ce0632.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/app/layout-1cfcfb656b6cf4a5.js" async=""></script><script src="/bayreuth_ai_association/_next/static/chunks/app/docs/use-cases-rag-agent/page-5bfbe4dab0caf16f.js" async=""></script><title>Building a RAG agent - Docs</title><meta name="description" content="Learn how to build a RAG agent."/><link rel="icon" href="/bayreuth_ai_association/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/bayreuth_ai_association/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="flex min-h-full bg-white dark:bg-zinc-900"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="flex w-full flex-col"><header class="sticky top-0 z-50 flex flex-none flex-wrap items-center justify-between bg-white px-4 py-5 shadow-md shadow-zinc-900/5 transition duration-500 sm:px-6 lg:px-8 dark:shadow-none dark:bg-transparent"><div class="mr-6 flex lg:hidden"><button type="button" class="relative" aria-label="Open navigation"><svg aria-hidden="true" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke-linecap="round" class="h-6 w-6 stroke-zinc-500"><path d="M4 7h16M4 12h16M4 17h16"></path></svg></button><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div></div><div class="relative flex flex-grow basis-0 items-center"><a aria-label="Home page" href="/bayreuth_ai_association"><p>scikit-ollama</p></a></div><div class="-my-5 mr-6 sm:mr-8 md:mr-0"><button type="button" class="group flex h-6 w-6 items-center justify-center sm:justify-start md:h-auto md:w-80 md:flex-none md:rounded-lg md:py-2.5 md:pl-4 md:pr-3.5 md:text-sm md:ring-1 md:ring-zinc-200 md:hover:ring-zinc-300 lg:w-96 dark:md:bg-zinc-800/75 dark:md:ring-inset dark:md:ring-white/5 dark:md:hover:bg-zinc-700/40 dark:md:hover:ring-zinc-500"><svg aria-hidden="true" viewBox="0 0 20 20" class="h-5 w-5 flex-none fill-zinc-400 group-hover:fill-zinc-500 md:group-hover:fill-zinc-400 dark:fill-zinc-500"><path d="M16.293 17.707a1 1 0 0 0 1.414-1.414l-1.414 1.414ZM9 14a5 5 0 0 1-5-5H2a7 7 0 0 0 7 7v-2ZM4 9a5 5 0 0 1 5-5V2a7 7 0 0 0-7 7h2Zm5-5a5 5 0 0 1 5 5h2a7 7 0 0 0-7-7v2Zm8.707 12.293-3.757-3.757-1.414 1.414 3.757 3.757 1.414-1.414ZM14 9a4.98 4.98 0 0 1-1.464 3.536l1.414 1.414A6.98 6.98 0 0 0 16 9h-2Zm-1.464 3.536A4.98 4.98 0 0 1 9 14v2a6.98 6.98 0 0 0 4.95-2.05l-1.414-1.414Z"></path></svg><span class="sr-only md:not-sr-only md:ml-2 md:text-zinc-500 md:dark:text-zinc-400">Search docs</span></button><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div></div><div class="relative flex basis-0 justify-end gap-6 sm:gap-8 md:flex-grow"><div class="h-6 w-6"></div><a class="group" aria-label="GitHub" href="https://github.com/AndreasKarasenko/scikit-ollama"><svg aria-hidden="true" viewBox="0 0 16 16" class="h-6 w-6 fill-zinc-400 group-hover:fill-zinc-500 dark:group-hover:fill-zinc-300"><path d="M8 0C3.58 0 0 3.58 0 8C0 11.54 2.29 14.53 5.47 15.59C5.87 15.66 6.02 15.42 6.02 15.21C6.02 15.02 6.01 14.39 6.01 13.72C4 14.09 3.48 13.23 3.32 12.78C3.23 12.55 2.84 11.84 2.5 11.65C2.22 11.5 1.82 11.13 2.49 11.12C3.12 11.11 3.57 11.7 3.72 11.94C4.44 13.15 5.59 12.81 6.05 12.6C6.12 12.08 6.33 11.73 6.56 11.53C4.78 11.33 2.92 10.64 2.92 7.58C2.92 6.71 3.23 5.99 3.74 5.43C3.66 5.23 3.38 4.41 3.82 3.31C3.82 3.31 4.49 3.1 6.02 4.13C6.66 3.95 7.34 3.86 8.02 3.86C8.7 3.86 9.38 3.95 10.02 4.13C11.55 3.09 12.22 3.31 12.22 3.31C12.66 4.41 12.38 5.23 12.3 5.43C12.81 5.99 13.12 6.7 13.12 7.58C13.12 10.65 11.25 11.33 9.47 11.53C9.76 11.78 10.01 12.26 10.01 13.01C10.01 14.08 10 14.94 10 15.21C10 15.42 10.15 15.67 10.55 15.59C13.71 14.53 16 11.53 16 8C16 3.58 12.42 0 8 0Z"></path></svg></a></div></header><div id="cde"></div><div class="relative mx-auto flex w-full max-w-8xl flex-auto justify-center sm:px-2 lg:px-8 xl:px-12"><div class="hidden lg:relative lg:block lg:flex-none"><div class="absolute inset-y-0 right-0 w-[50vw] bg-zinc-50 dark:hidden"></div><div class="absolute bottom-0 right-0 top-16 hidden h-12 w-px bg-gradient-to-t from-zinc-800 dark:block"></div><div class="absolute bottom-0 right-0 top-28 hidden w-px bg-zinc-800 dark:block"></div><div class="sticky top-[4.75rem] -ml-0.5 h-[calc(100vh-4.75rem)] w-64 overflow-y-auto overflow-x-hidden py-16 pl-0.5 pr-8 xl:w-72 xl:pr-16"><nav class="text-base lg:text-sm"><ul role="list" class="space-y-9"><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Introduction</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association">Home</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/about-our-meetings">About our meetings</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Text classification</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/zero-shot-text-classification">Zero-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/few-shot-text-classification">Few-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/dynamic-few-shot-text-classification">Dynamic few-shot text classification</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/tunable-text-classification">Tunable text classification</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Text-to-text modelling</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-summarization">Text summarization</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-translation">Text translation</a></li><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/tunable-text-to-text">Tunable text-to-text</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Resources</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/text-vectorization">Overview</a></li></ul></li><li><h2 class="font-display font-medium text-zinc-900 dark:text-white">Contributing</h2><ul role="list" class="mt-2 space-y-2 border-l-2 border-zinc-100 lg:mt-4 lg:space-y-4 lg:border-zinc-200 dark:border-zinc-800"><li class="relative"><a class="block w-full pl-3.5 before:pointer-events-none before:absolute before:-left-1 before:top-1/2 before:h-1.5 before:w-1.5 before:-translate-y-1/2 before:rounded-full text-zinc-500 before:hidden before:bg-zinc-300 hover:text-zinc-600 hover:before:block dark:text-zinc-400 dark:before:bg-zinc-700 dark:hover:text-zinc-300" href="/bayreuth_ai_association/docs/how-to-contribute">How to contribute</a></li></ul></li></ul></nav></div></div><div class="min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16"><article><header class="mb-9 space-y-1"><h1 class="font-display text-3xl tracking-tight text-zinc-900 dark:text-white">Building a RAG agent</h1></header><div class="prose prose-teal max-w-none dark:prose-invert dark:text-teal-200 prose-headings:scroll-mt-28 prose-headings:font-display prose-headings:font-normal lg:prose-headings:scroll-mt-[8.5rem] prose-lead:text-teal-500 dark:prose-lead:text-teal-400 prose-a:font-semibold dark:prose-a:text-teal-400 prose-a:no-underline prose-a:shadow-[inset_0_-2px_0_0_var(--tw-prose-background,#fff),inset_0_calc(-1*(var(--tw-prose-underline-size,4px)+2px))_0_0_var(--tw-prose-underline,theme(colors.teal.300))] hover:prose-a:[--tw-prose-underline-size:6px] dark:[--tw-prose-background:theme(colors.teal.900)] dark:prose-a:shadow-[inset_0_calc(-1*var(--tw-prose-underline-size,2px))_0_0_var(--tw-prose-underline,theme(colors.teal.800))] dark:hover:prose-a:[--tw-prose-underline-size:6px] prose-pre:rounded-xl prose-pre:bg-zinc-900 prose-pre:shadow-lg dark:prose-pre:bg-zinc-700/40 dark:prose-pre:shadow-none dark:prose-pre:ring-1 dark:prose-pre:ring-zinc-100/10 dark:prose-hr:border-teal-800"><h2 id="overview">Overview</h2><p>In previous tutorials, we built a pipeline that embeds the chunks of text similar to user&#x27;s query to a system message, which allows the chatbot to access the external knowledge base. However, in practice, this approach may be too naive, as it:</p><ul><li>Embeds the data regardless its necessity;</li><li>Does not provide a mechanism to selectively access different data sources;</li><li>Does not allow to modify the query before retrieving the data;</li><li>Does not allow to pass multiple queries.</li></ul><p>All of these limitations can be addressed by building a more sophisticated pipeline logic, that might have a routing and query-rewriting mechanisms. However, a viable alternative is to use an <code>Agent</code> which can inherently perform all of these tasks.</p><p>The fundamental concept of agents involves using a language model to determine a sequence of actions (including the usage of external tools) and their order. One possible action could be retrieving data from an external knowledge base in response to a user&#x27;s query. In this tutorial, we will develop a simple Agent that accesses multiple data sources and invokes data retrieval when needed.</p><p>As an example of external knowledge bases, we will use three webpages containing release announcement posts about recently released generative models:</p><ol><li><a href="https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/">Phi-3 family of models</a> from Microsoft;</li><li><a href="https://ai.meta.com/blog/meta-llama-3/">Llama 3 model</a> from Meta;</li><li><a href="https://research.myshell.ai/open-voice">OpenVoice model</a> from MyShell.</li></ol><p>Since all of these models were released recently and this information was not included in GPT-4&#x27;s training data, GPT can either provide no information about these topics, or may hallucinate and generate incorrect responses (see example in my previous article <a href="/docs/use-cases-rag-chatbot">here</a>). By creating an agent that is able to retrieve data from external datasources (such as webpages linked above), we will provide an LLM with relevant contextual information that will be used for generating responses.</p><hr/><h2 id="rag-agent-architecture-and-technical-stack">RAG Agent Architecture and Technical Stack</h2><p><img src="https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/f33293fd26a27e636286b8a9285b56d120bf1cab/dingo_agent_architecture.svg" alt="App Architecture"/></p><p>The application will consist of the following components:</p><ol><li><p><a href="https://streamlit.io/">Streamlit</a> application: provides a frontend interface for users to interact with a chatbot.</p></li><li><p><code>FastAPI</code>: facilitates communication between the frontend and backend.</p></li><li><p><code>Dingo Agent</code>: <code>GPT-4 Turbo</code> model from OpenAI that has access to provided knowledge bases and invokes data retrieval from them if needed.</p></li><li><p><code>LLMs docs</code>: a vector store containing documentation about two recently released Phi-3 family of models and Llama 3.</p></li><li><p><code>Audio gen docs</code>: a vector store containing documentation about recently released OpenVoice model.</p></li><li><p><code>Embedding V3 small</code> model from OpenAI: computes text embeddings.</p></li><li><p><a href="https://qdrant.tech/">QDrant</a>: vector database that stores embedded chunks of text.</p></li></ol><hr/><h2 id="implementation">Implementation</h2><h3 id="indexing">Indexing</h3><h4 id="step-1">Step 1:</h4><p>As the first step, we need to initialize an embedding model, a chat model, and two vector stores: one for storing documentation for Llama 3 and Phi-3, and another for storing documentation for OpenVoice.</p><div class="my-8 flex rounded-3xl p-6 bg-amber-50 dark:bg-zinc-800/60 dark:ring-1 dark:ring-zinc-300/10"><svg aria-hidden="true" viewBox="0 0 32 32" fill="none" class="h-8 w-8 flex-none [--icon-foreground:theme(colors.zinc.900)] [--icon-background:theme(colors.white)]"><defs><radialGradient cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" id=":S1:-gradient" gradientTransform="matrix(0 21 -21 0 20 11)"><stop stop-color="#F53803"></stop><stop stop-color="#FF7500" offset=".527"></stop><stop stop-color="#FDBA74" offset="1"></stop></radialGradient><radialGradient cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" id=":S1:-gradient-dark" gradientTransform="matrix(0 24.5001 -19.2498 0 16 5.5)"><stop stop-color="#F53803"></stop><stop stop-color="#FF7500" offset=".527"></stop><stop stop-color="#FDBA74" offset="1"></stop></radialGradient></defs><g class="dark:hidden"><circle cx="20" cy="20" r="12" fill="url(#:S1:-gradient)"></circle><path fill-rule="evenodd" clip-rule="evenodd" d="M20 24.995c0-1.855 1.094-3.501 2.427-4.792C24.61 18.087 26 15.07 26 12.231 26 7.133 21.523 3 16 3S6 7.133 6 12.23c0 2.84 1.389 5.857 3.573 7.973C10.906 21.494 12 23.14 12 24.995V27a2 2 0 0 0 2 2h4a2 2 0 0 0 2-2v-2.005Z" class="fill-[var(--icon-background)]" fill-opacity="0.5"></path><path d="M25 12.23c0 2.536-1.254 5.303-3.269 7.255l1.391 1.436c2.354-2.28 3.878-5.547 3.878-8.69h-2ZM16 4c5.047 0 9 3.759 9 8.23h2C27 6.508 21.998 2 16 2v2Zm-9 8.23C7 7.76 10.953 4 16 4V2C10.002 2 5 6.507 5 12.23h2Zm3.269 7.255C8.254 17.533 7 14.766 7 12.23H5c0 3.143 1.523 6.41 3.877 8.69l1.392-1.436ZM13 27v-2.005h-2V27h2Zm1 1a1 1 0 0 1-1-1h-2a3 3 0 0 0 3 3v-2Zm4 0h-4v2h4v-2Zm1-1a1 1 0 0 1-1 1v2a3 3 0 0 0 3-3h-2Zm0-2.005V27h2v-2.005h-2ZM8.877 20.921C10.132 22.136 11 23.538 11 24.995h2c0-2.253-1.32-4.143-2.731-5.51L8.877 20.92Zm12.854-1.436C20.32 20.852 19 22.742 19 24.995h2c0-1.457.869-2.859 2.122-4.074l-1.391-1.436Z" class="fill-[var(--icon-foreground)]"></path><path d="M20 26a1 1 0 1 0 0-2v2Zm-8-2a1 1 0 1 0 0 2v-2Zm2 0h-2v2h2v-2Zm1 1V13.5h-2V25h2Zm-5-11.5v1h2v-1h-2Zm3.5 4.5h5v-2h-5v2Zm8.5-3.5v-1h-2v1h2ZM20 24h-2v2h2v-2Zm-2 0h-4v2h4v-2Zm-1-10.5V25h2V13.5h-2Zm2.5-2.5a2.5 2.5 0 0 0-2.5 2.5h2a.5.5 0 0 1 .5-.5v-2Zm2.5 2.5a2.5 2.5 0 0 0-2.5-2.5v2a.5.5 0 0 1 .5.5h2ZM18.5 18a3.5 3.5 0 0 0 3.5-3.5h-2a1.5 1.5 0 0 1-1.5 1.5v2ZM10 14.5a3.5 3.5 0 0 0 3.5 3.5v-2a1.5 1.5 0 0 1-1.5-1.5h-2Zm2.5-3.5a2.5 2.5 0 0 0-2.5 2.5h2a.5.5 0 0 1 .5-.5v-2Zm2.5 2.5a2.5 2.5 0 0 0-2.5-2.5v2a.5.5 0 0 1 .5.5h2Z" class="fill-[var(--icon-foreground)]"></path></g><g class="hidden dark:inline"><path fill-rule="evenodd" clip-rule="evenodd" d="M16 2C10.002 2 5 6.507 5 12.23c0 3.144 1.523 6.411 3.877 8.691.75.727 1.363 1.52 1.734 2.353.185.415.574.726 1.028.726H12a1 1 0 0 0 1-1v-4.5a.5.5 0 0 0-.5-.5A3.5 3.5 0 0 1 9 14.5V14a3 3 0 1 1 6 0v9a1 1 0 1 0 2 0v-9a3 3 0 1 1 6 0v.5a3.5 3.5 0 0 1-3.5 3.5.5.5 0 0 0-.5.5V23a1 1 0 0 0 1 1h.36c.455 0 .844-.311 1.03-.726.37-.833.982-1.626 1.732-2.353 2.354-2.28 3.878-5.547 3.878-8.69C27 6.507 21.998 2 16 2Zm5 25a1 1 0 0 0-1-1h-8a1 1 0 0 0-1 1 3 3 0 0 0 3 3h4a3 3 0 0 0 3-3Zm-8-13v1.5a.5.5 0 0 1-.5.5 1.5 1.5 0 0 1-1.5-1.5V14a1 1 0 1 1 2 0Zm6.5 2a.5.5 0 0 1-.5-.5V14a1 1 0 1 1 2 0v.5a1.5 1.5 0 0 1-1.5 1.5Z" fill="url(#:S1:-gradient-dark)"></path></g></svg><div class="ml-4 flex-auto"><p class="m-0 font-display text-xl text-amber-900 dark:text-amber-500">Note</p><div class="prose mt-2.5 text-amber-800 [--tw-prose-underline:theme(colors.amber.400)] [--tw-prose-background:theme(colors.amber.50)] prose-a:text-amber-900 prose-code:text-amber-900 dark:text-zinc-300 dark:[--tw-prose-underline:theme(colors.amber.700)] dark:prose-code:text-zinc-300"><p>It is needed to set OPENAI_API_KEY environment variable.</p></div></div></div><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token comment" style="color:#6a9955"># components.py</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">rag</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">embedders</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">openai </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> OpenAIEmbedder</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">rag</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">vector_stores</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">qdrant </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> Qdrant</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">llm</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">openai </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> OpenAI</span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Initialize an embedding model</span><span class="token plain"></span>
<span class="token plain">embedder </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> OpenAIEmbedder</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">model</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;text-embedding-3-small&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Initialize a vector store with information about Phi-3 and Llama 3 models</span><span class="token plain"></span>
<span class="token plain">llm_vector_store </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> Qdrant</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">collection_name</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;llm&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> embedding_size</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">1536</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> path</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;./qdrant_db_llm&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Initialize a vector store with information about OpenVoice model</span><span class="token plain"></span>
<span class="token plain">audio_gen_vector_store </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> Qdrant</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">collection_name</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;audio_gen&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> embedding_size</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">1536</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> path</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;./qdrant_db_audio_gen&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Initialize an LLM</span><span class="token plain"></span>
<span class="token plain">llm </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> OpenAI</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">model </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;gpt-3.5-turbo&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span>
</code></pre><h4 id="step-2">Step 2:</h4><p>Then, the above-mentioned websites have to be parsed, chunked into smaller pieces, and embedded. The embedded chunks are used to populate the corresponding vector stores.</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token comment" style="color:#6a9955"># build.py</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> components </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> llm_vector_store</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> audio_gen_vector_store</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> embedder</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">rag</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">readers</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">web </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> WebpageReader</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">rag</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chunkers</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">recursive </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> RecursiveChunker</span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Read the content of the websites</span><span class="token plain"></span>
<span class="token plain">reader </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> WebpageReader</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">phi_3_docs </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> reader</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">llama_3_docs </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> reader</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;https://ai.meta.com/blog/meta-llama-3/&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">openvoice_docs </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> reader</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;https://research.myshell.ai/open-voice&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Chunk the documents</span><span class="token plain"></span>
<span class="token plain">chunker </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> RecursiveChunker</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">chunk_size</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">512</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">phi_3_chunks </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> chunker</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chunk</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">phi_3_docs</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">llama_3_chunks </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> chunker</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chunk</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">llama_3_docs</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">openvoice_chunks </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> chunker</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chunk</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">openvoice_docs</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Embed the chunks</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">for</span><span class="token plain"> doc </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> </span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token plain">phi_3_chunks</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> llama_3_chunks</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> openvoice_chunks</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    embedder</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">embed_chunks</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">doc</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Populate LLM vector store with embedded chunks about Phi-3 and Llama 3</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">for</span><span class="token plain"> chunk </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> </span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token plain">phi_3_chunks</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> llama_3_chunks</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    llm_vector_store</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">upsert_chunks</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">chunk</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Populate audio gen vector store with embedded chunks about OpenVoice</span><span class="token plain"></span>
<span class="token plain">audio_gen_vector_store</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">upsert_chunks</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">openvoice_chunks</span><span class="token punctuation" style="color:#a1a1aa">)</span>
</code></pre><p>Run this script:</p><pre class="prism-code language-bash" style="color:#e4e4e7;font-style:normal"><code><span class="token plain">python build.py</span>
</code></pre><h4 id="step-3">Step 3:</h4><p>Once the vector store is created, we can create a RAG pipeline. To access the pipeline from the streamlit application, we can serve it using the <code>serve_pipeline</code> function, which provides a REST API compatible with the OpenAI API (this means that we can use an official OpenAI Python client to interact with the pipeline).</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token comment" style="color:#6a9955"># serve.py</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">agent </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> Agent</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> agent_dingo</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">serve </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> serve_pipeline</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> components </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> llm_vector_store</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> audio_gen_vector_store</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> embedder</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> llm</span>
<!-- -->
<span class="token plain">agent </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> Agent</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">llm</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> max_function_calls</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">3</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Define a function that an agent can call if needed</span><span class="token plain"></span>
<span class="token plain"></span><span class="token decorator annotation punctuation" style="color:#a1a1aa">@agent</span><span class="token decorator annotation punctuation" style="color:#a1a1aa">.</span><span class="token decorator annotation punctuation" style="color:#a1a1aa">function</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">def</span><span class="token plain"> </span><span class="token function" style="color:#ff7500">retrieve</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">topic</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token builtin" style="color:#ff7500">str</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> query</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token builtin" style="color:#ff7500">str</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"> </span><span class="token operator" style="color:#a1a1aa">-</span><span class="token operator" style="color:#a1a1aa">&gt;</span><span class="token plain"> </span><span class="token builtin" style="color:#ff7500">str</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#fdba74">&quot;&quot;&quot;Retrieves the documents from the vector store based on the similarity to the query.</span>
<span class="token triple-quoted-string string" style="color:#fdba74">    This function is to be used to retrieve the additional information in order to answer users&#x27; queries.</span>
<!-- -->
<span class="token triple-quoted-string string" style="color:#fdba74">    Parameters</span>
<span class="token triple-quoted-string string" style="color:#fdba74">    ----------</span>
<span class="token triple-quoted-string string" style="color:#fdba74">    topic : str</span>
<span class="token triple-quoted-string string" style="color:#fdba74">        The topic, can be either &quot;large_language_models&quot; or &quot;audio_generation_models&quot;.</span>
<span class="token triple-quoted-string string" style="color:#fdba74">        &quot;large_language_models&quot; covers the documentation of Phi-3 family of models from Microsoft and Llama 3 model from Meta.</span>
<span class="token triple-quoted-string string" style="color:#fdba74">        &quot;audio_generation_models&quot; covers the documentation of OpenVoice voice cloning model from MyShell.</span>
<span class="token triple-quoted-string string" style="color:#fdba74">        Enum: [&quot;large_language_models&quot;, &quot;audio_generation_models&quot;]</span>
<span class="token triple-quoted-string string" style="color:#fdba74">    query : str</span>
<span class="token triple-quoted-string string" style="color:#fdba74">        A string that is used for similarity search of document chunks.</span>
<!-- -->
<span class="token triple-quoted-string string" style="color:#fdba74">    Returns</span>
<span class="token triple-quoted-string string" style="color:#fdba74">    -------</span>
<span class="token triple-quoted-string string" style="color:#fdba74">    str</span>
<span class="token triple-quoted-string string" style="color:#fdba74">        JSON-formatted string with retrieved chunks.</span>
<span class="token triple-quoted-string string" style="color:#fdba74">    &quot;&quot;&quot;</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">print</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string-interpolation string" style="color:#fdba74">f&#x27;called retrieve with topic </span><span class="token string-interpolation interpolation punctuation" style="color:#a1a1aa">{</span><span class="token string-interpolation interpolation">topic</span><span class="token string-interpolation interpolation punctuation" style="color:#a1a1aa">}</span><span class="token string-interpolation string" style="color:#fdba74"> and query </span><span class="token string-interpolation interpolation punctuation" style="color:#a1a1aa">{</span><span class="token string-interpolation interpolation">query</span><span class="token string-interpolation interpolation punctuation" style="color:#a1a1aa">}</span><span class="token string-interpolation string" style="color:#fdba74">&#x27;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">if</span><span class="token plain"> topic </span><span class="token operator" style="color:#a1a1aa">==</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;large_language_models&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">        vs </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> llm_vector_store</span>
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">elif</span><span class="token plain"> topic </span><span class="token operator" style="color:#a1a1aa">==</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;audio_generation_models&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">        vs </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> audio_gen_vector_store</span>
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">else</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">        </span><span class="token keyword" style="color:#ff7500">return</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;Unknown topic. The topic must be one of `large_language_models` or `audio_generation_models`&quot;</span><span class="token plain"></span>
<span class="token plain">    query_embedding </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> embedder</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">embed</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">query</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token number" style="color:#fdba74">0</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token plain"></span>
<span class="token plain">    retrieved_chunks </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> vs</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">retrieve</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">k</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">5</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> query</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain">query_embedding</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">print</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string-interpolation string" style="color:#fdba74">f&#x27;retrieved data: </span><span class="token string-interpolation interpolation punctuation" style="color:#a1a1aa">{</span><span class="token string-interpolation interpolation">retrieved_chunks</span><span class="token string-interpolation interpolation punctuation" style="color:#a1a1aa">}</span><span class="token string-interpolation string" style="color:#fdba74">&#x27;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">return</span><span class="token plain"> </span><span class="token builtin" style="color:#ff7500">str</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token plain">chunk</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">content </span><span class="token keyword" style="color:#ff7500">for</span><span class="token plain"> chunk </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> retrieved_chunks</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Create a pipeline</span><span class="token plain"></span>
<span class="token plain">pipeline </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> agent</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">as_pipeline</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># Serve the pipeline</span><span class="token plain"></span>
<span class="token plain">serve_pipeline</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;gpt-agent&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> pipeline</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">    host</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;127.0.0.1&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">    port</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token number" style="color:#fdba74">8000</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">    is_async</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token boolean" style="color:#fdba74">False</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain"></span><span class="token punctuation" style="color:#a1a1aa">)</span>
</code></pre><p>Run the script:</p><pre class="prism-code language-bash" style="color:#e4e4e7;font-style:normal"><code><span class="token plain">python serve.py</span>
</code></pre><p>At this stage, we have an openai-compatible compatible backend with a model named <code>gpt-agent</code>, running on <code>http://127.0.0.1:8000/</code>. The Streamlit application will send requests to this backend.</p><h4 id="step-4">Step 4:</h4><p>Finally, we can proceed with building a chatbot UI:</p><pre class="prism-code language-python" style="color:#e4e4e7;font-style:normal"><code><span class="token comment" style="color:#6a9955"># app.py</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> streamlit </span><span class="token keyword" style="color:#ff7500">as</span><span class="token plain"> st</span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">from</span><span class="token plain"> openai </span><span class="token keyword" style="color:#ff7500">import</span><span class="token plain"> OpenAI</span>
<!-- -->
<span class="token plain">st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">title</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;🦊 Agent&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token comment" style="color:#6a9955"># provide any string as an api_key parameter</span><span class="token plain"></span>
<span class="token plain">client </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> OpenAI</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">base_url</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;http://127.0.0.1:8000&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> api_key</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;123&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">if</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;openai_model&quot;</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">not</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;openai_model&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token plain"> </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;gpt-agent&quot;</span><span class="token plain"></span>
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">if</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;messages&quot;</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">not</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">messages </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">for</span><span class="token plain"> message </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">messages</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    avatar </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;🦊&quot;</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">if</span><span class="token plain"> message</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token plain"> </span><span class="token operator" style="color:#a1a1aa">==</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;assistant&quot;</span><span class="token plain"> </span><span class="token keyword" style="color:#ff7500">else</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;👤&quot;</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">with</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat_message</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">message</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> avatar</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain">avatar</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">        st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">markdown</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">message</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain"></span><span class="token keyword" style="color:#ff7500">if</span><span class="token plain"> prompt </span><span class="token operator" style="color:#a1a1aa">:=</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat_input</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;How can I assist you today?&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">    st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">messages</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;user&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> prompt</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">with</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat_message</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;user&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> avatar</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;👤&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">        st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">markdown</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">prompt</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<!-- -->
<span class="token plain">    </span><span class="token keyword" style="color:#ff7500">with</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat_message</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token string" style="color:#fdba74">&quot;assistant&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> avatar</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token string" style="color:#fdba74">&quot;🦊&quot;</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"></span>
<span class="token plain">        stream </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> client</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">chat</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">completions</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">create</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain"></span>
<span class="token plain">            model</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain">st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;openai_model&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">            messages</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token plain"></span>
<span class="token plain">                </span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> m</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> m</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token plain"></span>
<span class="token plain">                </span><span class="token keyword" style="color:#ff7500">for</span><span class="token plain"> m </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">messages</span>
<span class="token plain">            </span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">            stream</span><span class="token operator" style="color:#a1a1aa">=</span><span class="token boolean" style="color:#fdba74">False</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"></span>
<span class="token plain">        </span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">        response </span><span class="token operator" style="color:#a1a1aa">=</span><span class="token plain"> st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">write_stream</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token plain">i </span><span class="token keyword" style="color:#ff7500">for</span><span class="token plain"> i </span><span class="token keyword" style="color:#ff7500">in</span><span class="token plain"> stream</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">choices</span><span class="token punctuation" style="color:#a1a1aa">[</span><span class="token number" style="color:#fdba74">0</span><span class="token punctuation" style="color:#a1a1aa">]</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">message</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">content</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token punctuation" style="color:#a1a1aa">)</span><span class="token plain"></span>
<span class="token plain">    st</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">session_state</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">messages</span><span class="token punctuation" style="color:#a1a1aa">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#a1a1aa">(</span><span class="token punctuation" style="color:#a1a1aa">{</span><span class="token string" style="color:#fdba74">&quot;role&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;assistant&quot;</span><span class="token punctuation" style="color:#a1a1aa">,</span><span class="token plain"> </span><span class="token string" style="color:#fdba74">&quot;content&quot;</span><span class="token punctuation" style="color:#a1a1aa">:</span><span class="token plain"> response</span><span class="token punctuation" style="color:#a1a1aa">}</span><span class="token punctuation" style="color:#a1a1aa">)</span>
</code></pre><p>Run the application:</p><pre class="prism-code language-bash" style="color:#e4e4e7;font-style:normal"><code><span class="token plain">streamlit run app.py</span>
</code></pre><p>🎉 We have successfully developed an agent that is augmented with the technical documentation of several newly released generative models, and can retrieve information from these documents if necessary. To assess the agent&#x27;s ability to decide when to call the <code>retrieve</code> function and its effectiveness in retrieving data from external sources, we can pose some questions about the documents provided. As you can see below, the agent generated correct responses to these questions:</p><p><img src="https://i.ibb.co/Kh3zVGV/Screenshot-2024-05-05-at-15-33-02.png" alt="Dingo Agent"/></p><hr/><h2 id="conclusion">Conclusion</h2><p>In this tutorial, we have developed a RAG agent that can access external knowledge bases and retrieve data from them if needed. Unlike a &quot;naive&quot; RAG pipeline, the agent can selectively decide whether to access the external data, which data source to use (and how many times), and how to rewrite the user&#x27;s query before retrieving the data. This approach allows the agent to provide more accurate and relevant responses, while the high-level pipeline logic remains as simple as of a &quot;naive&quot; RAG pipeline.</p></div></article></div><div class="hidden xl:sticky xl:top-[4.75rem] xl:-mr-6 xl:block xl:h-[calc(100vh-4.75rem)] xl:flex-none xl:overflow-y-auto xl:py-16 xl:pr-6"><nav aria-labelledby="on-this-page-title" class="w-56"><h2 id="on-this-page-title" class="font-display text-sm font-medium text-zinc-900 dark:text-white">On this page</h2><ol role="list" class="mt-4 space-y-3 text-sm"><li><h3><a class="text-teal-500 dark:text-teal-200" href="#overview">Overview</a></h3></li><li><h3><a class="font-normal text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="#rag-agent-architecture-and-technical-stack">RAG Agent Architecture and Technical Stack</a></h3></li><li><h3><a class="font-normal text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="#implementation">Implementation</a></h3><ol role="list" class="mt-2 space-y-3 pl-5 text-zinc-500 dark:text-zinc-400"><li><a class="hover:text-zinc-600 dark:hover:text-zinc-300" href="#indexing">Indexing</a></li></ol></li><li><h3><a class="font-normal text-zinc-500 hover:text-zinc-600 dark:text-zinc-400 dark:hover:text-zinc-300" href="#conclusion">Conclusion</a></h3></li></ol></nav></div></div></div><script src="/bayreuth_ai_association/_next/static/chunks/webpack-ce4d567c9f2a94d3.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/bayreuth_ai_association/_next/static/media/8935352d0bfcf3a9-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/bayreuth_ai_association/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"4:I[5751,[],\"\"]\n7:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[1747,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"472\",\"static/chunks/472-cd817ab664ce0632.js\",\"185\",\"static/chunks/app/layout-1cfcfb656b6cf4a5.js\"],\"Providers\"]\na:I[1227,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"472\",\"static/chunks/472-cd817ab664ce0632.js\",\"185\",\"static/chunks/app/layout-1cfcfb656b6cf4a5.js\"],\"Layout\"]\nb:I[231,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"852\",\"static/chunks/app/docs/use-cases-rag-agent/page-5bfbe4dab0caf16f.js\"],\"\"]\nd:I[6130,[],\"\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/bayreuth_ai_association/_next/static/css/c6750efcaf6fd8ff.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L4\",null,{\"buildId\":\"KteObhGWKN4CvpFO1QXQU\",\"assetPrefix\":\"/bayreuth_ai_association\",\"initialCanonicalUrl\":\"/docs/use-cases-rag-agent\",\"initialTree\":[\"\",{\"children\":[\"docs\",{\"children\":[\"use-cases-rag-agent\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"docs\",{\"children\":[\"use-cases-rag-agent\",{\"children\":[\"__PAGE__\",{},[[\"$L5\",\"$L6\"],null],null]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"docs\",\"children\",\"use-cases-rag-agent\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"docs\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"h-full antialiased __variable_01f60e __variable_a0637f\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"flex min-h-full bg-white dark:bg-zinc-900\",\"children\":[\"$\",\"$L9\",null,{\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex h-full flex-col items-center justify-center text-center\",\"children\":[[\"$\",\"p\",null,{\"className\":\"font-display text-sm font-medium text-zinc-900 dark:text-white\",\"children\":\"404\"}],[\"$\",\"h1\",null,{\"className\":\"mt-3 font-display text-3xl tracking-tight text-zinc-900 dark:text-white\",\"children\":\"Page not found\"}],[\"$\",\"p\",null,{\"className\":\"mt-2 text-sm text-zinc-500 dark:text-zinc-400\",\"children\":\"Sorry, we couldn’t find the page you’re looking for.\"}],[\"$\",\"$Lb\",null,{\"href\":\"/\",\"className\":\"mt-8 text-sm font-medium text-zinc-900 dark:text-white\",\"children\":\"Go back home\"}]]}]}],\"notFoundStyles\":[],\"styles\":null}]}]}]}]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"f:I[4456,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"852\",\"static/chunks/app/docs/use-cases-rag-agent/page-5bfbe4dab0caf16f.js\"],\"DocsHeader\"]\n10:I[7408,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"852\",\"static/chunks/app/docs/use-cases-rag-agent/page-5bfbe4dab0caf16f.js\"],\"Fence\"]\n14:I[2553,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"852\",\"static/chunks/app/docs/use-cases-rag-agent/page-5bfbe4dab0caf16f.js\"],\"PrevNextLinks\"]\n15:I[817,[\"231\",\"static/chunks/231-829f56fdb7c75283.js\",\"55\",\"static/chunks/55-c141fc066ff4a19d.js\",\"852\",\"static/chunks/app/docs/use-cases-rag-agent/page-5bfbe4dab0caf16f.js\"],\"TableOfContents\"]\n11:T467,# build.py\nfrom components import llm_vector_store, audio_gen_vector_store, embedder\nfrom agent_dingo.rag.readers.web import WebpageReader\nfrom agent_dingo.rag.chunkers.recursive import RecursiveChunker\n\n# Read the content of the websites\nreader = WebpageReader()\nphi_3_docs = reader.read(\"https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/\")\nllama_3_docs = reader.read(\"https://ai.meta.com/blog/meta-llama-3/\")\nopenvoice_docs = reader.read(\"https://research.myshell.ai/open-voice\")\n\n# Chunk the documents\nchunker = RecursiveChunker(chunk_size=512)\nphi_3_chunks = chunker.chunk(phi_3_docs)\nllama_3_chunks = chunker.chunk(llama_3_docs)\nopenvoice_chunks = chunker.chunk(openvoice_docs)\n\n# Embed the chunks\nfor doc in [phi_3_chunks, llama_3_chunks, openvoice_chunks]:\n    embedder.embed_chunks(doc)\n\n# Populate LLM vector store with embedded chunks about Phi-3 and Llama 3\nfor chunk in [phi_3_chunks, llama_3_chunks]:\n    llm_vector_store.upsert_chunks(chunk)\n\n# Populate audio gen vector store with embedded chunks about OpenVoice\naudio_gen_vector_store.upsert_chunks(openvoice_chunks)\n12:T757,# serve.py\nfrom agent_dingo.agent import Agent\nfrom agent_dingo.serve import serve_pipeline\nfrom components import llm_vector_store, aud"])</script><script>self.__next_f.push([1,"io_gen_vector_store, embedder, llm\n\nagent = Agent(llm, max_function_calls=3)\n\n# Define a function that an agent can call if needed\n@agent.function\ndef retrieve(topic: str, query: str) -\u003e str:\n    \"\"\"Retrieves the documents from the vector store based on the similarity to the query.\n    This function is to be used to retrieve the additional information in order to answer users' queries.\n\n    Parameters\n    ----------\n    topic : str\n        The topic, can be either \"large_language_models\" or \"audio_generation_models\".\n        \"large_language_models\" covers the documentation of Phi-3 family of models from Microsoft and Llama 3 model from Meta.\n        \"audio_generation_models\" covers the documentation of OpenVoice voice cloning model from MyShell.\n        Enum: [\"large_language_models\", \"audio_generation_models\"]\n    query : str\n        A string that is used for similarity search of document chunks.\n\n    Returns\n    -------\n    str\n        JSON-formatted string with retrieved chunks.\n    \"\"\"\n    print(f'called retrieve with topic {topic} and query {query}')\n    if topic == \"large_language_models\":\n        vs = llm_vector_store\n    elif topic == \"audio_generation_models\":\n        vs = audio_gen_vector_store\n    else:\n        return \"Unknown topic. The topic must be one of `large_language_models` or `audio_generation_models`\"\n    query_embedding = embedder.embed(query)[0]\n    retrieved_chunks = vs.retrieve(k=5, query=query_embedding)\n    print(f'retrieved data: {retrieved_chunks}')\n    return str([chunk.content for chunk in retrieved_chunks])\n\n# Create a pipeline\npipeline = agent.as_pipeline()\n\n# Serve the pipeline\nserve_pipeline(\n    {\"gpt-agent\": pipeline},\n    host=\"127.0.0.1\",\n    port=8000,\n    is_async=False,\n)\n13:T506,# app.py\nimport streamlit as st\nfrom openai import OpenAI\n\nst.title(\"🦊 Agent\")\n\n# provide any string as an api_key parameter\nclient = OpenAI(base_url=\"http://127.0.0.1:8000\", api_key=\"123\")\n\nif \"openai_model\" not in st.session_state:\n    st.session_state[\"openai_model\"] = \"gpt-agent\"\nif \"messa"])</script><script>self.__next_f.push([1,"ges\" not in st.session_state:\n    st.session_state.messages = []\n\nfor message in st.session_state.messages:\n    avatar = \"🦊\" if message[\"role\"] == \"assistant\" else \"👤\"\n    with st.chat_message(message[\"role\"], avatar=avatar):\n        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"How can I assist you today?\"):\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\", avatar=\"👤\"):\n        st.markdown(prompt)\n\n    with st.chat_message(\"assistant\", avatar=\"🦊\"):\n        stream = client.chat.completions.create(\n            model=st.session_state[\"openai_model\"],\n            messages=[\n                {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n                for m in st.session_state.messages\n            ],\n            stream=False,\n        )\n        response = st.write_stream((i for i in stream.choices[0].message.content))\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"div\",null,{\"className\":\"min-w-0 max-w-2xl flex-auto px-4 py-16 lg:max-w-none lg:pl-8 lg:pr-0 xl:px-16\",\"children\":[[\"$\",\"article\",null,{\"children\":[[\"$\",\"$Lf\",null,{\"title\":\"Building a RAG agent\"}],[\"$\",\"div\",null,{\"className\":\"prose prose-teal max-w-none dark:prose-invert dark:text-teal-200 prose-headings:scroll-mt-28 prose-headings:font-display prose-headings:font-normal lg:prose-headings:scroll-mt-[8.5rem] prose-lead:text-teal-500 dark:prose-lead:text-teal-400 prose-a:font-semibold dark:prose-a:text-teal-400 prose-a:no-underline prose-a:shadow-[inset_0_-2px_0_0_var(--tw-prose-background,#fff),inset_0_calc(-1*(var(--tw-prose-underline-size,4px)+2px))_0_0_var(--tw-prose-underline,theme(colors.teal.300))] hover:prose-a:[--tw-prose-underline-size:6px] dark:[--tw-prose-background:theme(colors.teal.900)] dark:prose-a:shadow-[inset_0_calc(-1*var(--tw-prose-underline-size,2px))_0_0_var(--tw-prose-underline,theme(colors.teal.800))] dark:hover:prose-a:[--tw-prose-underline-size:6px] prose-pre:rounded-xl prose-pre:bg-zinc-900 prose-pre:shadow-lg dark:prose-pre:bg-zinc-700/40 dark:prose-pre:shadow-none dark:prose-pre:ring-1 dark:prose-pre:ring-zinc-100/10 dark:prose-hr:border-teal-800\",\"children\":[[\"$\",\"h2\",null,{\"id\":\"overview\",\"children\":\"Overview\"}],[\"$\",\"p\",null,{\"children\":\"In previous tutorials, we built a pipeline that embeds the chunks of text similar to user's query to a system message, which allows the chatbot to access the external knowledge base. However, in practice, this approach may be too naive, as it:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Embeds the data regardless its necessity;\"}],[\"$\",\"li\",null,{\"children\":\"Does not provide a mechanism to selectively access different data sources;\"}],[\"$\",\"li\",null,{\"children\":\"Does not allow to modify the query before retrieving the data;\"}],[\"$\",\"li\",null,{\"children\":\"Does not allow to pass multiple queries.\"}]]}],[\"$\",\"p\",null,{\"children\":[\"All of these limitations can be addressed by building a more sophisticated pipeline logic, that might have a routing and query-rewriting mechanisms. However, a viable alternative is to use an \",[\"$\",\"code\",null,{\"children\":\"Agent\"}],\" which can inherently perform all of these tasks.\"]}],[\"$\",\"p\",null,{\"children\":\"The fundamental concept of agents involves using a language model to determine a sequence of actions (including the usage of external tools) and their order. One possible action could be retrieving data from an external knowledge base in response to a user's query. In this tutorial, we will develop a simple Agent that accesses multiple data sources and invokes data retrieval when needed.\"}],[\"$\",\"p\",null,{\"children\":\"As an example of external knowledge bases, we will use three webpages containing release announcement posts about recently released generative models:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/\",\"children\":\"Phi-3 family of models\"}],\" from Microsoft;\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://ai.meta.com/blog/meta-llama-3/\",\"children\":\"Llama 3 model\"}],\" from Meta;\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://research.myshell.ai/open-voice\",\"children\":\"OpenVoice model\"}],\" from MyShell.\"]}]]}],[\"$\",\"p\",null,{\"children\":[\"Since all of these models were released recently and this information was not included in GPT-4's training data, GPT can either provide no information about these topics, or may hallucinate and generate incorrect responses (see example in my previous article \",[\"$\",\"a\",null,{\"href\":\"/docs/use-cases-rag-chatbot\",\"children\":\"here\"}],\"). By creating an agent that is able to retrieve data from external datasources (such as webpages linked above), we will provide an LLM with relevant contextual information that will be used for generating responses.\"]}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"id\":\"rag-agent-architecture-and-technical-stack\",\"children\":\"RAG Agent Architecture and Technical Stack\"}],[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"https://gist.githubusercontent.com/iryna-kondr/f4779bfaa918e8af9ab1d455d63e142c/raw/f33293fd26a27e636286b8a9285b56d120bf1cab/dingo_agent_architecture.svg\",\"alt\":\"App Architecture\"}]}],[\"$\",\"p\",null,{\"children\":\"The application will consist of the following components:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://streamlit.io/\",\"children\":\"Streamlit\"}],\" application: provides a frontend interface for users to interact with a chatbot.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"FastAPI\"}],\": facilitates communication between the frontend and backend.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"Dingo Agent\"}],\": \",[\"$\",\"code\",null,{\"children\":\"GPT-4 Turbo\"}],\" model from OpenAI that has access to provided knowledge bases and invokes data retrieval from them if needed.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"LLMs docs\"}],\": a vector store containing documentation about two recently released Phi-3 family of models and Llama 3.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"Audio gen docs\"}],\": a vector store containing documentation about recently released OpenVoice model.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"Embedding V3 small\"}],\" model from OpenAI: computes text embeddings.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://qdrant.tech/\",\"children\":\"QDrant\"}],\": vector database that stores embedded chunks of text.\"]}]}]]}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"id\":\"implementation\",\"children\":\"Implementation\"}],[\"$\",\"h3\",null,{\"id\":\"indexing\",\"children\":\"Indexing\"}],[\"$\",\"h4\",null,{\"id\":\"step-1\",\"children\":\"Step 1:\"}],[\"$\",\"p\",null,{\"children\":\"As the first step, we need to initialize an embedding model, a chat model, and two vector stores: one for storing documentation for Llama 3 and Phi-3, and another for storing documentation for OpenVoice.\"}],[\"$\",\"div\",null,{\"className\":\"my-8 flex rounded-3xl p-6 bg-amber-50 dark:bg-zinc-800/60 dark:ring-1 dark:ring-zinc-300/10\",\"children\":[[\"$\",\"svg\",null,{\"aria-hidden\":\"true\",\"viewBox\":\"0 0 32 32\",\"fill\":\"none\",\"className\":\"h-8 w-8 flex-none [--icon-foreground:theme(colors.zinc.900)] [--icon-background:theme(colors.white)]\",\"children\":[[\"$\",\"defs\",null,{\"children\":[[\"$\",\"radialGradient\",null,{\"cx\":0,\"cy\":0,\"r\":1,\"gradientUnits\":\"userSpaceOnUse\",\"id\":\":S1:-gradient\",\"gradientTransform\":\"matrix(0 21 -21 0 20 11)\",\"children\":[[\"$\",\"stop\",\"0\",{\"stopColor\":\"#F53803\"}],[\"$\",\"stop\",\"1\",{\"stopColor\":\"#FF7500\",\"offset\":\".527\"}],[\"$\",\"stop\",\"2\",{\"stopColor\":\"#FDBA74\",\"offset\":1}]]}],[\"$\",\"radialGradient\",null,{\"cx\":0,\"cy\":0,\"r\":1,\"gradientUnits\":\"userSpaceOnUse\",\"id\":\":S1:-gradient-dark\",\"gradientTransform\":\"matrix(0 24.5001 -19.2498 0 16 5.5)\",\"children\":[[\"$\",\"stop\",\"0\",{\"stopColor\":\"#F53803\"}],[\"$\",\"stop\",\"1\",{\"stopColor\":\"#FF7500\",\"offset\":\".527\"}],[\"$\",\"stop\",\"2\",{\"stopColor\":\"#FDBA74\",\"offset\":1}]]}]]}],[\"$\",\"g\",null,{\"className\":\"dark:hidden\",\"children\":[[\"$\",\"circle\",null,{\"cx\":20,\"cy\":20,\"r\":12,\"fill\":\"url(#:S1:-gradient)\"}],[\"$\",\"path\",null,{\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\",\"d\":\"M20 24.995c0-1.855 1.094-3.501 2.427-4.792C24.61 18.087 26 15.07 26 12.231 26 7.133 21.523 3 16 3S6 7.133 6 12.23c0 2.84 1.389 5.857 3.573 7.973C10.906 21.494 12 23.14 12 24.995V27a2 2 0 0 0 2 2h4a2 2 0 0 0 2-2v-2.005Z\",\"className\":\"fill-[var(--icon-background)]\",\"fillOpacity\":0.5}],[\"$\",\"path\",null,{\"d\":\"M25 12.23c0 2.536-1.254 5.303-3.269 7.255l1.391 1.436c2.354-2.28 3.878-5.547 3.878-8.69h-2ZM16 4c5.047 0 9 3.759 9 8.23h2C27 6.508 21.998 2 16 2v2Zm-9 8.23C7 7.76 10.953 4 16 4V2C10.002 2 5 6.507 5 12.23h2Zm3.269 7.255C8.254 17.533 7 14.766 7 12.23H5c0 3.143 1.523 6.41 3.877 8.69l1.392-1.436ZM13 27v-2.005h-2V27h2Zm1 1a1 1 0 0 1-1-1h-2a3 3 0 0 0 3 3v-2Zm4 0h-4v2h4v-2Zm1-1a1 1 0 0 1-1 1v2a3 3 0 0 0 3-3h-2Zm0-2.005V27h2v-2.005h-2ZM8.877 20.921C10.132 22.136 11 23.538 11 24.995h2c0-2.253-1.32-4.143-2.731-5.51L8.877 20.92Zm12.854-1.436C20.32 20.852 19 22.742 19 24.995h2c0-1.457.869-2.859 2.122-4.074l-1.391-1.436Z\",\"className\":\"fill-[var(--icon-foreground)]\"}],[\"$\",\"path\",null,{\"d\":\"M20 26a1 1 0 1 0 0-2v2Zm-8-2a1 1 0 1 0 0 2v-2Zm2 0h-2v2h2v-2Zm1 1V13.5h-2V25h2Zm-5-11.5v1h2v-1h-2Zm3.5 4.5h5v-2h-5v2Zm8.5-3.5v-1h-2v1h2ZM20 24h-2v2h2v-2Zm-2 0h-4v2h4v-2Zm-1-10.5V25h2V13.5h-2Zm2.5-2.5a2.5 2.5 0 0 0-2.5 2.5h2a.5.5 0 0 1 .5-.5v-2Zm2.5 2.5a2.5 2.5 0 0 0-2.5-2.5v2a.5.5 0 0 1 .5.5h2ZM18.5 18a3.5 3.5 0 0 0 3.5-3.5h-2a1.5 1.5 0 0 1-1.5 1.5v2ZM10 14.5a3.5 3.5 0 0 0 3.5 3.5v-2a1.5 1.5 0 0 1-1.5-1.5h-2Zm2.5-3.5a2.5 2.5 0 0 0-2.5 2.5h2a.5.5 0 0 1 .5-.5v-2Zm2.5 2.5a2.5 2.5 0 0 0-2.5-2.5v2a.5.5 0 0 1 .5.5h2Z\",\"className\":\"fill-[var(--icon-foreground)]\"}]]}],[\"$\",\"g\",null,{\"className\":\"hidden dark:inline\",\"children\":[\"$\",\"path\",null,{\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\",\"d\":\"M16 2C10.002 2 5 6.507 5 12.23c0 3.144 1.523 6.411 3.877 8.691.75.727 1.363 1.52 1.734 2.353.185.415.574.726 1.028.726H12a1 1 0 0 0 1-1v-4.5a.5.5 0 0 0-.5-.5A3.5 3.5 0 0 1 9 14.5V14a3 3 0 1 1 6 0v9a1 1 0 1 0 2 0v-9a3 3 0 1 1 6 0v.5a3.5 3.5 0 0 1-3.5 3.5.5.5 0 0 0-.5.5V23a1 1 0 0 0 1 1h.36c.455 0 .844-.311 1.03-.726.37-.833.982-1.626 1.732-2.353 2.354-2.28 3.878-5.547 3.878-8.69C27 6.507 21.998 2 16 2Zm5 25a1 1 0 0 0-1-1h-8a1 1 0 0 0-1 1 3 3 0 0 0 3 3h4a3 3 0 0 0 3-3Zm-8-13v1.5a.5.5 0 0 1-.5.5 1.5 1.5 0 0 1-1.5-1.5V14a1 1 0 1 1 2 0Zm6.5 2a.5.5 0 0 1-.5-.5V14a1 1 0 1 1 2 0v.5a1.5 1.5 0 0 1-1.5 1.5Z\",\"fill\":\"url(#:S1:-gradient-dark)\"}]}]]}],[\"$\",\"div\",null,{\"className\":\"ml-4 flex-auto\",\"children\":[[\"$\",\"p\",null,{\"className\":\"m-0 font-display text-xl text-amber-900 dark:text-amber-500\",\"children\":\"Note\"}],[\"$\",\"div\",null,{\"className\":\"prose mt-2.5 text-amber-800 [--tw-prose-underline:theme(colors.amber.400)] [--tw-prose-background:theme(colors.amber.50)] prose-a:text-amber-900 prose-code:text-amber-900 dark:text-zinc-300 dark:[--tw-prose-underline:theme(colors.amber.700)] dark:prose-code:text-zinc-300\",\"children\":[\"$\",\"p\",null,{\"children\":\"It is needed to set OPENAI_API_KEY environment variable.\"}]}]]}]]}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"# components.py\\nfrom agent_dingo.rag.embedders.openai import OpenAIEmbedder\\nfrom agent_dingo.rag.vector_stores.qdrant import Qdrant\\nfrom agent_dingo.llm.openai import OpenAI\\n\\n# Initialize an embedding model\\nembedder = OpenAIEmbedder(model=\\\"text-embedding-3-small\\\")\\n\\n# Initialize a vector store with information about Phi-3 and Llama 3 models\\nllm_vector_store = Qdrant(collection_name=\\\"llm\\\", embedding_size=1536, path=\\\"./qdrant_db_llm\\\")\\n\\n# Initialize a vector store with information about OpenVoice model\\naudio_gen_vector_store = Qdrant(collection_name=\\\"audio_gen\\\", embedding_size=1536, path=\\\"./qdrant_db_audio_gen\\\")\\n\\n# Initialize an LLM\\nllm = OpenAI(model = \\\"gpt-3.5-turbo\\\")\\n\"}],[\"$\",\"h4\",null,{\"id\":\"step-2\",\"children\":\"Step 2:\"}],[\"$\",\"p\",null,{\"children\":\"Then, the above-mentioned websites have to be parsed, chunked into smaller pieces, and embedded. The embedded chunks are used to populate the corresponding vector stores.\"}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"$11\"}],[\"$\",\"p\",null,{\"children\":\"Run this script:\"}],[\"$\",\"$L10\",null,{\"language\":\"bash\",\"children\":\"python build.py\\n\"}],[\"$\",\"h4\",null,{\"id\":\"step-3\",\"children\":\"Step 3:\"}],[\"$\",\"p\",null,{\"children\":[\"Once the vector store is created, we can create a RAG pipeline. To access the pipeline from the streamlit application, we can serve it using the \",[\"$\",\"code\",null,{\"children\":\"serve_pipeline\"}],\" function, which provides a REST API compatible with the OpenAI API (this means that we can use an official OpenAI Python client to interact with the pipeline).\"]}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"$12\"}],[\"$\",\"p\",null,{\"children\":\"Run the script:\"}],[\"$\",\"$L10\",null,{\"language\":\"bash\",\"children\":\"python serve.py\\n\"}],[\"$\",\"p\",null,{\"children\":[\"At this stage, we have an openai-compatible compatible backend with a model named \",[\"$\",\"code\",null,{\"children\":\"gpt-agent\"}],\", running on \",[\"$\",\"code\",null,{\"children\":\"http://127.0.0.1:8000/\"}],\". The Streamlit application will send requests to this backend.\"]}],[\"$\",\"h4\",null,{\"id\":\"step-4\",\"children\":\"Step 4:\"}],[\"$\",\"p\",null,{\"children\":\"Finally, we can proceed with building a chatbot UI:\"}],[\"$\",\"$L10\",null,{\"language\":\"python\",\"children\":\"$13\"}],[\"$\",\"p\",null,{\"children\":\"Run the application:\"}],[\"$\",\"$L10\",null,{\"language\":\"bash\",\"children\":\"streamlit run app.py\\n\"}],[\"$\",\"p\",null,{\"children\":[\"🎉 We have successfully developed an agent that is augmented with the technical documentation of several newly released generative models, and can retrieve information from these documents if necessary. To assess the agent's ability to decide when to call the \",[\"$\",\"code\",null,{\"children\":\"retrieve\"}],\" function and its effectiveness in retrieving data from external sources, we can pose some questions about the documents provided. As you can see below, the agent generated correct responses to these questions:\"]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"https://i.ibb.co/Kh3zVGV/Screenshot-2024-05-05-at-15-33-02.png\",\"alt\":\"Dingo Agent\"}]}],[\"$\",\"hr\",null,{}],[\"$\",\"h2\",null,{\"id\":\"conclusion\",\"children\":\"Conclusion\"}],[\"$\",\"p\",null,{\"children\":\"In this tutorial, we have developed a RAG agent that can access external knowledge bases and retrieve data from them if needed. Unlike a \\\"naive\\\" RAG pipeline, the agent can selectively decide whether to access the external data, which data source to use (and how many times), and how to rewrite the user's query before retrieving the data. This approach allows the agent to provide more accurate and relevant responses, while the high-level pipeline logic remains as simple as of a \\\"naive\\\" RAG pipeline.\"}]]}]]}],[\"$\",\"$L14\",null,{}]]}],[\"$\",\"$L15\",null,{\"tableOfContents\":[{\"level\":2,\"id\":\"overview\",\"title\":\"Overview\",\"children\":[]},{\"level\":2,\"id\":\"rag-agent-architecture-and-technical-stack\",\"title\":\"RAG Agent Architecture and Technical Stack\",\"children\":[]},{\"level\":2,\"id\":\"implementation\",\"title\":\"Implementation\",\"children\":[{\"level\":3,\"id\":\"indexing\",\"title\":\"Indexing\"}]},{\"level\":2,\"id\":\"conclusion\",\"title\":\"Conclusion\",\"children\":[]}]}]]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Building a RAG agent - Docs\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Learn how to build a RAG agent.\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/bayreuth_ai_association/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\n5:null\n"])</script></body></html>